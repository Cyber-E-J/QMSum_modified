phd a: OK , we 're going . phd c: Eight , eight ? phd d: This is three . phd c: Three . phd d: Yep . Yep . professor b: Test . Hmm . Let 's see . Move it bit . Test ? Test ? OK , I guess it 's alright . So , let 's see . Yeah , Barry 's not here and Dave 's not here . Um , I can say about {disfmarker} just q just quickly to get through it , that Dave and I submitted this ASRU . phd a: This is for ASRU . professor b: Yeah . So . Um . Yeah , it 's {disfmarker} it 's interesting . I mean , basically we 're dealing with rever reverberation , and , um , when we deal with pure reverberation , the technique he 's using works really , really well . Uh , and when they had the reverberation here , uh , we 'll measure the signal - to - noise ratio and it 's , uh , about nine DB . So , phd d: Hmm . professor b: um , phd a: You mean , from the actual , uh , recordings ? professor b: a fair amount of {disfmarker} phd d: k phd a: It 's nine DB ? professor b: Yeah . Yeah . Um {disfmarker} And actually it brought up a question which may be relevant to the Aurora stuff too . Um , I know that when you figured out the filters that we 're using for the Mel scale , there was some experimentation that went on at {disfmarker} at , uh {disfmarker} at OGI . Um , but one of the differences that we found between the two systems that we were using , {comment} the {disfmarker} the Aurora HTK system baseline system {comment} and the system that we were {disfmarker} the {disfmarker} the uh , other system we were using , the uh , the SRI system , was that the SRI system had maybe a , um , hundred hertz high - pass . And the , uh , Aurora HTK , it was like twenty . phd d: Yep . S sixty - four . professor b: Uh . phd d: S sixty - four . professor b: Sixty - four ? Uh . phd d: Yeah , if you 're using the baseline . professor b: Is that the ba band center ? phd d: No , the edge . professor b: The edge is really , uh , sixty - four ? phd d: Yeah . professor b: For some reason , uh , Dave thought it was twenty , phd d:  So the , uh , center would be somewhere around like hundred professor b: but . phd d: and {disfmarker} hundred and {disfmarker} hundred {disfmarker} hundred and {disfmarker} maybe {disfmarker} it 's like {disfmarker} fi hundred hertz . professor b: But do you know , for instance , h how far down it would be at twenty hertz ? What the {disfmarker} how much rejection would there be at twenty hertz , let 's say ? phd d: At twenty hertz . professor b: Yeah , any idea what the curve looks like ? phd d: Twenty hertz frequency {disfmarker} Oh , it 's {disfmarker} it 's zero at twenty hertz , right ? The filter ? phd c: Yea - actually , the left edge of the first filter is at sixty - four . phd d: Sixt - s sixty - four . phd c: So {disfmarker} phd d: So anything less than sixty - four is zero . phd c: Mmm . professor b: It 's actually set to zero ? What kind of filter is that ? phd c: Yeah . phd d: Yeah . professor b: Is this {disfmarker} oh , from the {disfmarker} from {disfmarker} phd c: It {disfmarker} This is the filter bank in the frequency domain that starts at sixty - four . professor b: Oh , so you , uh {disfmarker} so you really set it to zero , the FFT ? phd d: Yeah , phd c: Yeah . phd d: yeah . So it 's {disfmarker} it 's a weight on the ball spectrum . Triangular weighting . professor b: Right . OK . Um {disfmarker} OK . So that 's {disfmarker} that 's a little different than Dave thought , I think . But {disfmarker} but , um , still , it 's possible that we 're getting in some more noise . So I wonder , is it {disfmarker} @ @ Was there {disfmarker} their experimentation with , uh , say , throwing away that filter or something ? And , uh {disfmarker} phd d: Uh , throwing away the first ? professor b: Yeah . phd d: Um , yeah , we {disfmarker} we 've tried including the full {disfmarker} full bank . Right ? From zero to four K . phd c: Mm - hmm . phd d: And that 's always worse than using sixty - four hertz . professor b: Right , but the question is , whether sixty - four hertz is {disfmarker} is , uh , too , uh , low . phd d: Yeah , I mean , make it a hundred or so ? professor b: Yeah . phd d: I t I think I 've tried a hundred and it was more or less the same , or slightly worse . professor b: On what test set ? phd d: On the same , uh , SpeechDat - Car , Aurora . professor b: Um , it was on the SpeechDat - Car . phd d: Yeah . So I tried a hundred to four K . Yeah . professor b: Um , phd d: So it was {disfmarker} professor b: and on {disfmarker} and on the , um , um , {vocalsound} TI - digits also ? phd d: No , no , no . I think I just tried it on SpeechDat - Car . professor b: Mmm . That 'd be something to look at sometime because what , um , eh , he was looking at was performance in this room . phd d: Mm - hmm . professor b: Would that be more like {disfmarker} Well , you 'd think that 'd be more like SpeechDat - Car , I guess , in terms of the noise . The SpeechDat - Car is more , uh , sort of roughly stationary , a lot of it . And {disfmarker} and TI - digits maybe is not so much as {disfmarker} phd d: Yeah . phd c: Mm - hmm . professor b: Yeah . phd d: Yeah . professor b: Mm - hmm . OK . Well , maybe it 's not a big deal . But , um {disfmarker} Anyway , that was just something we wondered about . But , um , uh , certainly a lot of the noise , uh , is , uh , below a hundred hertz . Uh , the signal - to - noise ratio , you know , looks a fair amount better if you {disfmarker} if you high - pass filter it from this room . phd d: Yeah . professor b: But , um {disfmarker} but it 's still pretty noisy . Even {disfmarker} even for a hundred hertz up , it 's {disfmarker} it 's still fairly noisy . The signal - to - noise ratio is {disfmarker} is {disfmarker} is actually still pretty bad . phd c: Mm - hmm . phd a: Hmm . professor b: So , um , I mean , the main {disfmarker} the {disfmarker} the {disfmarker} phd a: So that 's on th that 's on the f the far field ones though , right ? Yeah . professor b: Yeah , that 's on the far field . Yeah , the near field 's pretty good . phd a: So wha what is , uh {disfmarker} what 's causing that ? professor b: Well , we got a {disfmarker} a video projector in here , uh , and , uh {disfmarker} which we keep on during every {disfmarker} every session we record , phd a: Yeah . professor b: which , you know , I {disfmarker} I {disfmarker} w we were aware of phd a: Uh - huh . professor b: but {disfmarker} but we thought it wasn't a bad thing . phd a: Yeah . professor b: I mean , that 's a nice noise source . Uh , and there 's also the , uh {disfmarker} uh , air conditioning . phd a: Hmm . professor b: Which , uh , you know , is a pretty low frequency kind of thing . phd a: Mm - hmm . professor b: But {disfmarker} but , uh {disfmarker} So , those are {disfmarker} those are major components , I think , phd a: I see . professor b: uh , for the stationary kind of stuff . phd a: Mmm . professor b: Um , but , um , it , uh {disfmarker} I guess , I {disfmarker} maybe I said this last week too but it {disfmarker} it {disfmarker} it really became apparent to us that we need to {disfmarker} to take account of noise . And , uh , so I think when {disfmarker} when he gets done with his prelim study I think {vocalsound} one of the next things we 'd want to do is to take this , uh {disfmarker} uh , noise , uh , processing stuff and {disfmarker} and , uh {disfmarker} uh , synthesize some speech from it . phd a: When are his prelims ? professor b: And then {disfmarker} Um , I think in about , um , a little less than two weeks . phd a: Oh . Wow . professor b: Yeah . Yeah . So . Uh , it might even be sooner . Uh , let 's see , this is the sixteenth , seventeenth ? Yeah , I don't know if he 's before {disfmarker} It might even be in a week . phd a: So , I professor b: A week , phd a: Huh . I {disfmarker} I guessed that they were gonna do it some time during the semester professor b: week and a half . phd a: but they 'll do it any time , huh ? professor b: They seem to be {disfmarker} Well , the semester actually is starting up . phd a: Is it already ? professor b: Yeah , the semester 's late {disfmarker} late August they start here . phd a: Yikes . professor b: So they do it right at the beginning of the semester . phd a: Yeah . professor b: Yeah . So , uh {disfmarker} Yep . I mean , that {disfmarker} that was sort of one {disfmarker} I mean , the overall results seemed to be first place in {disfmarker} in {disfmarker} in the case of either , um , artificial reverberation or a modest sized training set . Uh , either way , uh , i uh , it helped a lot . And {disfmarker} But if you had a {disfmarker} a really big training set , a recognizer , uh , system that was capable of taking advantage of a really large training set {disfmarker} I thought that {disfmarker} One thing with the HTK is that is has the {disfmarker} as we 're using {disfmarker} the configuration we 're using is w s is {disfmarker} being bound by the terms of Aurora , we have all those parameters just set as they are . So even if we had a hundred times as much data , we wouldn't go out to , you know , ten or t or a hundred times as many Gaussians or anything . So , um , it 's kind of hard to take advantage of {disfmarker} of {disfmarker} of big chunks of data . Uh , whereas the other one does sort of expand as you have more training data . phd c: Mm - hmm . phd d: Mmm , yeah . professor b: It does it automatically , actually . And so , um , uh , that one really benefited from the larger set . And it was also a diverse set with different noises and so forth . Uh , so , um , that , uh {disfmarker} that seemed to be {disfmarker} So , if you have that {disfmarker} that better recognizer that can {disfmarker} that can build up more parameters , and if you , um , have the natural room , which in this case has a p a pretty bad signal - to - noise ratio , then in that case , um , the right thing to do is just do {disfmarker} u use speaker adaptation . And {disfmarker} and not bother with {disfmarker} with this acoustic , uh , processing . But I think that that would not be true if we did some explicit noise - processing as well as , uh , the convolutional kind of things we were doing . phd c: Mm - hmm . professor b: So . That 's sort of what we found . phd d: Hmm . phd a: I , um {disfmarker} {vocalsound} uh , started working on the uh {disfmarker} Mississippi State recognizer . So , I got in touch with Joe and {disfmarker} and , uh , from your email and things like that . phd d: Oh , OK . phd a: And , uh , they added me to the list {disfmarker} uh , the mailing list . phd d: OK , great . phd a: And he gave me all of the pointers and everything that I needed . And so I downloaded the , um {disfmarker} There were two things , uh , that they had to download . One was the , uh , I guess the software . And another wad {disfmarker} was a , um , sort of like a sample {disfmarker} a sample run . So I downloaded the software and compiled all of that . And it compiled fine . phd d: Eight . phd a: No problems . phd d: Oh , eh , great . phd a: And , um , I grabbed the sample stuff but I haven't , uh , compiled it . phd d: That sample was released only yesterday or the day before , right ? phd a: No {disfmarker} Well , I haven't grabbed that one yet . So there 's two . phd d: Oh , there is another short sample set {disfmarker} phd a: There was another short one , yeah . phd d: o o sample . phd a: And so I haven't grabbed the latest one that he just , uh , put out yet . phd d: OK . Oh , OK . F Yeah , OK . phd a: So . Um , but , the software seemed to compile fine and everything , so . And , um , So . professor b: Is there any word yet about the issues about , um , adjustments for different feature sets or anything ? phd a: No , I {disfmarker} I d You asked me to write to him and I think I forgot to ask him about that . Or if I did ask him , he didn't reply . professor b: Yeah . phd a: I {disfmarker} I don't remember yet . Uh , I 'll {disfmarker} I 'll d I 'll double check that and ask him again . professor b: Yeah . Yeah , it 's like that {disfmarker} that could r turn out to be an important issue for us . phd d: Hmm . Mmm . phd a: Yeah . Yeah . professor b: Yeah . phd d: Cuz they have it {disfmarker} phd a: Maybe I 'll send it to the list . Yeah . phd d: Cuz they have , uh , already frozen those in i insertion penalties and all those stuff is what {disfmarker} I feel . Because they have this document explaining the recognizer . phd a: Uh - huh . phd d: And they have these tables with , uh , various language model weights , insertion penalties . phd a: OK , I haven't seen that one yet . phd d: u phd a: So . phd d: Uh , it 's th it 's there on that web . phd a: OK . phd d: And , uh , on that , I mean , they have run some experiments using various insertion penalties and all those {disfmarker} phd a: And so they 've picked {disfmarker} the values . phd d: Yeah , I think they pi p phd a: Oh , OK . phd d: yeah , they picked the values from {disfmarker} phd a: OK . professor b: For r w what test set ? phd d: Uh , p the one that they have reported is a NIST evaluation , Wall Street Journal . professor b: But that has nothing to do with what we 're testing on , right ? phd c: Mm - hmm . phd d: You know . No . So they 're , like {disfmarker} um {disfmarker} So they are actually trying to , uh , fix that {disfmarker} those values using the clean , uh , training part of the Wall Street Journal . Which is {disfmarker} I mean , the Aurora . Aurora has a clean subset . professor b: Right . phd d: I mean , they want to train it and then this {disfmarker} they 're going to run some evaluations . professor b: So they 're set they 're setting it based on that ? phd d: Yeah . professor b: OK . So now , we may come back to the situation where we may be looking for a modification of the features to account for the fact that we can't modify these parameters . phd a: Yeah . professor b: But , um , phd d: Yeah . professor b: uh {disfmarker} but it 's still worth , I think , just {disfmarker} since {disfmarker} you know , just chatting with Joe about the issue . phd a: Yeah , OK . Do you think that 's something I should just send to him professor b: Um {disfmarker} phd a: or do you think I should send it to this {disfmarker} there 's an {disfmarker} a m a mailing list . professor b: Well , it 's not a secret . I mean , we 're , you know , certainly willing to talk about it with everybody , but I think {disfmarker} I think that , um {disfmarker} um , it 's probably best to start talking with him just to {disfmarker} phd a: OK . professor b: Uh @ @ {comment} you know , it 's a dialogue between two of you about what {disfmarker} you know , what does he think about this and what {disfmarker} what {disfmarker} you know {disfmarker} what could be done about it . phd a: Yeah . OK . professor b: Um , if you get ten people in {disfmarker} involved in it there 'll be a lot of perspectives based on , you know , how {disfmarker} phd a: Yeah . professor b: you know . phd a: Right . professor b: Uh {disfmarker} But , I mean , I think it all should come up eventually , phd a: OK . professor b: but if {disfmarker} if {disfmarker} if there is any , uh , uh , way to move in {disfmarker} a way that would {disfmarker} that would , you know , be more open to different kinds of features . But if {disfmarker} if , uh {disfmarker} if there isn't , and it 's just kind of shut down and {disfmarker} and then also there 's probably not worthwhile bringing it into a larger forum where {disfmarker} where political issues will come in . phd a: Yeah . OK . phd d: Oh . So this is now {disfmarker} it 's {disfmarker} it 's compiled under Solaris ? phd a: Yeah . phd d: Yeah , OK . phd a: Yep . phd d: Because he {disfmarker} there was some mail r saying that it 's {disfmarker} may not be stable for Linux and all those . phd a: Yeah . Yeah , i that was a particular version . phd d: SUSI phd a: Yeah , SUSI or whatever it was phd d: yeah . Yeah , yeah . phd a: but we don't have that . phd d: Yeah , OK . phd a: So . Should be OK . phd d: OK , that 's fine . phd a: Yeah , it compiled fine actually . phd d: Yeah . phd a: No {disfmarker} no errors . Nothing . So . professor b: Uh , this is slightly off topic phd d: That 's good . professor b: but , uh , I noticed , just glancing at the , uh , Hopkins workshop , uh , web site that , uh , um {disfmarker} one of the thing I don't know {disfmarker} Well , we 'll see how much they accomplish , but one of the things that they were trying to do in the graphical models thing was to put together a {disfmarker} a , uh , tool kit for doing , uh r um , arbitrary graphical models for , uh , speech recognition . phd a: Hmm . professor b: So {disfmarker} And Jeff , uh {disfmarker} the two Jeffs were phd a: Who 's the second Jeff ? professor b: Uh {disfmarker} Oh , uh , do you know Geoff Zweig ? phd a: No . professor b: Oh . Uh , he {disfmarker} he , uh {disfmarker} he was here for a couple years phd a: Oh , OK . professor b: and he , uh {disfmarker} got his PHD . He {disfmarker} And he 's , uh , been at IBM for the last couple years . phd a: Oh , OK . professor b: So . phd a: Wow . That would be neat . professor b: Uh , so he did {disfmarker} he did his PHD on dynamic Bayes - nets , uh , for {disfmarker} for speech recognition . He had some continuity built into the model , presumably to handle some , um , inertia in the {disfmarker} in the production system , and , um {disfmarker} phd a: Hmm . professor b: So . phd d: Hmm . phd c: Um , I 've been playing with , first , the , um , VAD . Um , {vocalsound} so it 's exactly the same approach , but the features that the VAD neural network use are , uh , MFCC after noise compensation . Oh , I think I have the results . professor b: What was it using before ? phd c: Before it was just P L phd d:  phd c: So . phd d: Yeah , it was actually {disfmarker} No . Not {disfmarker} I mean , it was just the noisy features I guess . phd c: Yeah , phd d: Yeah , yeah , yeah , phd c: noisy {disfmarker} noisy features . phd d: not compensated . phd c: Um {disfmarker} This is what we get after {disfmarker} This {disfmarker} So , actually , we , yeah , here the features are noise compensated and there is also the LDA filter . Um , and then it 's a pretty small neural network which use , um , {vocalsound} nine frames of {disfmarker} of six features from C - zero to C - fives , plus the first derivatives . And it has one hundred hidden units . phd a: Is that nine frames u s uh , centered around the current frame ? Or {disfmarker} phd c: Yeah . Mm - hmm . professor b: S so , I 'm {disfmarker} I 'm sorry , there 's {disfmarker} there 's {disfmarker} there 's how many {disfmarker} how many inputs ? phd c: So it 's twelve times nine . professor b: Twelve times nine inputs , and a hundred , uh , hidden . phd c: Hidden and phd d: Two outputs . phd c: two outputs . professor b: Two outputs . OK . So I guess about eleven thousand parameters , which {disfmarker} actually shouldn't be a problem , even in {disfmarker} in small phones . Yeah . phd c: Mm - hmm . phd a: So , I 'm {disfmarker} I 'm {disfmarker} s so what is different between this and {disfmarker} and what you {disfmarker} phd c: It should be OK . So the previous syst It 's based on the system that has a fifty - three point sixty - six percent improvement . It 's the same system . The only thing that changed is the n a p eh {disfmarker} a es the estimation of the silence probabilities . phd a: Ah . OK . phd c: Which now is based on , uh , cleaned features . professor b: And , it 's a l it 's a lot better . phd a: Wow . phd c: Yeah . professor b: That 's great . phd c: Um {disfmarker} So it 's {disfmarker} it 's not bad , but the problem is still that the latency is too large . professor b: What 's the latency ? phd c: Because {disfmarker} um {disfmarker} the {disfmarker} the latency of the VAD is two hundred and twenty milliseconds . And , uh , the VAD is used uh , i for on - line normalization , and it 's used before the delta computation . So if you add these components it goes t to a hundred and seventy , right ? professor b: I {disfmarker} I 'm confused . You started off with two - twenty and you ended up with one - seventy ? phd c: With two an two hundred and seventy . professor b: Two - seventy . phd c: If {disfmarker} Yeah , if you add the c delta comp delta computation professor b: Oh . phd c: which is done afterwards . Um {disfmarker} professor b: So it 's two - twenty . I the is this {disfmarker} are these twenty - millisecond frames ? Is that why ? Is it after downsampling ? or {disfmarker} phd c: The two - twenty is one hundred milliseconds for the um {disfmarker} No , it 's forty milliseconds for t for the , uh , uh , cleaning of the speech . Um {disfmarker} then there is , um , the neural network which use nine frames . So it adds forty milliseconds . professor b: a OK . phd c: Um , after that , um , you have the um , filtering of the silence probabilities . Which is a million filter it , and it creates a one hundred milliseconds delay . So , um {disfmarker} professor b:  phd d: Plus there is a delta at the input . phd c: Yeah , and there is the delta at the input which is , professor b: One hundred milliseconds for smoothing . phd c: um {disfmarker} So it 's {disfmarker} @ @ {disfmarker} professor b: Uh , median . phd c:  phd d: It 's like forty plus {disfmarker} forty {disfmarker} plus {disfmarker} professor b: And then forty {disfmarker} phd c: Mmm . Forty {disfmarker} This forty plus twenty , plus one hundred . professor b: forty p  phd c: Uh {disfmarker} phd d: So it 's two hundred actually . phd c: Yeah , there are twenty that comes from {disfmarker} There is ten that comes from the LDA filters also . Right ? phd d: Oh , OK . phd c: Uh , so it 's two hundred and ten , yeah . phd d: If you are using {disfmarker} professor b: Uh {disfmarker} phd c: Plus the frame , phd d: t If you are using three frames {disfmarker} phd c: so it 's two - twenty . phd d: If you are phrasing f {comment} using three frames , it is thirty here for delta . phd c: Yeah , I think it 's {disfmarker} it 's five frames , but . phd d: So five frames , that 's twenty . OK , so it 's who un {comment} two hundred and ten . professor b: Uh , p Wait a minute . It 's forty {disfmarker} {vocalsound} forty for the {disfmarker} for the cleaning of the speech , phd c: So . Forty cleaning . professor b: forty for the I N {disfmarker} ANN , a hundred for the smoothing . phd c: Yeah . professor b: Well , but at ten {disfmarker} , phd c: Twenty for the delta . professor b: Twenty for delta . phd d: At th {nonvocalsound} At the input . I mean , that 's at the input to the net . phd c: Yeah . professor b: Delta at input to net ? phd d: And there i phd c: Yeah . phd d: Yeah . So it 's like s five , six cepstrum plus delta at nine {disfmarker} nine frames of {disfmarker} professor b: And then ten milliseconds for {disfmarker} phd d: Fi - There 's an LDA filter . professor b: ten milliseconds for LDA filter , and t and ten {disfmarker} another ten milliseconds you said for the frame ? phd c: For the frame I guess . I computed two - twenty {disfmarker} Yeah , well , it 's {disfmarker} I guess it 's for the fr {disfmarker} the {disfmarker} professor b: OK . And then there 's delta besides that ? phd c: So this is the features that are used by our network and then afterwards , you have to compute the delta on the , uh , main feature stream , professor b: OK . phd c: which is um , delta and double - deltas , which is fifty milliseconds . professor b: Yeah . No , I mean , the {disfmarker} after the noise part , the forty {disfmarker} the {disfmarker} the other hundred and eighty {disfmarker} Well , I mean , Wait a minute . Some of this is , uh {disfmarker} is , uh {disfmarker} is in parallel , isn't it ? I mean , the LDA {disfmarker} Oh , you have the LDA as part of the V D - uh , VAD ? Or {disfmarker} phd c: The VAD use , uh , LDA filtered features also . professor b: Oh , it does ? phd c: Mm - hmm . professor b: Ah . So in that case there isn't too much in parallel . Uh {disfmarker} phd c: No . There is , um , just downsampling , upsampling , and the LDA . professor b: Um , so the delta at the end is how much ? phd c: It 's fifty . phd d: It 's {disfmarker} professor b: Fifty . Alright . So {disfmarker} phd c: But well , we could probably put the delta , um , {vocalsound} before on - line normalization . It should not that make a big difference , phd a: What if you used a smaller window for the delta ? phd c: because {disfmarker} phd a: Could that help a little bit ? I mean , I guess there 's a lot of things you could do to {disfmarker} phd c: Yeah . professor b: Yeah . phd c: Yeah , professor b: So phd c: but , nnn {disfmarker} professor b: Yeah . So if you {disfmarker} if you put the delta before the , uh , ana on - line {disfmarker} If {disfmarker} Yeah {disfmarker} phd c: Mm - hmm . professor b: uh {disfmarker} then {disfmarker} then it could go in parallel . phd c: Cuz i professor b: And then y then you don't have that additive {disfmarker} phd c: Yeah , phd d: Yep . phd c: cuz the time constant of the on - line normalization is pretty long compared to the delta window , professor b: OK . phd c: so . It should not make {disfmarker} professor b: OK . And you ought to be able to shove tw , uh {disfmarker} sh uh {disfmarker} pull off twenty milliseconds from somewhere else to get it under two hundred , right ? I mean {disfmarker} phd a: Is two hundred the d professor b: The hundred milla phd c: Mm - hmm . professor b: mill a hundred milliseconds for smoothing is sort of an arbitrary amount . It could be eighty and {disfmarker} and probably do @ @ {disfmarker} phd c: Yeah , phd a: i a hun phd c: yeah . phd a: uh {disfmarker} Wh - what 's the baseline you need to be under ? Two hundred ? professor b: Well , we don't know . They 're still arguing about it . phd c:  phd a: Oh . professor b: I mean , if it 's two {disfmarker} if {disfmarker} if it 's , uh {disfmarker} if it 's two - fifty , then we could keep the delta where it is if we shaved off twenty . If it 's two hundred , if we shaved off twenty , we could {disfmarker} we could , uh , meet it by moving the delta back . phd a: So , how do you know that what you have is too much if they 're still deciding ? professor b: Uh , we don't , but it 's just {disfmarker} I mean , the main thing is that since that we got burned last time , and {disfmarker} you know , by not worrying about it very much , we 're just staying conscious of it . phd a: Uh - huh . Oh , OK , I see . professor b: And so , th I mean , if {disfmarker} if {disfmarker} if a week before we have to be done someone says , " Well , you have to have fifty milliseconds less than you have now " , it would be pretty frantic around here . So {disfmarker} phd a: Ah , OK . professor b: Uh {disfmarker} phd a: But still , that 's {disfmarker} that 's a pretty big , uh , win . And it doesn't seem like you 're {disfmarker} in terms of your delay , you 're , uh , that {disfmarker} professor b: He added a bit on , I guess , because before we were {disfmarker} we were {disfmarker} had {disfmarker} were able to have the noise , uh , stuff , uh , and the LVA be in parallel . phd c: Hmm . professor b: And now he 's {disfmarker} he 's requiring it to be done first . phd c: Well , but I think the main thing , maybe , is the cleaning of the speech , which takes forty milliseconds or so . professor b: Right . Well , so you say {disfmarker} let 's say ten milliseconds {disfmarker} seconds for the LDA . phd c: And {disfmarker} and {disfmarker} but {disfmarker} the LDA is , well , pretty short right now . professor b: Well , ten . And then forty for the other . phd c: Yeah . phd d: Yeah , the LDA {disfmarker} LDA {disfmarker} we don't know , is , like {disfmarker} is it very crucial for the features , right ? phd c: No . I just {disfmarker} This is the first try . phd d: Yeah . professor b: Right , phd c: I mean , I {disfmarker} maybe the LDA 's not very useful then . professor b: so you could start pulling back , phd d: S s h professor b: but {disfmarker} phd d: Yeah , professor b: But I think you have {disfmarker} phd d: l professor b: I mean , you have twenty for delta computation which y now you 're sort of doing twice , right ? But yo w were you doing that before ? phd c: Mmm . Well , in the proposal , um , the input of the VAD network were just three frames , I think . phd d: On the {disfmarker} in the {disfmarker} Mm - hmm . Just {disfmarker} Yeah , just the static , no delta . professor b: Right . phd c: Uh , static features . professor b: So , what you have now is fort uh , forty for the {disfmarker} the noise , twenty for the delta , and ten for the LDA . That 's seventy milliseconds of stuff which was formerly in parallel , phd c:  professor b: right ? So I think , phd c: Mm - hmm . professor b: you know , that 's {disfmarker} that 's the difference as far as the timing , right ? phd c: Yeah . professor b: Um , and you could experiment with cutting various pieces of these back a bit , but {disfmarker} I mean , we 're s we 're not {disfmarker} we 're not in terrible shape . phd a: Yeah , that 's what it seems like to me . It 's pretty good . professor b: Yeah . phd c: Mm - hmm . professor b: It 's {disfmarker} it 's not like it 's adding up to four hundred milliseconds or something . phd a: Where {disfmarker} where is this {disfmarker} where is this fifty - seven point O two in {disfmarker} in comparison to the last evaluation ? professor b: Well , it 's {disfmarker} I think it 's better than anything , uh , anybody got . phd a: Oh , is that right ? phd c: Yeah . The best was fifty - four point five . professor b: Yeah . phd d: Point s phd a: Oh . professor b: Yeah . Uh phd c: And our system was forty - nine , but with the neural network . phd a: Wow . So this is almost ten percent . professor b: With the f with the neural net . Yeah , and r and {disfmarker} phd c: It would phd d: Yeah , so this is {disfmarker} this is like the first proposal . The proposal - one . It was forty - four , actually . professor b: Yeah . Yeah . And we still don't have the neural net in . So {disfmarker} so it 's {disfmarker} phd a: Wow . professor b: You know . So it 's {disfmarker} We 're {disfmarker} we 're doing better . phd a: This is {disfmarker} this is really good . professor b: I mean , we 're getting better recognition . I mean , I 'm sure other people working on this are not sitting still either , but {disfmarker} phd a: Yeah . professor b: but {disfmarker} but , uh {disfmarker} Uh , I mean , the important thing is that we learn how to do this better , and , you know . So . Um , Yeah . So , our , um {disfmarker} Yeah , you can see the kind of {disfmarker} kind of numbers that we 're having , say , on SpeechDat - Car which is a hard task , cuz it 's really , um {disfmarker} I think it 's just sort of {disfmarker} sort of reasonable numbers , starting to be . I mean , it 's still terri phd c: Mm - hmm . Yeah , even for a well - matched case it 's sixty percent error rate reduction , professor b: Yeah . phd c: which is {disfmarker} professor b: Yeah . Probably half . Good ! phd c: Um , Yeah . So actually , this is in between {vocalsound} what we had with the previous VAD and what Sunil did with an IDL VAD . Which gave sixty - two percent improvement , right ? phd d: Yeah , it 's almost that . phd c: So {disfmarker} phd d: It 's almost an average somewhere around {disfmarker} phd c: Yeah . phd d: Yeah . phd a: What was that ? Say that last part again ? phd c: So , if you use , like , an IDL VAD , uh , for dropping the frames , phd d: o o Or the best we can get . phd c: the best that we can get {disfmarker} i That means that we estimate the silence probability on the clean version of the utterances . Then you can go up to sixty - two percent error rate reduction , globally . phd a: Mmm . phd c: Mmm {disfmarker} Yeah . phd a: So that would be even {disfmarker} That wouldn't change this number down here to sixty - two ? phd c: Yeah . professor b: Yeah . So you {disfmarker} you were get phd c: If you add a g good v very good VAD , that works as well as a VAD working on clean speech , phd a: Yeah . Yeah . phd c: then you wou you would go {disfmarker} phd a: So that 's sort of the best you could hope for . phd c: Mm - hmm . phd a: I see . professor b: Probably . Yeah . So fi si fifty - three is what you were getting with the old VAD . phd c: Yeah . professor b: And , uh {disfmarker} and sixty - two with the {disfmarker} the , you know , quote , unquote , cheating VAD . And fifty - seven is what you got with the real VAD . phd c: Mm - hmm . professor b: That 's great . phd c: Uh , yeah , the next thing is , I started to play {disfmarker} Well , I don't want to worry too much about the delay , no . Maybe it 's better to wait professor b: OK . phd c: for the decision professor b: Yeah . phd c: from the committee . Uh , but I started to play with the , um , {vocalsound} {vocalsound} uh , tandem neural network . Mmm I just did the configuration that 's very similar to what we did for the February proposal . And {disfmarker} Um . So . There is a f a first feature stream that use uh straight MFCC features . professor b: Mm - hmm . phd c: Well , these features actually . And the other stream is the output of a neural network , using as input , also , these , um , cleaned MFCC . Um {disfmarker} phd a: Those are th those are th what is going into the tandem net ? phd c: I don't have the comp Mmm ? phd a: Those two ? phd c: So there is just this feature stream , {comment} the fifteen MFCC plus delta and double - delta . professor b: No . phd a: Yeah ? phd c: Um , so it 's {disfmarker} makes forty - five features {comment} that are used as input to the HTK . And then , there is {disfmarker} there are more inputs that comes from the tandem MLP . phd a: Oh , oh . OK . I see . professor b: Yeah , h he likes to use them both , phd a: Uh - huh . professor b: cuz then it has one part that 's discriminative , phd c: Yeah . Um {disfmarker} professor b: one part that 's not . phd a: Right . OK . phd c: So , um , uh , yeah . Right now it seems that {disfmarker} i I just tested on SpeechDat - Car while the experiment are running on your {disfmarker} on TI - digits . Well , it improves on the well - matched and the mismatched conditions , but it get worse on the highly mismatched . Um , phd a: Compared to these numbers ? phd c: Compared to these numbers , yeah . Um , professor b: y phd c: like , on the well - match and medium mismatch , the gain is around five percent relative , but it goes down a lot more , like fifteen percent on the HM case . professor b: You 're just using the full ninety features ? phd c:  The {disfmarker} professor b: Y you have ninety features ? phd c: i I have , um {disfmarker} From the networks , it 's twenty - eight . So {disfmarker} professor b: And from the other side it 's forty - five . phd c: So , d i It 's forty - five . professor b: So it 's {disfmarker} you have seventy - three features , phd c: Yeah . professor b: and you 're just feeding them like that . phd c: Yeah . professor b: There isn't any KLT or anything ? phd c: Mm - hmm . There 's a KLT after the neural network , as {disfmarker} as before . phd a: That 's how you get down to twenty - eight ? phd c: Yeah . phd a: Why twenty - eight ? phd c: I don't know . phd a: Oh . phd c: Uh . It 's {disfmarker} i i i It 's because it 's what we did for the first proposal . We tested , uh , trying to go down phd a: Ah . professor b: It 's a multiple of seven . phd c: and Yeah . phd d: Yeah . phd c: So {disfmarker} Um . phd d: Yeah . phd c: I wanted to do something very similar to the proposal as a first {disfmarker} first try . phd d: Yeah . phd a: I see . professor b: Yeah . phd a: Yeah . That makes sense . phd c: But we have to {disfmarker} for sure , we have to go down , because the limit is now sixty features . professor b: Yeah . phd c: So , uh , we have to find a way to decrease the number of features . Um {disfmarker} phd a: So , it seems funny that {disfmarker} I don't know , maybe I don't u quite understand everything , {comment} but that adding features {disfmarker} I guess {disfmarker} I guess if you 're keeping the back - end fixed . Maybe that 's it . Because it seems like just adding information shouldn't give worse results . But I guess if you 're keeping the number of Gaussians fixed in the recognizer , then {disfmarker} professor b: Well , yeah . phd c: Mmm . professor b: But , I mean , just in general , adding information {disfmarker} Suppose the information you added , well , was a really terrible feature and all it brought in was noise . phd a: Yeah . professor b: Right ? So {disfmarker} so , um {disfmarker} Or {disfmarker} or suppose it wasn't completely terrible , but it was completely equivalent to another one feature that you had , except it was noisier . phd a: Uh - huh . professor b: Right ? In that case you wouldn't necessarily expect it to be better at all . phd a: Oh , yeah , I wasn't necessarily saying it should be better . I 'm just surprised that you 're getting fifteen percent relative worse on the wel professor b: Uh - huh . phd c: But it 's worse . professor b: On the highly mismatched condition . phd a: On the highly mismatch . phd c: Yeah , I {disfmarker} phd a: Yeah . professor b: So , " highly mismatched condition " means that in fact your training is a bad estimate of your test . phd c: Uh - huh . professor b: So having {disfmarker} having , uh , a g a l a greater number of features , if they aren't maybe the right features that you use , certainly can e can easily , uh , make things worse . I mean , you 're right . If you have {disfmarker} if you have , uh , lots and lots of data , and you have {disfmarker} and your {disfmarker} your {disfmarker} your training is representative of your test , then getting more sources of information should just help . But {disfmarker} but it 's {disfmarker} It doesn't necessarily work that way . phd a: Huh . phd c: Mm - hmm . professor b: So I wonder , um , Well , what 's your {disfmarker} what 's your thought about what to do next with it ? phd c: Um , I don't know . I 'm surprised , because I expected the neural net to help more when there is more mismatch , as it was the case for the {disfmarker} professor b: Mm - hmm . phd d: So , was the training set same as the p the February proposal ? OK . phd c: Yeah , it 's the same training set , so it 's TIMIT with the TI - digits ' , uh , noises , uh , added . phd d:  professor b: Mm - hmm . phd c: Um {disfmarker} professor b: Well , we might {disfmarker} uh , we might have to experiment with , uh better training sets . Again . But , phd c: Mm - hmm . professor b: I {disfmarker} The other thing is , I mean , before you found that was the best configuration , but you might have to retest those things now that we have different {disfmarker} The rest of it is different , right ? So , um , uh , For instance , what 's the effect of just putting the neural net on without the o other {disfmarker} other path ? phd c: Mm - hmm . professor b: I mean , you know what the straight features do . phd c: Yeah . professor b: That gives you this . You know what it does in combination . phd c: Mm - hmm . professor b: You don't necessarily know what {disfmarker} phd a: What if you did the {disfmarker} Would it make sense to do the KLT on the full set of combined features ? Instead of just on the {disfmarker} phd c: Yeah . I g I guess . Um . The reason I did it this ways is that in February , it {disfmarker} we {disfmarker} we tested different things like that , so , having two KLT , having just a KLT for a network , or having a global KLT . phd a: Oh , I see . phd c: And {disfmarker} phd a: So you tried the global KLT before phd c: Well {disfmarker} phd a: and it didn't really {disfmarker} phd c: Yeah . And , uh , th Yeah . phd a: I see . phd c: The differences between these configurations were not huge , but {disfmarker} it was marginally better with this configuration . phd a: Uh - huh . Uh - huh . professor b: But , yeah , that 's obviously another thing to try , phd c: Um . professor b: since things are {disfmarker} things are different . phd c: Mm - hmm . Mm - hmm . professor b: And I guess if the {disfmarker} These are all {disfmarker} so all of these seventy - three features are going into , um , the , uh {disfmarker} the HMM . phd c: Yeah . professor b: And is {disfmarker} are {disfmarker} i i are {disfmarker} are any deltas being computed of tha of them ? phd c: Of the straight features , yeah . professor b: n Not of the {disfmarker} phd c: So . But n th the , um , tandem features are u used as they are . professor b: Are not . phd c: So , yeah , maybe we can add some context from these features also as {disfmarker} Dan did in {disfmarker} in his last work . professor b: Could . i Yeah , but the other thing I was thinking was , um {disfmarker} Uh , now I lost track of what I was thinking . But . phd a: What is the {disfmarker} You said there was a limit of sixty features or something ? phd c: Mm - hmm . phd a: What 's the relation between that limit and the , um , forty - eight {disfmarker} uh , forty eight hundred bits per second ? professor b: Oh , I know what I was gonna say . phd c: Um , not {disfmarker} no relation . professor b: No relation . phd a: So I {disfmarker} I {disfmarker} I don't understand , phd c: The f the forty - eight hundred bits is for transmission of some features . phd a: because i I mean , if you 're only using h phd c: And generally , i it {disfmarker} s allows you to transmit like , fifteen , uh , cepstrum . professor b: The issue was that , um , this is supposed to be a standard that 's then gonna be fed to somebody 's recognizer somewhere which might be , you know , it {disfmarker} it might be a concern how many parameters are use {disfmarker} u used and so forth . And so , uh , they felt they wanted to set a limit . So they chose sixty . Some people wanted to use hundreds of parameters and {disfmarker} and that bothered some other people . phd a: Uh - huh . professor b: u And so they just chose that . I {disfmarker} I {disfmarker} I think it 's kind of r arbitrary too . But {disfmarker} but that 's {disfmarker} that 's kind of what was chosen . I {disfmarker} I remembered what I was going to say . What I was going to say is that , um , maybe {disfmarker} {vocalsound} maybe with the noise removal , uh , these things are now more correlated . So you have two sets of things that are kind of uncorrelated , uh , within themselves , but they 're pretty correlated with one another . phd c: Mm - hmm . professor b: And , um , they 're being fed into these , uh , variants , only Gaussians and so forth , and {disfmarker} and , uh , phd c: Mm - hmm . professor b: so maybe it would be a better idea now than it was before to , uh , have , uh , one KLT over everything , to de - correlate it . phd c: Mm - hmm . Yeah , I see . professor b: Maybe . You know . phd d: What are the S N Rs in the training set , TIMIT ? phd c: It 's , uh , ranging from zero to clean ? Yeah . From zero to clean . phd d: Mm - hmm . professor b: Yeah . So we found this {disfmarker} this , uh {disfmarker} this Macrophone data , and so forth , that we were using for these other experiments , to be pretty good . phd c: Mm - hmm . professor b: So that 's {disfmarker} i after you explore these other alternatives , that might be another way to start looking , is {disfmarker} is just improving the training set . phd c: Mm - hmm . professor b: I mean , we were getting , uh , lots better recognition using that , than {disfmarker} Of course , you do have the problem that , um , u i {comment} we are not able to increase the number of Gaussians , uh , or anything to , uh , uh , to match anything . So we 're only improving the training of our feature set , but that 's still probably something . phd a: So you 're saying , add the Macrophone data to the training of the neural net ? The tandem net ? professor b: Yeah , that 's the only place that we can train . phd a: Yeah . professor b: We can't train the other stuff with anything other than the standard amount , phd a: Right . professor b: so . Um , um {disfmarker} phd a: What {disfmarker} what was it trained on again ? The one that you used ? phd c: It 's TIMIT with noise . phd a: Uh - huh . professor b: Yeah . phd c: So , yeah , it 's rather a small {disfmarker} professor b: How big is the net , by the way ? phd c: Um , Uh , it 's , uh , five hundred hidden units . And {disfmarker} professor b: And again , you did experiments back then where you made it bigger and it {disfmarker} and that was {disfmarker} that was sort of the threshold point . Much less than that , it was worse , phd c: Yeah . professor b: and phd c: Yeah . professor b: much more than that , it wasn't much better . Hmm . phd c: Yeah . @ @ ? phd d: So is it {disfmarker} is it though the performance , big relation in the high ma high mismatch has something to do with the , uh , cleaning up that you {disfmarker} that is done on the TIMIT after adding noise ? phd c:  phd d: So {disfmarker} it 's {disfmarker} i All the noises are from the TI - digits , phd c: Yeah . phd d: right ? So you {disfmarker} i phd c: Um {disfmarker} They {disfmarker} k uh {disfmarker} phd d: Well , it it 's like the high mismatch of the SpeechDat - Car after cleaning up , maybe having more noise than the {disfmarker} the training set of TIMIT after clean {disfmarker} s after you do the noise clean - up . phd c: Mmm . phd d: I mean , earlier you never had any compensation , you just trained it straight away . phd c: Mm - hmm . phd d: So it had like all these different conditions of S N Rs , actually in their training set of neural net . phd c: Mm - hmm . Mm - hmm . phd d: But after cleaning up you have now a different set of S N Rs , right ? phd c: Yeah . phd d: For the training of the neural net . phd c: Mm - hmm . phd d: And {disfmarker} is it something to do with the mismatch that {disfmarker} that 's created after the cleaning up , like the high mismatch {disfmarker} phd c: You mean the {disfmarker} the most noisy occurrences on SpeechDat - Car might be a lot more noisy than {disfmarker} phd d: Mm - hmm . Of {disfmarker} that {disfmarker} I mean , the SNR after the noise compensation of the SpeechDat - Car . professor b: Oh , so {disfmarker} Right . So the training {disfmarker} the {disfmarker} the neural net is being trained with noise compensated stuff . phd c: Maybe . phd d:  Yeah . phd c: Yeah , yeah . professor b: Which makes sense , phd d: Yeah . professor b: but , uh , you 're saying {disfmarker} Yeah , the noisier ones are still going to be , even after our noise compensation , are still gonna be pretty noisy . phd d: Yeah . phd c: Mm - hmm . phd d: Yeah , so now the after - noise compensation the neural net is seeing a different set of S N Rs than that was originally there in the training set . Of TIMIT . Because in the TIMIT it was zero to some clean . professor b: Right . Yes . phd d: So the net saw all the SNR @ @ conditions . professor b: Right . phd d: Now after cleaning up it 's a different set of SNR . professor b: Right . phd d: And that SNR may not be , like , com covering the whole set of S N Rs that you 're getting in the SpeechDat - Car . professor b: Right , but the SpeechDat - Car data that you 're seeing is also reduced in noise by the noise compensation . phd c: Yeah . phd d: Yeah , yeah , yeah , yeah , it is . But , I 'm saying , there could be some {disfmarker} some issues of {disfmarker} professor b: So . phd c: Mm - hmm . professor b: Yeah . phd c: Well , if the initial range of SNR is different , we {disfmarker} the problem was already there before . And {disfmarker} professor b: Yeah . phd c: Because {disfmarker} Mmm {disfmarker} professor b: Yeah , I mean , it depends on whether you believe that the noise compensation is equally reducing the noise on the test set and the training set . phd c: Hmm . professor b: Uh {disfmarker} phd d: On the test set , yeah .  professor b: Right ? I mean , you 're saying there 's a mismatch in noise that wasn't there before , phd d: Hmm . Mm - hmm . professor b: but if they were both the same before , then if they were both reduic reduced equally , then , there would not be a mismatch . phd d: Mm - hmm . professor b: So , I mean , this may be {disfmarker} Heaven forbid , this noise compensation process may be imperfect , but . Uh , so maybe it 's treating some things differently . phd c: Yeah , uh {disfmarker} phd d: Well , I {disfmarker} I don't know . I {disfmarker} I just {disfmarker} that could be seen from the TI - digits , uh , testing condition because , um , the noises are from the TI - digits , right ? Noise {disfmarker} phd c: Yeah . So {disfmarker} phd d: So cleaning up the TI - digits and if the performance goes down in the TI - digits mismatch {disfmarker} high mismatch like this {disfmarker} phd c: Clean training , yeah . phd d: on a clean training , or zero DB testing . phd c: Yeah , we 'll {disfmarker} so we 'll see . Uh . phd d: Yeah . phd c: Maybe . phd d: Then it 's something to do . phd c: Mm - hmm . professor b: I mean , one of the things about {disfmarker} phd c: Yeah . professor b: I mean , the Macrophone data , um , I think , you know , it was recorded over many different telephones . phd c: Mm - hmm . professor b: And , um , so , there 's lots of different kinds of acoustic conditions . I mean , it 's not artificially added noise or anything . So it 's not the same . I don't think there 's anybody recording over a car from a car , but {disfmarker} I think it 's {disfmarker} it 's varied enough that if {disfmarker} if doing this adjustments , uh , and playing around with it doesn't , uh , make it better , the most {disfmarker} uh , it seems like the most obvious thing to do is to improve the training set . Um {disfmarker} I mean , what we were {disfmarker} uh {disfmarker} the condition {disfmarker} It {disfmarker} it gave us an enormous amount of improvement in what we were doing with Meeting Recorder digits , even though there , again , these m Macrophone digits were very , very different from , uh , what we were going on here . I mean , we weren't talking over a telephone here . But it was just {disfmarker} I think just having a {disfmarker} a nice variation in acoustic conditions was just a good thing . phd c: Mm - hmm . Yep . phd d: Mmm . phd c: Yeah , actually {vocalsound} to s eh , what I observed in the HM case is that the number of deletion dramatically increases . It {disfmarker} it doubles . professor b: Number of deletions . phd c: When I added the num the neural network it doubles the number of deletions . Yeah , so I don't you know {vocalsound} how to interpret that , but , mmm {disfmarker} professor b: Yeah . Me either . phd c: t phd a: And {disfmarker} and did {disfmarker} an other numbers stay the same ? Insertion substitutions stay the same ? phd c: They p stayed the same , phd a: Roughly ? phd c: they {disfmarker} maybe they are a little bit uh , lower . phd a: Uh - huh . phd c: They are a little bit better . Yeah . But {disfmarker} professor b: Did they increase the number of deletions even for the cases that got better ? phd c: Mm - hmm . professor b: Say , for the {disfmarker} I mean , it {disfmarker} phd c: No , it doesn't . professor b: So it 's only the highly mismatched ? phd c: No . professor b: And it {disfmarker} Remind me again , the " highly mismatched " means that the {disfmarker} phd c: Clean training and {disfmarker} professor b: Uh , sorry ? phd c: It 's clean training {disfmarker} Well , close microphone training and distant microphone , um , high speed , I think . professor b: Close mike training {disfmarker} phd c: Well {disfmarker} The most noisy cases are the distant microphone for testing . professor b: Right . So {disfmarker} Well , maybe the noise subtraction is subtracting off speech . phd c: Separating . Yeah . professor b: Wh phd c: But {disfmarker} Yeah . I mean , but without the neural network it 's {disfmarker} well , it 's better . It 's just when we add the neural networks . professor b: Yeah , right . phd c: The feature are the same except that {disfmarker} professor b: Uh , that 's right , that 's right . Um {disfmarker} phd a: Well that {disfmarker} that says that , you know , the , um {disfmarker} the models in {disfmarker} in , uh , the recognizer are really paying attention to the neural net features . phd c: Yeah . phd a: Uh . phd c: Mm - hmm . professor b: But , yeah , actually {disfmarker} {nonvocalsound} the TIMIT noises {pause} are sort of a range of noises and they 're not so much the stationary driving kind of noises , right ? It 's {disfmarker} it 's pretty different . Isn't it ? phd c: Uh , there is a car noise . So there are f just four noises . Um , uh , " Car " , I think , " Babble " , phd d: " Babble . " phd c: " Subway " , right ? and {disfmarker} phd d: " Street " or " Airport " or something . phd c: and {disfmarker} " Street " isn't {disfmarker} phd d: Or " Train station " . phd c: " Train station " , yeah . phd d: Yeah . phd c: So {disfmarker} it 's mostly {disfmarker} Well , " Car " is stationary , professor b: Mm - hmm . phd c: " Babble " , it 's a stationary background plus some voices , professor b: Mm - hmm . phd c: some speech over it . And the other two are rather stationary also . professor b: Well , I {disfmarker} I think that if you run it {disfmarker} Actually , you {disfmarker} maybe you remember this . When you {disfmarker} in {disfmarker} in the old experiments when you ran with the neural net only , and didn't have this side path , um , uh , with the {disfmarker} the pure features as well , did it make things better to have the neural net ? phd c: Mm - hmm . professor b: Was it about the same ? Uh , w i phd c: It was {disfmarker} b a little bit worse . professor b: Than {disfmarker} ? phd c: Than just the features , yeah . professor b: So , until you put the second path in with the pure features , the neural net wasn't helping at all . phd c: Mm - hmm . professor b: Well , that 's interesting . phd c: It was helping , uh , if the features are b were bad , professor b: Yeah . phd c: I mean . Just plain P L Ps or M F professor b: Yeah . phd c: C Cs . as soon as we added LDA on - line normalization , and {vocalsound} all these things , then {disfmarker} professor b: They were doing similar enough things . Well , I still think it would be k sort of interesting to see what would happen if you just had the neural net without the side thing . phd c: Yeah , professor b: And {disfmarker} and the thing I {disfmarker} I have in mind is , uh , maybe you 'll see that the results are not just a little bit worse . phd c: mm - hmm . professor b: Maybe that they 're a lot worse . You know ? And , um {disfmarker} But if on the ha other hand , uh , it 's , say , somewhere in between what you 're seeing now and {disfmarker} and {disfmarker} and , uh , what you 'd have with just the pure features , then maybe there is some problem of a {disfmarker} of a , uh , combination of these things , or correlation between them somehow . phd c: Mm - hmm . professor b: If it really is that the net is hurting you at the moment , then I think the issue is to focus on {disfmarker} on , uh , improving the {disfmarker} the net . phd c: Yeah , professor b: Um . phd c: mm - hmm . professor b: So what 's the overall effe I mean , you haven't done all the experiments but you said it was i somewhat better , say , five percent better , for the first two conditions , and fifteen percent worse for the other one ? But it 's {disfmarker} but of course that one 's weighted lower , phd c: Y yeah , oh . Yeah . professor b: so I wonder what the net effect is . phd c: I d I {disfmarker} I think it 's {disfmarker} it was one or two percent . That 's not that bad , but it was l like two percent relative worse on SpeechDat - Car . I have to {disfmarker} to check that . Well , I have {disfmarker} I will . phd d: Well , it will {disfmarker} overall it will be still better even if it is fifteen percent worse , because the fifteen percent worse is given like f w twenty - five {disfmarker} point two five eight . professor b: Right . phd c: Mm - hmm . Hmm . professor b: Right . So the {disfmarker} so the worst it could be , if the others were exactly the same , is four , phd d: Is it like {disfmarker} professor b: and {disfmarker} and , uh , in fact since the others are somewhat better {disfmarker} phd d: Yeah , so it 's four . Is i So either it 'll get cancelled out , or you 'll get , like , almost the same . professor b: Uh . phd c: Yeah , it was {disfmarker} it was slightly worse . phd d: Slightly bad . Yeah . phd c: Um , professor b: Yeah , it should be pretty close to cancelled out . phd d: Yeah . phd a: You know , I 've been wondering about something . phd c: Mm - hmm . phd a: In the , um {disfmarker} a lot of the , um {disfmarker} the Hub - five systems , um , recently have been using LDA . and {disfmarker} and they , um {disfmarker} They run LDA on the features right before they train the models . So there 's the {disfmarker} the LDA is {disfmarker} is right there before the H M phd d: Yeah . phd a: So , you guys are using LDA but it seems like it 's pretty far back in the process . phd d: Uh , this LDA is different from the LDA that you are talking about . The LDA that you {disfmarker} saying is , like , you take a block of features , like nine frames or something , {comment} and then do an LDA on it , phd a: Yeah . Uh - huh . phd d: and then reduce the dimensionality to something like twenty - four or something like that . phd a: Yeah , you c you c you can . phd d: And then feed it to HMM . phd a: I mean , it 's {disfmarker} you know , you 're just basically i phd d: Yeah , so this is like a two d two dimensional tile . phd a: You 're shifting the feature space . Yeah . phd d: So this is a two dimensional tile . And the LDA that we are f applying is only in time , not in frequency {disfmarker} high cost frequency . So it 's like {disfmarker} more like a filtering in time , rather than doing a r phd a: Ah . OK . So what i what about , um {disfmarker} i u what i w I mean , I don't know if this is a good idea or not , but what if you put {disfmarker} ran the other kind of LDA , uh , on your features right before they go into the HMM ? phd d: Uh , it {disfmarker} phd c: Mm - hmm . No , actually , I think {disfmarker} i phd d: m phd c: Well . What do we do with the ANN is {disfmarker} is something like that except that it 's not linear . But it 's {disfmarker} it 's like a nonlinear discriminant analysis . phd a: Yeah . Right , it 's the {disfmarker} It 's {disfmarker} Right . The {disfmarker} So {disfmarker} Yeah , so it 's sort of like {disfmarker} phd c: But . phd a: The tandem stuff is kind of like i nonlinear LDA . phd c: Yeah . It 's {disfmarker} phd a: I g phd c: Yeah . phd a: Yeah . professor b: Yeah . phd a: But I mean , w but the other features that you have , um , th the non - tandem ones , phd c: Uh . Mm - hmm . Yeah , I know . That {disfmarker} that {disfmarker} Yeah . Well , in the proposal , they were transformed u using PCA , but {disfmarker} phd a: Uh - huh . phd c: Yeah , it might be that LDA could be better . professor b: The a the argument i is kind of i in {disfmarker} and it 's not like we really know , but the argument anyway is that , um , uh , we always have the prob I mean , discriminative things are good . LDA , neural nets , they 're good . phd a: Yeah . professor b: Uh , they 're good because you {disfmarker} you {disfmarker} you learn to distinguish between these categories that you want to be good at distinguishing between . And PCA doesn't do that . It {disfmarker} PAC - PCA {disfmarker} low - order PCA throws away pieces that are uh , maybe not {disfmarker} not gonna be helpful just because they 're small , basically . phd a: Right . professor b: But , uh , the problem is , training sets aren't perfect and testing sets are different . So you f you {disfmarker} you face the potential problem with discriminative stuff , be it LDA or neural nets , that you are training to discriminate between categories in one space but what you 're really gonna be g getting is {disfmarker} is something else . phd a: Uh - huh . professor b: And so , uh , Stephane 's idea was , uh , let 's feed , uh , both this discriminatively trained thing and something that 's not . So you have a good set of features that everybody 's worked really hard to make , phd a: Yeah . professor b: and then , uh , you {disfmarker} you discriminately train it , but you also take the path that {disfmarker} that doesn't have that , phd a: Uh - huh . professor b: and putting those in together . And that {disfmarker} that seem So it 's kind of like a combination of the {disfmarker} uh , what , uh , Dan has been calling , you know , a feature {disfmarker} uh , you know , a feature combination versus posterior combination or something . It 's {disfmarker} it 's , you know , you have the posterior combination but then you get the features from that and use them as a feature combination with these {disfmarker} these other things . And that seemed , at least in the last one , as he was just saying , he {disfmarker} he {disfmarker} when he only did discriminative stuff , i it actually was {disfmarker} was {disfmarker} it didn't help at all in this particular case . phd a: Yeah . professor b: There was enough of a difference , I guess , between the testing and training . But by having them both there {disfmarker} The fact is some of the time , the discriminative stuff is gonna help you . phd a: Mm - hmm . professor b: And some of the time it 's going to hurt you , phd a: Right . professor b: and by combining two information sources if , you know {disfmarker} if {disfmarker} if {disfmarker} phd a: So you wouldn't necessarily then want to do LDA on the non - tandem features because now you 're doing something to them that {disfmarker} professor b: That i i I think that 's counter to that idea . phd a: Yeah , right . professor b: Now , again , it 's {disfmarker} we 're just trying these different things . We don't really know what 's gonna work best . But if that 's the hypothesis , at least it would be counter to that hypothesis to do that . phd a: Right . professor b: Um , and in principle you would think that the neural net would do better at the discriminant part than LDA . phd a: Right . Yeah . Well {disfmarker} y professor b: Though , maybe not . phd a: Yeah . Exactly . I mean , we , uh {disfmarker} we were getting ready to do the tandem , uh , stuff for the Hub - five system , and , um , Andreas and I talked about it , and the idea w the thought was , " Well , uh , yeah , that i you know {disfmarker} th the neural net should be better , but we should at least have uh , a number , you know , to show that we did try the LDA in place of the neural net , so that we can you know , show a clear path . professor b: Right . phd a: You know , that you have it without it , then you have the LDA , then you have the neural net , and you can see , theoretically . So . I was just wondering {disfmarker} I {disfmarker} I {disfmarker} professor b: Well , I think that 's a good idea . phd a: Yeah . professor b: Did {disfmarker} did you do that phd a: Um . No . professor b: or {disfmarker} tha that 's a {disfmarker} phd a: That 's what {disfmarker} that 's what we 're gonna do next as soon as I finish this other thing . So . professor b: Yeah . Yeah . No , well , that 's a good idea . I {disfmarker} I {disfmarker} phd a: We just want to show . professor b: i Yeah . phd a: I mean , it {disfmarker} everybody believes it , professor b: Oh , no it 's a g phd a: but you know , we just {disfmarker} professor b: No , no , but it might not {disfmarker} not even be true . phd a: Yeah . professor b: I mean , it 's {disfmarker} it 's {disfmarker} it 's {disfmarker} it 's {disfmarker} it 's a great idea . I mean , one of the things that always disturbed me , uh , in the {disfmarker} the resurgence of neural nets that happened in the eighties was that , um , a lot of people {disfmarker} Because neural nets were pretty easy to {disfmarker} to use {disfmarker} a lot of people were just using them for all sorts of things without , uh , looking at all into the linear , uh {disfmarker} uh , versions of them . phd a: Yeah . Mm - hmm . Yeah . professor b: And , uh , people were doing recurrent nets but not looking at IIR filters , and {disfmarker} You know , I mean , uh , so I think , yeah , it 's definitely a good idea to try it . phd a: Yeah , and everybody 's putting that on their {vocalsound} systems now , and so , I that 's what made me wonder about this , professor b: Well , they 've been putting them in their systems off and on for ten years , phd a: but . professor b: but {disfmarker} but {disfmarker} but , uh , phd a: Yeah , what I mean is it 's {disfmarker} it 's like in the Hub - five evaluations , you know , and you read the system descriptions and everybody 's got , {vocalsound} you know , LDA on their features . professor b: And now they all have that . I see . phd a: And so . professor b: Yeah . phd a: Uh . phd c: It 's the transformation they 're estimating on {disfmarker} Well , they are trained on the same data as the final HMM are . phd a: Yeah , so it 's different . Yeah , exactly . Cuz they don't have these , you know , mismatches that {disfmarker} that you guys have . phd c: Mm - hmm . phd a: So that 's why I was wondering if maybe it 's not even a good idea . phd c: Mm - hmm . phd a: I don't know . I {disfmarker} I don't know enough about it , phd c: Mm - hmm . phd a: but {disfmarker} Um . professor b: I mean , part of why {disfmarker} I {disfmarker} I think part of why you were getting into the KLT {disfmarker} Y you were describing to me at one point that you wanted to see if , uh , you know , getting good orthogonal features was {disfmarker} and combining the {disfmarker} the different temporal ranges {disfmarker} was the key thing that was happening or whether it was this discriminant thing , right ? So you were just trying {disfmarker} I think you r I mean , this is {disfmarker} it doesn't have the LDA aspect but th as far as the orthogonalizing transformation , you were trying that at one point , right ? phd c: Mm - hmm . professor b: I think you were . phd c: Mm - hmm . Yeah . professor b: Does something . It doesn't work as well . Yeah . Yeah . phd d: So , yeah , I 've been exploring a parallel VAD without neural network with , like , less latency using SNR and energy , um , after the cleaning up . So what I 'd been trying was , um , uh {disfmarker} After the b after the noise compensation , n I was trying t to f find a f feature based on the ratio of the energies , that is , cl after clean and before clean . So that if {disfmarker} if they are , like , pretty c close to one , which means it 's speech . And if it is n if it is close to zero , which is {disfmarker} So it 's like a scale @ @ probability value . So I was trying , uh , with full band and multiple bands , m ps uh {disfmarker} separating them to different frequency bands and deriving separate decisions on each bands , and trying to combine them . Uh , the advantage being like it doesn't have the latency of the neural net if it {disfmarker} if it can professor b: Mm - hmm . phd d: g And {pause} it gave me like , uh , one point {disfmarker} One {disfmarker} more than one percent relative improvement . So , from fifty - three point six it went to fifty f four point eight . So it 's , like , only slightly more than a percent improvement , professor b: Mm - hmm . phd d: just like {disfmarker} Which means that it 's {disfmarker} it 's doing a slightly better job than the previous VAD , professor b: Mm - hmm . phd d: uh , at a l lower delay . professor b: Mm - hmm . phd d: Um , so , um {disfmarker} professor b: But {disfmarker} i d I 'm sorry , phd d: so {disfmarker} u professor b: does it still have the median {pause} filter stuff ? phd d: It still has the median filter . professor b: So it still has most of the delay , phd d: So {disfmarker} professor b: it just doesn't {disfmarker} phd d: Yeah , so d with the delay , that 's gone is the input , which is the sixty millisecond . The forty plus {pause} twenty . professor b: Well , w i phd d: At the input of the neural net you have this , uh , f nine frames of context plus the delta . professor b: Oh , plus the delta , phd c: Mm - hmm . professor b: right . OK . phd d: Yeah . So that delay , plus the LDA . professor b: Mm - hmm . phd d: Uh , so the delay is only the forty millisecond of the noise cleaning , plus the hundred millisecond smoothing at the output . professor b: Mm - hmm . Mm - hmm . phd d: Um . So . Yeah . So the {disfmarker} the {disfmarker} di the biggest {disfmarker} The problem f for me was to find a consistent threshold that works {pause} well across the different databases , because I t I try to make it work on tr SpeechDat - Car professor b: Mm - hmm . phd d: and it fails on TI - digits , or if I try to make it work on that it 's just the Italian or something , it doesn't work on the Finnish . professor b: Mm - hmm . phd d: So , um . So there are {disfmarker} there was , like , some problem in balancing the deletions and insertions when I try different thresholds . professor b: Mm - hmm . phd d: So {disfmarker} The {disfmarker} I 'm still trying to make it better by using some other features from the {disfmarker} after the p clean up {disfmarker} maybe , some , uh , correlation {disfmarker} auto - correlation or some s additional features of {disfmarker} to mainly the improvement of the VAD . I 've been trying . professor b: Now this {disfmarker} this {disfmarker} this , uh , " before and after clean " , it sounds like you think that 's a good feature . That {disfmarker} that , it {disfmarker} you th think that the , uh {disfmarker} the {disfmarker} i it appears to be a good feature , right ? phd d: Mm - hmm . professor b: What about using it in the neural net ? phd d: Yeah . phd c: Yeah , eventually we could {disfmarker} could just phd d: Yeah , so {disfmarker} Yeah , so that 's the {disfmarker} Yeah . So we 've been thinking about putting it into the neural net also . professor b: Yeah . phd d: Because they did {disfmarker} that itself {disfmarker} phd c: Then you don't have to worry about the thresholds and {disfmarker} phd d: There 's a threshold and {disfmarker} Yeah . professor b: Yeah . phd c: but just {disfmarker} phd d: Yeah . So that {disfmarker} that 's , uh {disfmarker} professor b: Yeah . So if we {disfmarker} if we can live with the latency or cut the latencies elsewhere , then {disfmarker} then that would be a , uh , good thing . phd d: Yeah . Yeah . professor b: Um , anybody {disfmarker} has anybody {disfmarker} you guys or {disfmarker} or Naren , uh , somebody , tried the , uh , um , second th second stream thing ? Uh . phd d: Oh , I just {disfmarker} I just h put the second stream in place and , uh ran one experiment , but just like {disfmarker} just to know that everything is fine . professor b: Uh - huh . phd d: So it was like , uh , forty - five cepstrum plus twenty - three mel {disfmarker} log mel . professor b: Yeah . phd d: And {disfmarker} and , just , like , it gave me the baseline performance of the Aurora , which is like zero improvement . professor b: Yeah . Yeah . phd d: So I just tried it on Italian just to know that everything is {disfmarker} But I {disfmarker} I didn't export anything out of it because it was , like , a weird feature set . professor b: Yeah . phd d: So . professor b: Yeah . Well , what I think , you know , would be more what you 'd want to do is {disfmarker} is {disfmarker} is , uh , put it into another neural net . Right ? phd c: Mm - hmm . phd d: Yeah , yeah , yeah , yeah . professor b: And then {disfmarker} But , yeah , we 're {disfmarker} we 're not quite there yet . So we have to {vocalsound} figure out the neural nets , I guess . phd c: Yeah . phd d: The uh , other thing I was wondering was , um , if the neural net , um , has any {disfmarker} because of the different noise con unseen noise conditions for the neural net , where , like , you train it on those four noise conditions , while you are feeding it with , like , a additional {disfmarker} some four plus some {disfmarker} f few more conditions which it hasn't seen , actually , phd c: Mm - hmm . phd d: from the {disfmarker} f f while testing . phd c: Yeah , yeah . Right . phd d: Um {disfmarker} instead of just h having c uh , those cleaned up t cepstrum , sh should we feed some additional information , like {disfmarker} The {disfmarker} the {disfmarker} We have the VAD flag . I mean , should we f feed the VAD flag , also , at the input so that it {disfmarker} it has some additional discriminating information at the input ? phd c: Hmm - hmm ! Um {disfmarker} professor b: Wh - uh , the {disfmarker} the VAD what ? phd d: We have the VAD information also available at the back - end . professor b: Uh - huh . phd d: So if it is something the neural net is not able to discriminate the classes {disfmarker} professor b: Yeah . phd d: I mean {disfmarker} Because most of it is sil I mean , we have dropped some silence f We have dropped so silence frames ? professor b: Mm - hmm . phd d: No , we haven't dropped silence frames still . phd c: Uh , still not . Yeah . phd d: Yeah . So {disfmarker} phd c: Th phd d: the b b biggest classification would be the speech and silence . So , by having an additional , uh , feature which says " this is speech and this is nonspeech " , I mean , it certainly helps in some unseen noise conditions for the neural net . phd a: What {disfmarker} Do y do you have that feature available for the test data ? phd d: Well , I mean , we have {disfmarker} we are transferring the VAD to the back - end {disfmarker} feature to the back - end . Because we are dropping it at the back - end after everything {disfmarker} all the features are computed . phd a: Oh , oh , I see . phd d: So {disfmarker} phd a: I see . phd d: so the neural {disfmarker} so that is coming from a separate neural net or some VAD . phd a: OK . OK . phd d: Which is {disfmarker} which is certainly giving a phd a: So you 're saying , feed that , also , into {pause} the neural net . phd d: to {disfmarker} Yeah . So it it 's an {disfmarker} additional discriminating information . phd a: Yeah . Yeah . Right . phd d: So that {disfmarker} professor b: You could feed it into the neural net . The other thing {comment} you could do is just , um , p modify the , uh , output probabilities of the {disfmarker} of the , uh , uh , um , neural net , tandem neural net , {comment} based on the fact that you have a silence probability . phd d: Mm - hmm . professor b: Right ? phd c: Mm - hmm . professor b: So you have an independent estimator of what the silence probability is , and you could multiply the two things , and renormalize . phd c: Yeah . professor b: Uh , I mean , you 'd have to do the nonlinearity part and deal with that . Uh , I mean , go backwards from what the nonlinearity would , you know {disfmarker} would be . phd d: Through {disfmarker} t to the soft max . professor b: But {disfmarker} but , uh {disfmarker} phd c: Yeah , so {disfmarker} maybe , yeah , when {disfmarker} phd a: But in principle wouldn't it be better to feed it in ? And let the net do that ? professor b: Well , u Not sure . phd a: Hmm . professor b: I mean , let 's put it this way . I mean , y you {disfmarker} you have this complicated system with thousands and thousand parameters phd a: Yeah . professor b: and you can tell it , uh , " Learn this thing . " Or you can say , " It 's silence ! Go away ! " I mean , I mean , i Doesn't {disfmarker} ? I think {disfmarker} I think the second one sounds a lot more direct . phd a: What {disfmarker} what if you {disfmarker} professor b: Uh . phd a: Right . So , what if you then , uh {disfmarker} since you know this , what if you only use the neural net on the speech portions ? professor b: Well , uh , phd c: That 's what {disfmarker} phd a: Well , I guess that 's the same . Uh , that 's similar . professor b: Yeah , I mean , y you 'd have to actually run it continuously , phd a: But I mean {disfmarker} I mean , train the net only on {disfmarker} professor b: but it 's {disfmarker} @ @ {disfmarker} Well , no , you want to train on {disfmarker} on the nonspeech also , because that 's part of what you 're learning in it , to {disfmarker} to {disfmarker} to generate , that it 's {disfmarker} it has to distinguish between . phd d: Speech . phd a: But I mean , if you 're gonna {disfmarker} if you 're going to multiply the output of the net by this other decision , uh , would {disfmarker} then you don't care about whether the net makes that distinction , right ? professor b: Well , yeah . But this other thing isn't perfect . phd a: Ah . professor b: So that you bring in some information from the net itself . phd a: Right , OK . That 's a good point . professor b: Yeah . Now the only thing that {disfmarker} that bothers me about all this is that I {disfmarker} I {disfmarker} I {disfmarker} The {disfmarker} the fact {disfmarker} i i It 's sort of bothersome that you 're getting more deletions . phd c: Yeah . But {disfmarker} So I might maybe look at , is it due to the fact that um , the probability of the silence at the output of the network , is , uh , professor b: Is too high . phd c: too {disfmarker} too high or {disfmarker} professor b: Yeah . So maybe {disfmarker} So {disfmarker} phd c: If it 's the case , then multiplying it again by {disfmarker} i by something ? phd d: It may not be {disfmarker} it {disfmarker} professor b: Yeah . phd c: Mm - hmm . phd d: Yeah , it {disfmarker} it may be too {disfmarker} it 's too high in a sense , like , everything is more like a , um , flat probability . professor b: Yeah . phd c: Oh - eee - hhh . phd d: So , like , it 's not really doing any distinction between speech and nonspeech {disfmarker} phd c: Uh , yeah . phd d: or , I mean , different {disfmarker} among classes . professor b: Yeah . phd c: Mm - hmm . phd a: Be interesting to look at the {disfmarker} Yeah , for the {disfmarker} I wonder if you could do this . But if you look at the , um , highly mism high mismat the output of the net on the high mismatch case and just look at , you know , the distribution versus the {disfmarker} the other ones , do you {disfmarker} do you see more peaks or something ? phd c: Yeah . Yeah , like the entropy of the {disfmarker} the output , phd a: Yeah . professor b: Yeah , for instance . phd c: or {disfmarker} professor b: But I {disfmarker} bu phd c: It {disfmarker} it seems that the VAD network doesn't {disfmarker} Well , it doesn't drop , uh , too many frames because the dele the number of deletion is reasonable . But it 's just when we add the tandem , the final MLP , and then {disfmarker} professor b: Yeah . Now the only problem is you don't want to ta I guess wait for the output of the VAD before you can put something into the other system , phd c: u professor b: cuz that 'll shoot up the latency a lot , right ? Am I missing something here ? phd c: But {disfmarker} phd d: Mm - hmm . phd c: Yeah . Right . professor b: Yeah . So that 's maybe a problem with what I was just saying . But {disfmarker} but {disfmarker} I I guess {disfmarker} phd a: But if you were gonna put it in as a feature it means you already have it by the time you get to the tandem net , right ? phd d: Um , well . We {disfmarker} w we don't have it , actually , professor b: No . phd d: because it 's {disfmarker} it has a high rate energy {disfmarker} phd a: Ah . phd d: the VAD has a {disfmarker} professor b: Yeah . phd a: OK . professor b: It 's kind of done in {disfmarker} I mean , some of the things are , not in parallel , but certainly , it would be in parallel with the {disfmarker} with a tandem net . phd a: Right . professor b: In time . So maybe , if that doesn't work , um {disfmarker} But it would be interesting to see if that was the problem , anyway . And {disfmarker} and {disfmarker} and then I guess another alternative would be to take the feature that you 're feeding into the VAD , and feeding it into the other one as well . phd c: Mm - hmm . professor b: And then maybe it would just learn {disfmarker} learn it better . phd c: Mm - hmm . professor b: Um {disfmarker} But that 's {disfmarker} Yeah , that 's an interesting thing to try to see , if what 's going on is that in the highly mismatched condition , it 's , um , causing deletions by having this silence probability up {disfmarker} up too high , phd c: Mm - hmm . professor b: at some point where the VAD is saying it 's actually speech . phd c: Yeah . professor b: Which is probably true . phd c: So , m professor b: Cuz {disfmarker} Well , the V A if the VAD said {disfmarker} since the VAD is {disfmarker} is {disfmarker} is right a lot , uh {disfmarker} phd c: Yeah . professor b: Hmm . Anyway . Might be . phd c: Mm - hmm . professor b: Yeah . Well , we just started working with it . But these are {disfmarker} these are some good ideas I think . phd c: Mm - hmm . Yeah , and the other thing {disfmarker} Well , there are other issues maybe for the tandem , like , uh , well , do we want to , w uh n Do we want to work on the targets ? Or , like , instead of using phonemes , using more context dependent units ? phd a: For the tandem net you mean ? phd c: Well , I 'm {disfmarker} Yeah . phd a: Hmm . phd c: I 'm thinking , also , a w about Dan 's work where he {disfmarker} he trained {vocalsound} a network , not on phoneme targets but on the HMM state targets . And {disfmarker} it was giving s slightly better results . professor b: Problem is , if you are going to run this on different m test sets , including large vocabulary , phd c: Yeah . Yeah . professor b: um , phd c: Uh {disfmarker} professor b: I think {disfmarker} phd c: Mmm . I was just thinking maybe about , like , generalized diphones , and {disfmarker} come up with a {disfmarker} a reasonable , not too large , set of context dependent units , and {disfmarker} and {disfmarker} Yeah . And then anyway we would have to reduce this with the KLT . professor b: Yeah . phd c: So . But {disfmarker} I don't know . professor b: Yeah . Well , maybe . But I d I d it {disfmarker} it {disfmarker} i it 's all worth looking at , phd c: Mm - hmm . professor b: but it sounds to me like , uh , looking at the relationship between this and the {disfmarker} speech noise stuff is {disfmarker} is {disfmarker} is probably a key thing . phd c: Mm - hmm . professor b: That and the correlation between stuff . phd a: So if , uh {disfmarker} if the , uh , high mismatch case had been more like the , uh , the other two cases {comment} in terms of giving you just a better performance , {comment} how would this number have changed ? phd c: Mm - hmm . Oh , it would be {disfmarker} Yeah . Around five percent better , I guess . If {disfmarker} if {disfmarker} i phd a: y Like sixty ? professor b: Well , we don't know what 's it 's gonna be the TI - digits yet . He hasn't got the results back yet . phd c: Yeah . If you extrapolate the SpeechDat - Car well - matched and medium - mismatch , it 's around , yeah , maybe five . phd a: Uh - huh . Yeah . So this would be sixty - two ? professor b: Sixty - two . phd a: Which is {disfmarker} professor b: Yeah . phd c: Sixty - two , yeah . phd d: Somewhere around sixty , must be . Right ? Yeah . phd c: Well , it 's around five percent , because it 's {disfmarker} s Right ? If everything is five percent . phd d: Yeah . Yeah . phd a: All the other ones were five percent , phd c: Mm - hmm . phd a: the {disfmarker} professor b: Yeah . phd c: I d I d I just have the SpeechDat - Car right now , so {disfmarker} phd a: Yeah . phd c: It 's running {disfmarker} it shou we should have the results today during the afternoon , phd a: Hmm . phd c: but {disfmarker} Well . professor b: Hmm . Well {disfmarker} Um {disfmarker} So I won't be here for {disfmarker} phd a: When {disfmarker} When do you leave ? professor b: Uh , I 'm leaving next Wednesday . May or may not be in in the morning . I leave in the afternoon . Um , phd a: But you 're {disfmarker} professor b: so I {disfmarker} phd a: are you {disfmarker} you 're not gonna be around this afternoon ? professor b: Yeah . phd a: Oh . professor b: Oh , well . I 'm talking about next week . I 'm leaving {disfmarker} leaving next Wednesday . phd a: Uh - huh . professor b: This afternoon {disfmarker} uh {disfmarker} Oh , right , for the Meeting meeting ? Yeah , that 's just cuz of something on campus . phd a: Ah , OK , OK . professor b: Yeah . But , um , yeah , so next week I won't , and the week after I won't , cuz I 'll be in Finland . And the week after that I won't . By that time you 'll be {disfmarker} {comment} Uh , you 'll both be gone {pause} from here . So there 'll be no {disfmarker} definitely no meeting on {disfmarker} on September sixth . Uh , phd a: What 's September sixth ? professor b: and {disfmarker} Uh , that 's during Eurospeech . phd a: Oh , oh , right . OK . professor b: So , uh , Sunil will be in Oregon . Uh , Stephane and I will be in Denmark . Uh {disfmarker} Right ? So it 'll be a few weeks , really , before we have a meeting of the same cast of characters . Um , but , uh {disfmarker} I guess , just {disfmarker} I mean , you guys should probably meet . And maybe Barry {disfmarker} Barry will be around . And {disfmarker} and then uh , uh , we 'll start up again with Dave and {disfmarker} Dave and Barry and Stephane and us on the , uh , twentieth . No . Thirteenth ? About a month ? phd a: So , uh , you 're gonna be gone for the next three weeks or something ? professor b: I 'm gone for two and a half weeks starting {disfmarker} starting next Wed - late next Wednesday . phd a: So that 's {disfmarker} you won't be at the next three of these meetings . Is that right ? professor b: Uh , I won't {disfmarker} it 's probably four because of {disfmarker} is it three ? Let 's see , twenty - third , thirtieth , sixth . That 's right , next three . And the {disfmarker} the third one won't {disfmarker} probably won't be a meeting , cuz {disfmarker} cuz , uh , Su - Sunil , Stephane , and I will all not be here . phd a: Oh , right . Right . professor b: Um {disfmarker} Mmm . {comment} So it 's just , uh , the next two where there will be {disfmarker} there , you know , may as well be meetings , phd a: OK . professor b: but I just won't be at them . And then starting up on the thirteenth , {nonvocalsound} uh , we 'll have meetings again but we 'll have to do without Sunil here somehow . phd a: When do you go back ? professor b: So . phd d: Thirty - first , August . professor b: Yeah . Yeah . So . Cool . phd a: When is the evaluation ? November , or something ? professor b: Yeah , it was supposed to be November fifteenth . Has anybody heard anything different ? phd c: I don't know . The meeting in {disfmarker} is the five and six of December . So {disfmarker} phd d: p s It 's like {disfmarker} Yeah , it 's tentatively all full . Yeah . phd c: Mm - hmm . phd d: Uh , that 's a proposed date , I guess . phd c: Yeah , um {disfmarker} so the evaluation should be on a week before or {disfmarker} phd a: Yeah . professor b: Yep . But , no , this is good progress . So . Uh {disfmarker} OK . phd a: Should we do digits ? professor b: Guess we 're done . Digits ? Yep . phd a: OK . professor b: It 's a wrap .