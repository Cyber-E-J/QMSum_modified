phd b: OK . We 're on . grad e: Hello ? professor a: OK , so uh {vocalsound} had some interesting mail from uh Dan Ellis . Actually , I think he {disfmarker} he {vocalsound} redirected it to everybody also so uh {vocalsound} the PDA mikes uh have a big bunch of energy at {disfmarker} at uh five hertz uh where this came up was that uh I was showing off these wave forms that we have on the web and {disfmarker} and uh {vocalsound} I just sort of hadn't noticed this , but that {disfmarker} the major , major component in the wave {disfmarker} in the second wave form in that pair of wave forms is actually the air conditioner . grad c: Huh . professor a: So . So . I {vocalsound} {vocalsound} I have to be more careful about using that as a {disfmarker} as a {disfmarker} {vocalsound} as a good illustration , uh , in fact it 's not , of uh {disfmarker} {vocalsound} of the effects of room reverberation . It is isn't a bad illustration of the effects of uh room noise . {vocalsound} on {disfmarker} on uh some mikes uh but So . And then we had this other discussion about um {vocalsound} whether this affects the dynamic range , cuz I know , although we start off with thirty two bits , you end up with uh sixteen bits and {vocalsound} you know , are we getting hurt there ? But uh Dan is pretty confident that we 're not , that {disfmarker} that quantization error is not {disfmarker} is still not a significant {vocalsound} factor there . So . So there was a question of whether we should change things here , whether we should {vocalsound} change a capacitor on the input box for that or whether we should phd b: Yeah , he suggested a smaller capacitor , right ? professor a: Right . But then I had some other uh thing discussions with him phd b: For the P D professor a: and the feeling was {vocalsound} once we start monk monkeying with that , uh , many other problems could ha happen . And additionally we {disfmarker} we already have a lot of data that 's been collected with that , so . phd b: Yeah . professor a: A simple thing to do is he {disfmarker} he {disfmarker} he has a {disfmarker} I forget if it {disfmarker} this was in that mail or in the following mail , but he has a {disfmarker} a simple filter , a digital filter that he suggested . We just run over the data before we deal with it . phd b: Mm - hmm . professor a: um The other thing that I don't know the answer to , but when people are using Feacalc here , uh whether they 're using it with the high - pass filter option or not . And I don't know if anybody knows . grad e: Um . {vocalsound} I could go check . professor a: But . Yeah . So when we 're doing all these things using our software there is {disfmarker} um if it 's {disfmarker} if it 's based on the RASTA - PLP program , {vocalsound} which does both PLP and RASTA - PLP {vocalsound} um then {vocalsound} uh there is an option there which then comes up through to Feacalc which {vocalsound} um allows you to do high - pass filtering and in general we like to do that , because of things like this and {vocalsound} it 's {disfmarker} it 's pretty {disfmarker} it 's not a very severe filter . Doesn't affect speech frequencies , even pretty low speech frequencies , at all , but it 's phd b: What 's the {pause} cut - off frequency it used ? professor a: Oh . I don't know I wrote this a while ago phd b: Is it like twenty ? professor a: Something like that . phd b: Yeah . professor a: Yeah . I mean I think there 's some effect above twenty but it 's {disfmarker} it 's {disfmarker} it 's {disfmarker} it 's mild . So , I mean it probably {disfmarker} there 's probably some effect up to a hundred hertz or something but it 's {disfmarker} it 's pretty mild . I don't know in the {disfmarker} in the STRUT implementation of the stuff is there a high - pass filter or a pre pre - emphasis or something in the {disfmarker} phd f: Uh . I think we use a pre - emphasis . Yeah . Yeah . professor a: So . We {disfmarker} we {disfmarker} we want to go and check that in i for anything that we 're going to use the P D A mike for . {vocalsound} uh He says that there 's a pretty good roll off in the PZM mikes so {vocalsound} we don't need {disfmarker} need to worry about them one way or the other but if we do make use of the cheap mikes , {vocalsound} uh we want to be sure to do that {disfmarker} that filtering before we {vocalsound} process it . And then again if it 's uh depending on the option that the {disfmarker} our {disfmarker} our software is being run with , it 's {disfmarker} it 's quite possible that 's already being taken care of . uh But I also have to pick a different picture to show the effects of reverberation . uh phd b: Did somebody notice it during your talk ? professor a: uh No . phd b: Huh . professor a: Well . uh Well . If they made output they were {disfmarker} they were , you know {disfmarker} they were nice . phd b: Didn't say anything ? professor a: But . {vocalsound} I mean the thing is it was since I was talking about reverberation and showing this thing that was noise , it wasn't a good match , but it certainly was still uh an indication of the fact that you get noise with distant mikes . uh It 's just not a great example because not only isn't it reverberation but it 's a noise that we definitely know what to do . phd b: Mm - hmm . professor a: So , I mean , it doesn't take deep {disfmarker} {vocalsound} a new {disfmarker} bold new methods to get rid of uh five hertz noise , so . phd b: Yeah . professor a: um {vocalsound} uh But . So it was {disfmarker} it was a bad example in that way , but it 's {disfmarker} it still is {disfmarker} it 's the real thing that we did get out of the microphone at distance , so it wasn't {vocalsound} it w it w wasn't wrong it was inappropriate . So . {vocalsound} So uh , but uh , Yeah , someone noticed it later pointed it out to me , and I went " oh , man . Why didn't I notice that ? " phd b: Hmm . professor a: um . So . {vocalsound} um So I think we 'll change our {disfmarker} our picture on the web , when we 're @ @ . One of the things I was {disfmarker} I mean , I was trying to think about what {disfmarker} what 's the best {vocalsound} way to show the difference an and I had a couple of thoughts one was , {vocalsound} that spectrogram that we show {vocalsound} is O K , but the thing is {vocalsound} the eyes uh and the {vocalsound} the brain behind them are so good at picking out patterns {vocalsound} from {disfmarker} from noise {vocalsound} that in first glance you look at them it doesn't seem like it 's that bad uh because there 's many features that are still preserved . So one thing to do might be to just take a piece of the spec uh of the spectrogram where you can see {vocalsound} that something looks different , an and blow it up , and have that be the part that 's {disfmarker} just to show as well . You know . phd b: Mm - hmm . Mm - hmm . professor a: i i Some things are going to be hurt . um {vocalsound} Another , I was thinking of was um {vocalsound} taking some spectral slices , like uh {disfmarker} like we look at with the recognizer , and look at the spectrum or cepstrum that you get out of there , and the {disfmarker} the uh , um , {vocalsound} the reverberation uh does make it {disfmarker} does change that . And so maybe {disfmarker} maybe that would be more obvious . phd b: Hmm . grad c: Spectral slices ? professor a: Yeah . grad c: W w what d what do you mean ? professor a: Well , I mean um all the recognizers look at frames . So they {disfmarker} they look at {disfmarker} phd b: So like one instant in time . professor a: Yeah , look at a {disfmarker} grad c: OK . professor a: So it 's , yeah , at one point in time or uh twenty {disfmarker} over twenty milliseconds or something , {vocalsound} you have a spectrum or a cepstrum . grad c: OK . professor a: That 's what I meant by a slice . grad c: I see . professor a: Yeah . And {vocalsound} if you look at {disfmarker} phd b: You could just {disfmarker} you could just throw up , you know , uh {vocalsound} the uh {disfmarker} some MFCC feature vectors . You know , one from one , one from the other , and then , you know , you can look and see how different the numbers are . professor a: Right . Well , that 's why I saying either {vocalsound} {vocalsound} Well , either spectrum or cepstrum phd b: I 'm just kidding . professor a: but {disfmarker} {vocalsound} but I think the thing is you wanna {disfmarker}  phd b: I don't mean a graph . I mean the actual numbers . professor a: Oh . I see . Oh . That would be lovely , yeah . phd b: Yeah . " See how different these {vocalsound} sequences of numbers are ? " professor a: Yeah . Or I could just add them up and get a different total . phd b: Yeah . It 's not the square . professor a: OK . Uh . What else {disfmarker} wh what 's {disfmarker} what else is going on ? phd f: Uh , yeah . Yeah , at first I had a remark why {disfmarker} I am wondering why the PDA is always so far . I mean we are always meeting at the {vocalsound} beginning of the table and {vocalsound} the PDA 's there . professor a: Uh . I guess cuz we haven't wanted to move it . We {disfmarker} we could {disfmarker} {vocalsound} we could move us , phd f: Yeah ? professor a: and . phd f: OK . grad e: That 's right . phd f: Well , anyway . Um . Yeah , so . Uh . Since the last meeting we 've {disfmarker} we 've tried to put together um {vocalsound} the clean low - pass um downsampling , upsampling , I mean , Uh the new filter that 's replacing the LDA filters , and also {vocalsound} the um delay issue so that {disfmarker} We considered th the {disfmarker} the delay issue on the {disfmarker} for the on - line normalization . Mmm . So we 've put together all this and then we have results that are not um {vocalsound} {vocalsound} very impressive . Well , there is no {vocalsound} real improvement . professor a: But it 's not wer worse and it 's better {disfmarker} better latency , phd f: It 's not {disfmarker} professor a: right ? phd f: Yeah . Yeah . Well . Actually it 's better . It seems better when we look at the mismatched case but {vocalsound} I think we are like {disfmarker} like cheated here by the {disfmarker} th this problem that {vocalsound} uh in some cases when you modify slight {disfmarker} slightly modify the initial condition you end up {vocalsound} completely somewhere air somewhere else in the {disfmarker} in the space , {vocalsound} the parameters . professor a: Yeah . phd f: So . Well . The other system are for instance . For Italian is at seventy - eight {vocalsound} percent recognition rate on the mismatch , and this new system has eighty - nine . But I don't think it indicates something , really . I don't {disfmarker} I don't think it means that the new system is more robust professor a: Uh - huh . phd f: or {disfmarker} It 's simply the fact that {disfmarker} Well . professor a: Well , the test would be if you then tried it on one of the other test sets , if {disfmarker} if it was {disfmarker} phd f: Y professor a: Right . So this was Italian , right ? phd f: Yeah . Yeah . professor a: So then if you take your changes phd f: It 's similar for other test sets professor a: and then {disfmarker} phd f: but I mean {vocalsound} from this se seventy - eight um percent recognition rate system , {vocalsound} I could change the transition probabilities for the {disfmarker} the first HMM and {pause} it will end up to eighty - nine also . professor a: Uh - huh . phd f: By using point five instead of point six , point four {vocalsound} as in the {disfmarker} the HTK script . professor a: Uh - huh . Yeah . phd f: So . Well . That 's {disfmarker} phd b: Yeah . Yeah I looked at um {disfmarker} {vocalsound} looked at the results when Stephane did that phd f: Well . Eh uh {disfmarker} phd b: and it 's {disfmarker} it 's really wo really happens . phd f: This really happens . phd b: I mean th the only difference is you change the self - loop transition probability by a tenth of a percent phd f: Yeah . professor a: Yeah . phd b: and it causes ten percent difference in the word error rate . professor a: A tenth of a per cent . phd b: Yeah . From point {disfmarker} phd f: Even tenth of a percent ? phd b: I {disfmarker} I 'm sorry phd f: Well , we tried {disfmarker} we tried point one , phd b: f for point {disfmarker} from {disfmarker} You change at point one phd f: yeah . professor a: Oh ! phd b: and n not tenth of a percent , one tenth , phd f: Hmm . professor a: Yeah . phd b: alright ? Um so from point five {disfmarker} so from point six to point five and you get ten percent better . professor a: Mm - hmm . phd b: And it 's {disfmarker} {vocalsound} I think it 's what you basically hypothesized in the last meeting {vocalsound} about uh it just being very {disfmarker} phd f: Mm - hmm . phd b: and I think you mentioned this in your email too {disfmarker} it 's just very um {disfmarker} phd f: Mmm , yeah . phd b: you know get stuck in some local minimum and this thing throws you out of it I guess . phd f: Mm - hmm . professor a: Well , what 's {disfmarker} what are {disfmarker} according to the rules what {disfmarker} what are we supposed to do about the transition probabilities ? Are they supposed to be point five or point six ? phd b: I think you 're not allowed to {disfmarker} Yeah . That 's supposed to be point six , for the self - loop . phd f: Yeah . professor a: Point {disfmarker} It 's supposed to be point six . phd b: Yeah . But changing it to point five I think is {disfmarker} which gives you much better results , but that 's {vocalsound} not allowed . professor a: But not allowed ? Yeah . OK . phd b: Yeah . phd f: Yeah , but even if you use point five , I 'm not sure it will always give you the better results phd b: Yeah . phd f: on other test set or it phd b: Right . We only tested it on the {disfmarker} the medium mismatch , phd f: on the other training set , I mean . phd b: right ? You said on the other cases you didn't notice {disfmarker} phd f: Yeah . But . I think , yeah . I think the reason is , yeah , I not I {disfmarker} it was in my mail I think also , {vocalsound} is the fact that the mismatch is trained only on the far microphone . Well , in {disfmarker} for the mismatched case everything is um using the far microphone training and testing , whereas for the highly mismatched , training is done on the close microphone so {vocalsound} it 's {disfmarker} it 's clean speech basically so you don't have this problem of local minima probably and for the well - match , it 's a mix of close microphone and distant microphone and {disfmarker} Well . phd b: I did notice uh something {disfmarker} phd f: So th I think the mismatch is the more difficult for the training part . phd b: Somebody , I think it was Morgan , suggested at the last meeting that I actually count to see {vocalsound} how many parameters and how many frames . professor a: Mm - hmm . phd f: Mm - hmm . phd b: And there are uh almost one point eight million frames of training data and less than forty thousand parameters in the baseline system . professor a: Hmm . phd f: Yeah . phd b: So it 's very , very few parameters compared to how much training data . professor a: Well . Yes . phd d: Mm - hmm . professor a: So . And that {disfmarker} that says that we could have lots more parameters actually . phd b: Yeah . Yeah . phd f: Mm - hmm . phd b: I did one quick experiment just to make sure I had everything worked out and I just {disfmarker} {vocalsound} uh f for most of the um {disfmarker} For {disfmarker} for all of the digit models , they end up at three mixtures per state . And so I just did a quick experiment , where I changed it so it went to four and um {vocalsound} it it {disfmarker} it didn't have a r any significant effect at the uh medium mismatch and high mismatch cases and it had {disfmarker} {vocalsound} it was just barely significant for the well - matched better . Uh so I 'm r gonna run that again but {vocalsound} um with many more uh mixtures per state . professor a: Yeah . Cuz at forty thou I mean you could you could have uh {disfmarker} Yeah , easily four times as many {vocalsound} parameters . phd b: Mm - hmm . And I think also {vocalsound} just seeing what we saw {vocalsound} uh in terms of the expected duration of the silence model ? when we did this tweaking of the self - loop ? The silence model expected duration was really different . phd f: Yeah . phd b: And so in the case where {vocalsound} um {vocalsound} it had a better score , the silence model expected duration was much longer . phd f: Yeah . phd b: So it was like {disfmarker} {vocalsound} it was a better match . I think {vocalsound} you know if we make a better silence model I think that will help a lot too um for a lot of these cases so but one one thing I {disfmarker} I wanted to check out before I increased the um {vocalsound} number of mixtures per state was {vocalsound} uh {vocalsound} in their {vocalsound} default training script they do an initial set of three re - estimations and then they built the silence model and then they do seven iterations then the add mixtures and they do another seven then they add mixtures then they do a final set of seven and they quit . Seven seems like a lot to me and it also makes the experiments go take a really long time I mean to do one turn - around of the well matched case takes like a day . professor a: Mm - hmm . Mm - hmm . phd b: And so {vocalsound} you know in trying to run these experiments I notice , you know , it 's difficult to find machines , you know , compute the run on . And so one of the things I did was I compiled HTK for the Linux {vocalsound} machines professor a: Mm - hmm . phd b: cuz we have this one from IBM that 's got like five processors in it ? professor a: Right . phd b: and so now I 'm {disfmarker} you can run stuff on that and that really helps a lot because now we 've got {vocalsound} you know , extra machines that we can use for compute . And if {disfmarker} I 'm do running an experiment right now where I 'm changing the number of iterations ? {vocalsound} from seven to three ? phd d: Mm - hmm . professor a: Yeah . phd b: just to see how it affects the baseline system . And so if we can get away with just doing three , we can do {vocalsound} many more experiments more quickly . And if it 's not a {disfmarker} a huge difference from running with seven iterations , {vocalsound} um , you know , we should be able to get a lot more experiments done . phd f: Hmm . phd b: And so . I 'll let you know what {disfmarker} what happens with that . But if we can {vocalsound} you know , run all of these back - ends f with many fewer iterations and {vocalsound} on Linux boxes we should be able to get a lot more experimenting done . professor a: Mm - hmm . phd b: So . So I wanted to experiment with cutting down the number of iterations before I {vocalsound} increased the number of Gaussians . professor a: Right . Sorry . So um , how 's it going on the {disfmarker} phd f: Um . professor a: So . You {disfmarker} you did some things . They didn't improve things in a way that convinced you you 'd substantially improved anything . phd f: Yeah . professor a: But they 're not making things worse and we have reduced latency , right ? phd f: Yeah . But actually {disfmarker} um actually it seems to do a little bit worse for the well - matched case and we just noticed that {disfmarker} Yeah , actually the way the final score is computed is quite funny . It 's not a mean of word error rate . It 's not a weighted mean of word error rate , it 's a weighted mean of improvements . professor a: Uh - huh . phd f: So . Which means that {vocalsound} actually the weight on the well - matched is {disfmarker} Well I well what what {disfmarker} What happened is that if you have a small improvement or a small if on the well - matched case {vocalsound} it will have uh huge influence on the improvement compared to the reference because the reference system is {disfmarker} is {disfmarker} is quite good for {disfmarker} for the well - ma well - matched case also . phd b: So it {disfmarker} it weights the improvement on the well - matched case really heavily compared to the improvement on the other cases ? phd f: No , but it 's the weighting of the {disfmarker} of the improvement not of the error rate . phd b: Yeah . Yeah , and it 's hard to improve on the {disfmarker} on the best case , cuz it 's already so good , right ? phd f: Yeah but {pause} what I mean is that you can have a huge improvement on the H {disfmarker} HMK 's , uh like five percent uh absolute , and this will not affect the final score almost {disfmarker} Uh this will almost not affect the final score because {vocalsound} this improvement {disfmarker} because the improvement {vocalsound} uh relative to the {disfmarker} the baseline is small {disfmarker} professor a: So they do improvement in terms of uh accuracy ? rather than word error rate ? phd f: Uh . Uh improvement ? professor a: So {disfmarker} phd f: No , it 's compared to the word er it 's improvement on the word error rate , professor a: OK . phd f: yeah . Sorry . professor a: So if you have uh ten percent error and you get five percent absolute uh {vocalsound} improvement then that 's fifty percent . phd f: Mm - hmm . professor a: OK . So what you 're saying then is that if it 's something that has a small word error rate , {vocalsound} then uh a {disfmarker} even a relatively small improvement on it , in absolute terms , {vocalsound} will show up as quite {disfmarker} quite large in this . phd f: Mm - hmm . professor a: Is that what you 're saying ? phd f: Yeah . professor a: Yes . phd f: Yeah . professor a: OK . But yeah that 's {disfmarker} that 's {disfmarker} it 's the notion of relative improvement . Word error rate . phd f: Yeah . Sure , but when we think about the weighting , which is point five , point three , point two , {vocalsound} it 's on absolute on {disfmarker} on relative figures , professor a: Yeah . phd f: not {disfmarker} professor a: Yeah . phd f: So when we look at this error rate professor a: No . That 's why I 've been saying we should be looking at word error rate uh and {disfmarker} and not {disfmarker} not at {vocalsound} at accuracies . phd f: uh {disfmarker} Mmm , yeah . Mmm , yeah . professor a: It 's {disfmarker} phd f: Mm - hmm . professor a: I mean uh we probably should have standardized on that all the way through . It 's just {disfmarker} phd b: Well . phd f: Mm - hmm . phd b: I mean , it 's not {disfmarker} it 's not that different , right ? I mean , just subtract the accuracy . professor a: Yeah but you 're {disfmarker} but when you look at the numbers , your sense of the relative size of things is quite different . phd b: I mean {disfmarker} Oh . Oh , I see . Yeah . professor a: If you had ninety percent uh correct {vocalsound} and five percent , five over ninety doesn't look like it 's a big difference , but {vocalsound} five over ten is {disfmarker} is big . phd b: Mm - hmm . phd f: Mm - hmm . professor a: So just when we were looking at a lot of numbers and {vocalsound} getting sense of what was important . phd b: I see . I see . Yeah . That makes sense . professor a: Um . phd f: Mmm . professor a: Um . phd f: Well anyway uh . So . Yeah . So it hurts a little bit on the well - match and yeah . professor a: What 's a little bit ? Like {disfmarker} phd f: Like , it 's difficult to say because again um {vocalsound} {vocalsound} I 'm not sure I have the um {disfmarker} phd b: Hey Morgan ? Do you remember that Signif program that we used to use for testing signi ? Is that still valid ? I {disfmarker} I 've been using that . professor a: Yeah . Yeah , it was actually updated . phd b: OK . professor a: Uh . {vocalsound} Jeff updated it some years ago phd b: Oh , it was . Oh , I shoul professor a: and {disfmarker} and uh cleaned it up made some things better in it . So . phd b: OK . I should find that new one . I just use my old one from {vocalsound} ninety - two or whatever professor a: Yeah , I 'm sure it 's not that different but {disfmarker} but he {disfmarker} {vocalsound} he uh {disfmarker} he was a little more rigorous , as I recall . phd b: OK . phd f: Right . So it 's around , like , point five . No , point six {comment} uh percent absolute on Italian {disfmarker} professor a: Worse . phd f: Worse , yep . professor a: Out of what ? I mean . s phd f: Uh well we start from ninety - four point sixty - four , and we go to ninety - four point O four . professor a: Uh - huh . So that 's six {disfmarker} six point th phd f: Uh . phd b: Ninety - three point six four , right ? is the baseline . phd f: Oh , no , I 've ninety - four . Oh , the baseline , you mean . phd b: Yeah . phd f: Well I don't {disfmarker} I 'm not talking about the baseline here . phd b: Oh . Oh . I 'm sorry . phd f: I uh {disfmarker} My baseline is the submitted system . phd b: Ah ! OK . Ah , ah . phd f: Hmm . professor a: Yeah . phd b: Sorry . phd f:  Oh yeah . For Finnish , we start to ninety - three point eight - four and we go to ninety - three point seventy - four . And for Spanish we are {disfmarker} we were at ninety - five point O five and we go to ninety - three - s point sixty one . professor a: OK , so we are getting hurt somewhat . phd f: So . professor a: And is that wh what {disfmarker} do you know what piece {disfmarker} you 've done several changes here . Uh , do you know what pie phd f: Yeah . I guess {disfmarker} I guess it 's {disfmarker} it 's the filter . Because nnn , well uh we don't have complete result , but the filter {disfmarker} So the filter with the shorter delay hurts on Italian well - matched , which {disfmarker} And , yeah . And the other things , like um {vocalsound} downsampling , upsampling , don't seem to hurt and {vocalsound} the new on - line normalization , neither . phd b: I 'm {disfmarker} phd f: So . phd b: I 'm really confused about something . If we saw that making a small change like , you know , a tenth , to the self - loop had a huge effect , {vocalsound} can we really make any conclusions about differences in this stuff ? phd f: Mm - hmm . Yeah that 's th Yeah . phd b: I mean , especially when they 're this small . I mean . phd f: I think we can be completely fooled by this thing , but {disfmarker} I don't know . professor a: Well , yeah . phd f: So . There is first this thing , and then the {disfmarker} yeah , I computed the um {disfmarker} {vocalsound} like , the confidence level on the different test sets . And for the well - matched they are around um {vocalsound} point six uh percent . For the mismatched they are around like let 's say one point five percent . And for the well - m uh HM they are also around one point five . professor a: But {disfmarker} OK , so you {disfmarker} these {disfmarker} these degradations you were talking about were on the well - matched case phd f: So . professor a: Uh . Do the {disfmarker} does the new filter make things uh better or worse for the other cases ? phd f: Yeah . But . Uh . About the same . It doesn't hurt . Yeah . professor a: Doesn't hurt , but doesn't get a little better , or something . phd f: No . professor a: No . OK , so {vocalsound} um I guess the argument one might make is that , " Yeah , if you looked at one of these cases {vocalsound} and you jiggle something and it changes {vocalsound} then uh you 're not quite sure what to make of it . But when you look across a bunch of these and there 's some {disfmarker} some pattern , um {disfmarker} I mean , so eh h here 's all the {disfmarker} if {disfmarker} if in all these different cases {vocalsound} it never gets better , and there 's significant number of cases where it gets worse , {vocalsound} then you 're probably {pause} hurting things , {vocalsound} I would say . So um {vocalsound} I mean at the very least that would be a reasonably prediction of what would happen with {disfmarker} with a different test set , that you 're not jiggling things with . So I guess the question is if you can do better than this . If you can {disfmarker} if we can approximate {vocalsound} the old numbers while still keeping the latency down . phd f: Mmm . Yeah . professor a: Uh , so . Um . What I was asking , though , is uh {disfmarker} are {disfmarker} what 's {disfmarker} what 's the level of communication with uh {vocalsound} the O G I gang now , about this and {disfmarker} phd f: Well , we are exchanging mail as soon as we {disfmarker} {vocalsound} we have significant results . professor a: Yeah . phd f: Um . Yeah . For the moment , they are working on integrating {vocalsound} the um {vocalsound} spectral subtraction apparently from Ericsson . professor a: Mm - hmm . phd f: Um . Yeah . And so . Yeah . We are working on our side on other things like {vocalsound} uh also trying a sup spectral subtraction but of {disfmarker} of our own , I mean , another {vocalsound} spectral substraction . professor a: Mm - hmm . phd f: Um . Yeah . So I think it 's {disfmarker} it 's OK . It 's going {disfmarker} professor a: Is there any further discussion about this {disfmarker} this idea of {disfmarker} of having some sort of source code control ? phd f: Yeah . Well . For the moment they 're {disfmarker} uh everybody 's quite um {disfmarker} There is this Eurospeech deadline , so . professor a: I see . phd f: Um . And . Yeah . But yeah . As soon as we have something that 's significant and that 's better than {disfmarker} than what was submitted , we will fix {disfmarker} fix the system and {disfmarker} But we 've not discussed it {disfmarker} it {disfmarker} it {disfmarker} this yet , yeah . professor a: Yeah . Sounds like a great idea but {disfmarker} but I think that {disfmarker} that um {vocalsound} he 's saying people are sort of scrambling for a Eurospeech deadline . phd f: Mmm . professor a: But that 'll be uh , uh done in a week . So , maybe after {vocalsound} this next one . phd f: Yeah . phd b: Wow ! Already a week ! Man ! professor a: Yeah . phd b: You 're right . That 's amazing . professor a: Yeah . Anybo - anybody in the {disfmarker} in this group do doing anything for Eurospeech ? phd f: S professor a: Or , is that what {disfmarker} is that {disfmarker} phd f: Yeah we are {disfmarker} {vocalsound} We are trying to {disfmarker} to do something with the Meeting Recorder digits , professor a: Right . phd f: and {disfmarker} But yeah . Yeah . And the good thing is that {pause} there is this first deadline , professor a: Yeah . phd f: and , well , some people from OGI are working on a paper for this , but there is also the um {vocalsound} special session about th Aurora which is {disfmarker} {vocalsound} uh which has an extended deadline . So . The deadline is in May . professor a: For uh {disfmarker} {vocalsound} Oh , for Eurospeech ? phd f: For th Yeah . professor a: Oh ! phd f: So f only for the experiments on Aurora . So it {disfmarker} it 's good , professor a: Oh , a special dispensation . phd f: yeah . professor a: That 's great . phd b: Mm - hmm . Where is Eurospeech this year ? phd f: It 's in Denmark . professor a: Aalborg {disfmarker} Aalborg uh phd b: Oh . professor a: So the deadline {disfmarker} When 's the deadline ? When 's the deadline ? phd f: Hmm ? I think it 's the thirteenth of May . professor a: That 's great ! It 's great . So we should definitely get something in for that . phd f: Yeah . professor a: But on meeting digits , maybe there 's {disfmarker} Maybe . phd f: Yeah . professor a: Maybe . phd f: So it would be for the first deadline . professor a: Yeah . phd f: Nnn . professor a: Yeah . So , I mean , I {disfmarker} I think that you could certainly start looking at {disfmarker} at the issue uh but {disfmarker} but uh {vocalsound} I think it 's probably , on s from what Stephane is saying , it 's {disfmarker} it 's unlikely to get sort of active participation from the two sides until after they 've {disfmarker} phd b: Well I could at least {disfmarker} Well , I 'm going to be out next week but I could {pause} try to look into like this uh CVS over the web . That seems to be a very popular {vocalsound} way of {pause} people distributing changes and {disfmarker} over , you know , multiple sites and things professor a: Mm - hmm . phd b: so maybe {vocalsound} if I can figure out how do that easily and then pass the information on to everybody so that it 's {vocalsound} you know , as easy to do as possible and {disfmarker} and people don't {disfmarker} it won't interfere with {comment} their regular work , then maybe that would be good . And I think we could use it for other things around here too . So . professor a: Good . grad c: That 's cool . And if you 're interested in using CVS , I 've set it up here , phd b: Oh great . grad c: so . phd b: OK . grad c: um j phd b: I used it a long time ago but it 's been a while so maybe I can ask you some questions . grad c: Oh . So . I 'll be away tomorrow and Monday but I 'll be back on Tuesday or Wednesday . phd b: OK . professor a: Yeah . Dave , the other thing , actually , is {disfmarker} is this business about this wave form . Maybe you and I can talk a little bit at some point about {vocalsound} coming up with a better {vocalsound} uh demonstration of the effects of reverberation for our web page , cuz uh {vocalsound} {disfmarker} the uh {vocalsound} um I mean , actually the {disfmarker} the uh It made a good {disfmarker} good audio demonstration because when we could play that clip the {disfmarker} the {disfmarker} the really {vocalsound} obvious difference is that you can hear two voices and {disfmarker} {vocalsound} {vocalsound} in the second one and only hear {disfmarker} phd b: Maybe we could just {pause} like , talk into a cup . professor a: Yeah . phd b: Some good reverb . professor a: No , I mean , it sound {disfmarker} it sounds pretty reverberant , but I mean you can't {disfmarker} when you play it back in a room with a {disfmarker} you know a big room , {vocalsound} nobody can hear that difference really . grad c: Yeah . professor a: They hear that it 's lower amplitude and they hear there 's a second voice , grad c: Uh - huh . professor a: um {vocalsound} but uh that {disfmarker} actually that makes for a perfectly good demo because that 's a real obvious thing , that you hear two voices . phd b: But not of reverberation . professor a: Yeah . grad c: A boom . professor a: Well that {disfmarker} that {disfmarker} that 's OK . But for the {disfmarker} the visual , just , you know , I 'd like to have uh {vocalsound} uh , you know , the spectrogram again , grad c: Yeah . professor a: because you 're {disfmarker} you 're {disfmarker} you 're visual {vocalsound} uh abilities as a human being are so good {vocalsound} you can pick out {disfmarker} you know , you {disfmarker} you look at the good one , you look at the cru the screwed up one , and {disfmarker} and you can see the features in it without trying to @ @ {disfmarker} phd b: I noticed that in the pictures . professor a: yeah . phd b: I thought " hey , you know th " I {disfmarker} My initial thought was " this is not too bad ! " professor a: Right . But you have to {disfmarker} you know , if you look at it closely , you see " well , here 's a place where this one has a big formant {disfmarker} uh uh formant {disfmarker} maj major formants here are {disfmarker} {vocalsound} are moving quite a bit . " And then you look in the other one and they look practically flat . phd b: Mm - hmm . professor a: So I mean you could {disfmarker} that 's why I was thinking , in a section like that , you could take a look {disfmarker} look at just that part of the spectrogram and you could say " Oh yeah . This {disfmarker} this really distorted it quite a bit . " phd b: Yeah . The main thing that struck me in looking at those two spectrograms was the difference in the high frequencies . It looked like {vocalsound} for the one that was farther away , you know , it really {disfmarker} everything was attenuated professor a: Right . phd b: and {disfmarker} I mean that was the main visual thing that I noticed . professor a: Right . But it 's {disfmarker} it 's uh {disfmarker} So . Yeah . So there are {disfmarker} clearly are spectral effects . Since you 're getting all this indirect energy , then a lot of it does have {disfmarker} have uh {vocalsound} reduced high frequencies . But um the other thing is the temporal courses of things really are changed , and {disfmarker} {vocalsound} and uh we want to show that , in some obvious way . The reason I put the wave forms in there was because {vocalsound} uh they {disfmarker} they do look quite different . Uh . And so I thought " Oh , this is good . " but I {disfmarker} {vocalsound} I just uh {disfmarker} After {disfmarker} after uh they were put in there I didn't really look at them anymore , cuz I just {disfmarker} they were different . So {vocalsound} I want something that has a {disfmarker} is a more interesting explanation for why they 're different . Um . grad c: Oh . So maybe we can just substitute one of these wave forms and um {vocalsound} then do some kind of zoom in on the spectrogram on an interesting area . professor a: Something like that . Yeah . grad c: Uh - huh . professor a: The other thing that we had in there that I didn't like was that um {vocalsound} the most obvious characteristic of the difference uh when you listen to it is that there 's a second voice , and the {disfmarker} the {disfmarker} the {disfmarker} the {disfmarker} the uh {vocalsound} cuts that we have there actually don't correspond to the full wave form . It 's just the first {disfmarker} I think there was something where he was having some trouble getting so much in , or . I {disfmarker} I forget the reason behind it . But {vocalsound} it {disfmarker} it 's um {disfmarker} {vocalsound} it 's the first six seconds or something {vocalsound} of it and it 's in {vocalsound} the seventh or eighth second or something where @ @ the second voice comes in . So we {disfmarker} we would like to actually see {vocalsound} the voice coming in , too , I think , since that 's the most obvious thing {pause} when you listen to it . grad c: Mm - hmm . professor a: So . Um . phd f: Uh , yeah . Yeah . I brought some {disfmarker} I don't know if {disfmarker} {vocalsound} some {vocalsound} figures here . Well . I start {disfmarker} we started to work on spectral subtraction . And {vocalsound} um {vocalsound} the preliminary results were very bad . professor a: Uh - huh . phd f: So the thing that we did is just to add spectral subtraction before this , the Wall uh process , which contains LDA on - line normalization . And it hurts uh a lot . professor a: Uh - huh . phd f: And so we started to look at {disfmarker} at um things like this , which is , well , it 's {disfmarker} Yeah . So you have the C - zero parameters for one uh Italian utterance . phd d: You can @ @ . phd f: And I plotted this for two channels . Channel zero is the close mic microphone , and channel one is the distant microphone . And it 's perfectly synchronized , so . And the sentence contain only one word , which is " Due " And it can't clearly be seen . Where {disfmarker} where is it ? professor a: Uh - huh . phd f: Where is the word ? phd b: This is {disfmarker} this is , grad e: Hmm . phd b: oh , a plot of C - zero , phd f: So . phd b: the energy . phd f: This is a plot of C - zero , uh when we don't use spectral substraction , and when there is no on - line normalization . professor a: Mm - hmm . phd f: So . There is just some filtering with the LDA and {vocalsound} and some downsampling , upsampling . phd b: C - zero is the close talking ? {disfmarker} phd f: So . phd b: uh the close channel ? phd f: Yeah . Yeah . phd b: and s channel one is the {disfmarker} phd f: Yeah . So C - zero is very clean , actually . phd b: Yeah . phd f: Uh then when we apply mean normalization it looks like the second figure , though it is not . Which is good . Well , the noise part is around zero professor a: Mm - hmm . phd f: and {disfmarker} {vocalsound} {vocalsound} And then the third figure is what happens when we apply mean normalization and variance normalization . So . What we can clearly see is that on the speech portion {vocalsound} the two channel come {disfmarker} becomes very close , but also what happens on the noisy portion is that the variance of the noise is {disfmarker} professor a: Mm - hmm . phd b: This is still being a plot of C - zero ? OK . phd f: Yeah . This is still C - zero . phd b: Can I ask um what does variance normalization do ? w What is the effect of that ? professor a: Normalizes the variance . phd f: So it {disfmarker} it {disfmarker} Yeah . phd b: I mean phd f: It normalized th the standard deviation . phd b: y Yeah . phd f: So it {disfmarker} phd b: No , I understand that , phd f: You {disfmarker} you get an estimate of the standard deviation . phd b: but I mean {disfmarker} phd f: That 's phd b: No . phd f: um {disfmarker} phd b: No , I understand what it is , but I mean , what does it {disfmarker} what 's {disfmarker} what is phd f: Yeah but . phd b: uh {disfmarker} professor a: What 's the rationale ? phd b: We Yeah . Yeah . Why {disfmarker} why do it ? phd f: Uh . professor a: Well , I mean , because {vocalsound} everything uh {disfmarker} If you have a system based on Gaussians , everything is based on means and variances . phd b: Yeah . professor a: So if there 's an overall {vocalsound} reason {disfmarker} You know , it 's like uh if you were doing uh image processing and in some of the pictures you were looking at , uh there was a lot of light uh and {disfmarker} and in some , there was low light , phd b: Mm - hmm . professor a: you know , you would want to adjust for that in order to compare things . phd b: Mm - hmm . professor a: And the variance is just sort of like the next moment , you know ? So uh {vocalsound} what if um one set of pictures was taken uh so that throughout the course it was {disfmarker} went through daylight and night uh {vocalsound} um um ten times , another time it went thr I mean i is , you know , how {disfmarker} how much {disfmarker} {vocalsound} how much vari phd b: Oh , OK . professor a: Or no . I guess a better example would be {vocalsound} how much of the light was coming in from outside rather than artificial light . So if it was a lot {disfmarker} {vocalsound} if more was coming from outside , then there 'd be the bigger effect of the {disfmarker} of the {disfmarker} of the change in the {disfmarker} So every mean {disfmarker} every {disfmarker} all {disfmarker} all of the {disfmarker} the parameters that you have , especially the variances , are going to be affected by the overall variance . phd b: Oh , OK . Uh - huh . professor a: And so , in principle , you {disfmarker} if you remove that source , then , you know , you can {disfmarker} phd b: I see . OK . So would {disfmarker} the major effect is {disfmarker} that you 're gonna get is by normalizing the means , professor a: That 's the first order but {disfmarker} thing , phd b: but it may help {disfmarker} First - order effects . professor a: but then the second order is {disfmarker} is the variances phd b: And it may help to do the variance . OK . professor a: because , again , if you {disfmarker} if you 're trying to distinguish between E and B phd b: OK . professor a: if it just so happens that the E 's {vocalsound} were a more {disfmarker} you know , were recorded when {disfmarker} when the energy was {disfmarker} was {disfmarker} was larger or something , phd b: Mm - hmm . Mm - hmm . Mm - hmm .  professor a: or the variation in it was larger , {vocalsound} uh than with the B 's , then this will be {disfmarker} give you some {disfmarker} some bias . phd b:  professor a: So the {disfmarker} {vocalsound} it 's removing these sources of variability in the data {vocalsound} that have nothing to do with the linguistic component . phd b: OK . phd f: Mmm . phd b: Gotcha . OK . Sorry to interrupt . professor a: But the {disfmarker} the uh {disfmarker} but let me as ask {disfmarker} ask you something . phd f: Yep . And it {disfmarker} and this {disfmarker} professor a: i is {disfmarker} if {disfmarker} If you have a good voice activity detector , isn't {disfmarker} isn't it gonna pull that out ? phd f: Yeah . Sure . If they are good . Yeah . Well what it {disfmarker} it shows is that , yeah , perhaps a good voice activity detector is {disfmarker} is good before on - line normalization and that 's what uh {vocalsound} we 've already observed . But uh , yeah , voice activity detection is not {vocalsound} {vocalsound} an easy thing neither . phd b: But after you do this , after you do the variance normalization {disfmarker} I mean . phd f: Mm - hmm . phd b: I don't know , it seems like this would be a lot easier than this signal to work with . phd f: Yeah . So . What I notice is that , while I prefer to look at the second figure than at the third one , well , because you clearly see where speech is . professor a: Yeah . phd b: Yeah . phd f: But the problem is that on the speech portion , channel zero and channel one are more different than when you use variance normalization where channel zero and channel one become closer . professor a: Right . phd b: But for the purposes of finding the speech {disfmarker} phd f: And {disfmarker} Yeah , but here {disfmarker} phd b: You 're more interested in the difference between the speech and the nonspeech , phd f: Yeah . phd b: right ? phd f: Yeah . So I think , yeah . For I th I think that it {disfmarker} perhaps it shows that {vocalsound} uh the parameters that the voice activity detector should use {disfmarker} uh have to use should be different than the parameter that have to be used for speech recognition . professor a: Yeah . So basically you want to reduce this effect . phd f: Well , y professor a: So you can do that by doing the voi voice activity detection . You also could do it by spect uh spectral subtraction before the {vocalsound} variance normalization , right ? phd f: Yeah , but it 's not clear , yeah . professor a: So uh {disfmarker} phd f: We So . Well . It 's just to professor a: Yeah . phd f: the {disfmarker} the number that at that are here are recognition experiments on Italian HM and MM {vocalsound} with these two kinds of parameters . And , {pause} well , it 's better with variance normalization . professor a: Yeah . Yeah . So it does get better even though it looks ugly . phd f: Uh {disfmarker} professor a: OK . but does this have the voice activity detection in it ? phd f: Yeah . professor a: OK . phd f: Um . professor a: So . grad e: OK . phd b: Where 's th phd f: But the fact is that the voice activity detector doesn't work on channel one . So . Yeah . professor a: Uh - huh . phd b: Where {disfmarker} at what stage is the voice activity detector applied ? Is it applied here or a after the variance normalization ? phd f: Hmm ? professor a: Spectral subtraction , I guess . phd b: or {disfmarker} phd f: It 's applied before variance normalization . So it 's a good thing , phd b: Oh . phd f: because I guess voice activity detection on this should {disfmarker} could be worse . phd b: Yeah . Is it applied all the way back here ? phd f: It 's applied the um on , yeah , something like this , phd b: Maybe that 's why it doesn't work for channel one . phd f: yeah . Perhaps , yeah . professor a: Can I {disfmarker} phd f: So we could perhaps do just mean normalization before VAD . phd b: Mm - hmm . professor a: Mm - hmm . Can I ask a , I mean {disfmarker} a sort of top - level question , which is {vocalsound} um " if {disfmarker} if most of what the OGI folk are working with is trying to {vocalsound} integrate this other {disfmarker} other uh spectral subtraction , {vocalsound} why are we worrying about it ? " phd f: Mm - hmm . About ? Spectral subtraction ? professor a: Yeah . phd f: It 's just uh {disfmarker} Well it 's another {disfmarker} They are trying to u to use the um {disfmarker} {vocalsound} the Ericsson and we 're trying to use something {disfmarker} something else . And . Yeah , and also to understand what happens because professor a: OK . phd f: uh fff Well . When we do spectral subtraction , actually , I think {vocalsound} that this is the {disfmarker} the two last figures . professor a: Yeah . phd f: Um . It seems that after spectral subtraction , speech is more emerging now uh {vocalsound} than {disfmarker} than before . professor a: Mm - hmm . phd b: Speech is more what ? phd f: Well , the difference between the energy of the speech and the energy of the n spectral subtrac subtracted noise portion is {disfmarker} is larger . professor a: Mm - hmm . phd f: Well , if you compare the first figure to this one {disfmarker} Actually the scale is not the same , but if you look at the {disfmarker} the numbers um {vocalsound} you clearly see that the difference between the C - zero of the speech and C - zero of the noise portion is larger . Uh but what happens is that after spectral subtraction , {vocalsound} you also increase the variance of this {disfmarker} of C - zero . professor a: Mm - hmm . phd f: And so if you apply variance normalization on this , it completely sc screw everything . Well . professor a: Mm - hmm . phd f: Um . Uh . Yeah . So yeah . And what they did at OGI is just {vocalsound} uh they don't use on - line normalization , for the moment , on spectral subtraction and I think {disfmarker} Yeah . I think as soon as they will try on - line normalization {vocalsound} there will be a problem . So yeah , we 're working on the same thing but {vocalsound} I think uh with different {disfmarker} different system and {disfmarker} professor a: Right . I mean , i the Intellectually it 's interesting to work on things th uh one way or the other phd f: Mm - hmm . professor a: but I 'm {disfmarker} I 'm just wondering if um {disfmarker} {vocalsound} on the list of things that there are to do , if there are things that we won't do because {vocalsound} we 've got two groups doing the same thing . phd f: Mm - hmm . professor a: Um . That 's {disfmarker} phd f: Mm - hmm . professor a: Um . Just {disfmarker} just asking . Uh . I mean , it 's {disfmarker} phd f: Yeah , well , phd b: There also could be {disfmarker} I mean . I can maybe see a reason f for both working on it too phd f: uh . phd b: if {vocalsound} um you know , if {disfmarker} if {disfmarker} if you work on something else and {disfmarker} and you 're waiting for them to give you {vocalsound} spectral subtraction {disfmarker} I mean it 's hard to know whether {vocalsound} the effects that you get from the other experiments you do will {vocalsound} carry over once you then bring in their spectral subtraction module . So it 's {disfmarker} it 's almost like everything 's held up waiting for this {vocalsound} one thing . I don't know if that 's true or not , but I could see how {disfmarker} phd f: Mmm . professor a: I don't know . phd b: Maybe that 's what you were thinking . professor a: I don't know . {vocalsound} I mean , we still evidently have a latency reduction plan which {disfmarker} which isn't quite what you 'd like it to be . That {disfmarker} that seems like one prominent thing . And then uh weren't issues of {disfmarker} of having a {disfmarker} a second stream or something ? That was {disfmarker} Was it {disfmarker} There was this business that , you know , we {disfmarker} we could use up the full forty - eight hundred bits , and {disfmarker} phd f: Yeah . But I think they ' I think we want to work on this . They also want to work on this , so . Uh . {vocalsound} yeah . We {disfmarker} we will try MSG , but um , yeah . And they are t I think they want to work on the second stream also , but more with {vocalsound} some kind of multi - band or , well , what they call TRAP or generalized TRAP . professor a: Mm - hmm . phd f: Um . So . professor a: OK . Do you remember when the next meeting is supposed to be ? the next uh {disfmarker} phd f: It 's uh in June . professor a: In June . OK . phd f: Yeah . professor a: Yeah . Um . Yeah , the other thing is that you saw that {disfmarker} that mail about uh the VAD {disfmarker} V A Ds performing quite differently ? That that uh So um . This {disfmarker} there was this experiment of uh " what if we just take the baseline ? " phd f: Mmm . professor a: set uh of features , just mel cepstra , and you inc incorporate the different V A And it looks like the {disfmarker} the French VAD is actually uh better {disfmarker} significantly better . phd b: Improves the baseline ? professor a: Yeah . Yeah . phd f: Yeah but I don't know which VAD they use . Uh . If the use the small VAD I th I think it 's on {disfmarker} I think it 's easy to do better because it doesn't work at all . So . I {disfmarker} I don't know which {disfmarker} which one . It 's Pratibha that {disfmarker} that did this experiment . phd d: Yeah . phd f: Um . We should ask which VAD she used . phd d: I don't @ @ . He {disfmarker} Actually , I think that he say with the good VAD of {disfmarker} from OGI and with the Alcatel VAD . And the experiment was sometime better , sometime worse . phd f: Yeah but I {disfmarker} it 's uh {disfmarker} I think you were talking about the other mail that used VAD on the reference features . professor a: Yes . phd f: Yeah . professor a: And on that one , uh the French one is {disfmarker} was better . phd d: I don't remember . professor a: It was just better . phd d: Mm - hmm . professor a: I mean it was enough better that {disfmarker} that it would {vocalsound} uh account for a fair amount of the difference between our performance , actually . phd f: Mm - hmm . phd d: Mm - hmm . professor a: So . {vocalsound} Uh . So if they have a better one , we should use it . I mean . You know ? it 's {disfmarker} you can't work on everything . phd f: Yeah . professor a: Uh . {vocalsound} Uh . Yeah . phd f: Yeah , so we should find out if it 's really better . I mean if it {disfmarker} the {disfmarker} compared to the small or the big network . phd d: Mm - hmm . professor a: Yeah . phd f: And perhaps we can easily improve if {disfmarker} if we put like mean normalization before the {disfmarker} before the VAD . Because {disfmarker} {vocalsound} as {disfmarker} as you 've {pause} mentioned . professor a: Yeah . phd f: Mmm . professor a: H Hynek will be back in town uh the week after next , back {disfmarker} back in the country . So . And start {disfmarker} start organizing uh {vocalsound} more visits and connections and so forth , phd f: Mm - hmm . professor a: and {disfmarker} uh working towards June . phd f: Yeah . phd d: Also is Stephane was thinking that {vocalsound} maybe it was useful to f to think about uh {vocalsound} voiced - unvoiced {disfmarker} phd f: Mm - hmm . phd d: to work uh here in voiced - unvoiced detection . phd f: Yeah . Yeah . phd d: And we are looking {vocalsound} {vocalsound} in the uh signal . phd f: Yeah , my feeling is that um actually {vocalsound} when we look at all the proposals , ev everybody is still using some kind of spectral envelope professor a: Right . phd f: and um it 's {disfmarker} professor a: No use of pitch uh basically . Yeah . phd f: Yeah , well , not pitch , but to look at the um fine {disfmarker} at the {disfmarker} at the high re high resolution spectrum . professor a: Yeah . Well , it {disfmarker} phd f: So . We don't necessarily want to find the {disfmarker} the pitch of the {disfmarker} of the sound but uh {disfmarker} Cuz I have a feeling that {vocalsound} when we look {disfmarker} when we look at the {disfmarker} just at the envelope there is no way you can tell if it 's voiced and unvoiced , if there is some {disfmarker} It 's {disfmarker} it 's easy in clean speech because voiced sound are more low frequency and . So there would be more , professor a: Yeah . phd f: uh {disfmarker} there is the first formant , which is the larger and then voiced sound are more high frequencies cuz it 's frication and {disfmarker} professor a: Right . phd f: But , yeah . When you have noise there is no um {disfmarker} {vocalsound} if {disfmarker} if you have a low frequency noise it could be taken for {disfmarker} for voiced speech and . professor a: Yeah , you can make these mistakes , phd f: So . professor a: but {disfmarker} but {disfmarker} phd b: Isn't there some other phd f: S phd b: uh d phd f: So I think that it {disfmarker} it would be good {disfmarker} Yeah , yeah , well , go {disfmarker} go on . phd b: Uh , I was just gonna say isn't there {disfmarker} {vocalsound} aren't {disfmarker} aren't there lots of ideas for doing voice activity , or speech - nonspeech rather , {comment} um by looking at {vocalsound} um , you know , uh {vocalsound} I guess harmonics or looking across time {disfmarker} professor a: Well , I think he was talking about the voiced - unvoiced , though , phd f: Mmm . professor a: right ? So , not the speech - nonspeech . phd b: Yeah . Well even with e professor a: Yeah . phd b: uh w ah you know , uh even with the voiced - non {pause} voiced - unvoiced phd f: Mmm . phd b: um {disfmarker} I thought that you or {pause} somebody was talking about {disfmarker} professor a: Well . Uh yeah . B We should let him finish what he w he was gonna say , phd f: So . phd b: OK . professor a: and {disfmarker} phd b: So go ahead . phd f: Um yeah , so yeah , I think if we try to develop a second stream well , there would be one stream that is the envelope and the second , it could be interesting to have that 's {disfmarker} something that 's more related to the fine structure of the spectrum . And . Yeah , so I don't know . We were thinking about like using ideas from {disfmarker} from Larry Saul , have a good voice detector , have a good , well , voiced - speech detector , that 's working on {disfmarker} on the FFT and {vocalsound} uh professor a: U phd f: Larry Saul could be an idea . We were are thinking about just {vocalsound} kind of uh taking the spectrum and computing the variance of {disfmarker} of the high resolution spectrum {vocalsound} and things like this . professor a: So u s u OK . So {disfmarker} So many {vocalsound} tell you something about that . Uh we had a guy here some years ago who did some work on {vocalsound} um {vocalsound} making use of voicing information uh to {vocalsound} help in reducing the noise . phd f: Yeah ? professor a: So what he was doing is basically y you {disfmarker} {vocalsound} you do estimate the pitch . phd f: Mm - hmm . professor a: And um you {disfmarker} from that you {disfmarker} you estimate {disfmarker} or you estimate fine harmonic structure , whichev ei either way , it 's more or less the same . But {vocalsound} uh the thing is that um you then {vocalsound} can get rid of things that are not {disfmarker} i if there is strong harmonic structure , {vocalsound} you can throw away stuff that 's {disfmarker} that 's non - harmonic . phd f: Mm - hmm . Mm - hmm . professor a: And that {disfmarker} that is another way of getting rid of part of the noise phd f: Yeah . professor a: So um that 's something {vocalsound} that is sort of finer , phd f: Yeah . professor a: brings in a little more information than just spectral subtraction . Um . phd f: Mm - hmm . professor a: And he had some {disfmarker} I mean , he did that sort of in combination with RASTA . It was kind of like RASTA was taking care of convolutional stuff phd f: Mmm . professor a: and he was {disfmarker} phd f: Mm - hmm . professor a: and {disfmarker} and got some {disfmarker} some decent results doing that . So that {disfmarker} that 's another {disfmarker} another way . But yeah , there 's {disfmarker} there 's {disfmarker} phd f: Yeah . Mmm . professor a: Right . There 's all these cues . We 've actually back when Chuck was here we did some voiced - unvoiced uh {vocalsound} classification using a bunch of these , phd f: But {disfmarker} professor a: and {disfmarker} and uh works OK . Obviously it 's not perfect but um {disfmarker} phd f: Mm - hmm . professor a: But the thing is that you can't {disfmarker} given the constraints of this task , we can't , {vocalsound} in a very nice way , feed {pause} forward to the recognizer the information {disfmarker} the probabilistic information that you might get about whether it 's voiced or unvoiced , where w we can't you know affect the {disfmarker} {vocalsound} the uh distributions or anything . phd f: Mm - hmm . professor a: But we {disfmarker} what we uh {disfmarker} I guess we could Yeah . phd b: Didn't the head dude send around that message ? Yeah , I think you sent us all a copy of the message , where he was saying that {disfmarker} I I 'm not sure , exactly , what the gist of what he was saying , but something having to do with the voice {vocalsound} activity detector and that it will {disfmarker} {vocalsound} that people shouldn't put their own in or something . It was gonna be a {disfmarker} professor a: That {disfmarker} But {disfmarker} OK . So that 's voice activity detector as opposed to voicing detector . phd f: They didn't . professor a: So we 're talking about something a little different . phd f: Mmm . phd b: Oh , I 'm sorry . professor a: Right ? phd b: I {disfmarker} I missed that . phd f: Mmm . professor a: I guess what you could do , maybe this would be w useful , if {disfmarker} if you have {disfmarker} if you view the second stream , yeah , before you {disfmarker} before you do KLT 's and so forth , if you do view it as probabilities , and if it 's an independent {disfmarker} So , if it 's {disfmarker} if it 's uh not so much {vocalsound} envelope - based by fine - structure - based , uh looking at harmonicity or something like that , um if you get a probability from that information and then multiply it by {disfmarker} you know , multiply by all the voiced {vocalsound} outputs and all the unvoiced outputs , you know , then {vocalsound} use that as the phd f: Mm - hmm . professor a: uh {disfmarker} take the log of that or {vocalsound} uh pre pre uh {disfmarker} pre - nonlinearity , phd f: Yeah . i if {disfmarker} professor a: uh and do the KLT on the {disfmarker} on {disfmarker} on that , phd f: Yeah . professor a: then that would {disfmarker} that would I guess be uh a reasonable use of independent information . So maybe that 's what you meant . And then that would be {disfmarker} phd f: Yeah , well , I was not thinking this {disfmarker} yeah , this could be an yeah So you mean have some kind of probability for the v the voicing professor a: R Right . So you have a second neural net . phd f: and then use a tandem system professor a: It could be pretty small . Yeah . If you have a tandem system and then you have some kind of {disfmarker} it can be pretty small {disfmarker} net {disfmarker} phd f: Mm - hmm . professor a: we used {disfmarker} we d did some of this stuff . Uh I {disfmarker} I did , some years ago , phd f: Yeah . professor a: and the {disfmarker} and {disfmarker} and you use {disfmarker} {vocalsound} the thing is to use information primarily that 's different as you say , it 's more fine - structure - based than {disfmarker} than envelope - based phd f: Mm - hmm . professor a: uh so then it you {disfmarker} you {disfmarker} you can pretty much guarantee it 's stuff that you 're not looking at very well with the other one , and uh then you only use for this one distinction . phd f: Alright . professor a: And {disfmarker} and so now you 've got a probability of the cases , and you 've got uh the probability of the finer uh categories on the other side . You multiply them where appropriate and uh {vocalsound} um phd f: I see , yeah . Mm - hmm . professor a: if they really are from independent {pause} information sources then {vocalsound} they should have different kinds of errors phd f: Mm - hmm . professor a: and roughly independent errors , and {vocalsound} it 's a good choice for {disfmarker} phd f: Mm - hmm . Mm - hmm . Yeah . professor a: Uh . Yeah , that 's a good idea . phd f: Yeah . Because , yeah , well , spectral subtraction is good and we could u we could use the fine structure to {disfmarker} to have a better estimate of the noise but {vocalsound} still there is this issue with spectral subtraction that it seems to increase the variance of {disfmarker} of {disfmarker} of professor a: Yeah . phd f: um Well it 's this musical noise which is annoying if you d you do some kind of on - line normalization after . professor a: Right . phd f: So . Um . Yeah . Well . Spectral subtraction and on - line normalization don't seem to {disfmarker} to go together very well . I professor a: Or if you do a spectral subtraction {disfmarker} do some spectral subtraction first and then do some on - line normalization then do some more spectral subtraction {disfmarker} I mean , maybe {disfmarker} maybe you can do it layers or something so it doesn't {disfmarker} doesn't hurt too much or something . phd f: Ah , yeah . professor a: But it {disfmarker} but uh , anyway I think I was sort of arguing against myself there by giving that example phd f: Yeah . professor a: uh I mean cuz I was already sort of {vocalsound} suggesting that we should be careful about not spending too much time on exactly what they 're doing In fact if you get {disfmarker} if you go into uh {disfmarker} a uh harmonics - related thing {vocalsound} it 's definitely going to be different than what they 're doing and uh uh phd f: Mm - hmm . professor a: should have some interesting properties in noise . Um . {vocalsound} I know that when have people have done {pause} um sort of the obvious thing of taking {vocalsound} uh your feature vector and adding {pause} in some variables which are {vocalsound} pitch related or uh that {disfmarker} it hasn't {disfmarker} my impression it hasn't particularly helped . Uh . Has not . phd f: It {disfmarker} it i has not , professor a: Yeah . phd f: yeah . professor a: But I think uh {pause} that 's {disfmarker} that 's a question for this uh you know extending the feature vector versus having different streams . phd f: Oh . Was it nois noisy condition ? the example that you {disfmarker} you just professor a: And {disfmarker} and it may not have been noisy conditions . phd f: Yeah . professor a: Yeah . I {disfmarker} I don't remember the example but it was {disfmarker} {vocalsound} it was on some DARPA data and some years ago and so it probably wasn't , actually phd f: Mm - hmm . Mm - hmm . Yeah . But we were thinking , we discussed with Barry about this , and {vocalsound} perhaps {vocalsound} thinking {disfmarker} we were thinking about some kind of sheet cheating experiment where we would use TIMIT professor a: Uh - huh . phd f: and see if giving the d uh , this voicing bit would help in {disfmarker} in terms of uh frame classification . professor a: Why don't you {disfmarker} why don't you just do it with Aurora ? phd f: Mmm . professor a: Just any i in {disfmarker} in each {disfmarker} in each frame phd f: Yeah , but {disfmarker} but {disfmarker} B but we cannot do the cheating , this cheating thing . grad e: We 're {disfmarker} professor a: uh {disfmarker} grad e: We need labels . professor a: Why not ? phd f: Well . Cuz we don't have {disfmarker} Well , for Italian perhaps we have , but we don't have this labeling for Aurora . We just have a labeling with word models professor a: I see . phd f: but not for phonemes . phd d: Not for foreigners . grad e: we don't have frame {disfmarker} frame level transcriptions . professor a: Um . phd d: Right . phd f: Um . {vocalsound} Yeah . professor a: But you could {disfmarker} I mean you can {disfmarker} you can align so that {disfmarker} It 's not perfect , but if you {disfmarker} if you know what was said and {disfmarker} phd b: But the problem is that their models are all word level models . So there 's no phone models {pause} that you get alignments for . phd f: Mm - hmm . professor a: Oh . phd b: You {disfmarker} So you could find out where the word boundaries are but that 's about it . professor a: Yeah . I see . grad e: S But we could use uh the {disfmarker} the noisy version that TIMIT , which {vocalsound} you know , is similar to the {disfmarker} the noises found in the TI - digits {vocalsound} um portion of Aurora . phd f: Yeah . noise , yeah . Yeah , that 's right , yep . Mmm . professor a: Yeah . phd f: Well , I guess {disfmarker} I guess we can {disfmarker} we can say that it will help , but I don't know . If this voicing bit doesn't help , uh , I think we don't have to {disfmarker} to work more about this because {disfmarker} professor a: Uh . phd f: Uh . It 's just to know if it {disfmarker} how much i it will help professor a: Yeah . phd f: and to have an idea of how much we can gain . professor a: Right . I mean in experiments that we did a long time ago phd f: Mmm . professor a: and different ta it was probably Resource Management or something , um , I think you were getting {pause} something like still eight or nine percent error on the voicing , as I recall . And um , so um grad e: Another person 's voice . professor a: what that said is that , sort of , left to its own devices , like without the {disfmarker} a strong language model and so forth , that you would {disfmarker} {vocalsound} you would make significant number of errors {vocalsound} just with your uh probabilistic machinery in deciding phd b: It also {disfmarker} professor a: one oh phd b: Yeah , the {disfmarker} though I think uh there was one problem with that in that , you know , we used canonical mapping so {vocalsound} our truth may not have really been {pause} true to the acoustics . professor a: Uh - huh . grad e: Hmm . phd b: So . phd f: Mmm . professor a: Yeah . Well back twenty years ago when I did this voiced - unvoiced stuff , we were getting more like {vocalsound} ninety - seven or ninety - eight percent correct in voicing . But that was {vocalsound} speaker - dependent {vocalsound} actually . We were doing training {vocalsound} on a particular announcer phd f: Mm - hmm . professor a: and {disfmarker} and getting a {vocalsound} very good handle on the features . phd f: Mm - hmm . professor a: And we did this complex feature selection thing where we looked at all the different possible features one could have for voicing and {disfmarker} {vocalsound} and {disfmarker} and uh {disfmarker} and exhaustively searched {vocalsound} all size subsets and {disfmarker} and uh {disfmarker} for {disfmarker} for that particular speaker and you 'd find you know the five or six features which really did well on them . phd b: Wow ! phd f: Mm - hmm . professor a: And then doing {disfmarker} doing all of that we could get down to two or three percent error . But that , again , was speaker - dependent with {vocalsound} lots of feature selection phd f: Mm - hmm . professor a: and a very complex sort of thing . phd f: Mmm . professor a: So I would {disfmarker} I would believe {vocalsound} that uh it was quite likely that um looking at envelope only , that we 'd be {vocalsound} significantly worse than that . phd f: Mm - hmm . professor a: Uh . phd f: And the {disfmarker} all the {disfmarker} the SpeechCorders ? what 's the idea behind ? Cuz they {disfmarker} they have to {disfmarker} Oh , they don't even have to detect voiced spe speech ? professor a: The modern ones don't do a {disfmarker} {vocalsound} a simple switch . phd f: They just work on the code book professor a: They work on the code book excitation . phd f: and find out the best excitation . professor a: Yeah they do {vocalsound} analysis - by - synthesis . They try {disfmarker} they {disfmarker} they try every {disfmarker} every possible excitation they have in their code book and find the one that matches best . phd f: Yeah . Mmm . Alright . Yeah . So it would not help . professor a: Yeah . grad e: Hmm . professor a: Uh . O K . phd b: Can I just mention one other interesting thing ? professor a: Yeah . phd b: Um . One of the ideas that we {pause} had come up with last week for things to try to {vocalsound} improve the system {disfmarker} Um . Actually I {disfmarker} I s we didn't {disfmarker} I guess I wrote this in after the meeting b but {vocalsound} the thought I had was um looking at the language model that 's used in the HTK recognizer , which is basically just a big {vocalsound} loop , grad e: Mm - hmm . phd b: right ? So you {disfmarker} it goes " digit " phd d: Mm - hmm . phd b: and then that can be {disfmarker} either go to silence or go to another digit , which {disfmarker} That model would allow for the production of {vocalsound} infinitely long sequences of digits , right ? professor a: Right . phd b: So . I thought " well I 'm gonna just look at the {disfmarker} what actual digit strings do occur in the training data . " professor a: Right . phd b: And the interesting thing was it turns out that there are no sequences of two - long or three - long digit strings {pause} in any of the Aurora training data . So it 's either one , four , five , six , uh up to eleven , and then it skips and then there 's some at sixteen . professor a: But what about the testing data ? phd b: Um . I don't know . I didn't look at the test data yet . professor a: Yeah . I mean if there 's some testing data that has {disfmarker} has {disfmarker} {vocalsound} has two or three {disfmarker} phd b: So . Yeah . But I just thought that was a little odd , that there were no two or three long {disfmarker} Sorry . So I {disfmarker} I {disfmarker} just for the heck of it , I made a little grammar which um , you know , had it 's separate path {pause} for each length digit string you could get . So there was a one - long path and there was a four - long and a five - long professor a: Mm - hmm . phd b: and I tried that and it got way worse . There were lots of deletions . professor a: Mm - hmm . phd b: So it was {disfmarker} {vocalsound} you know , I {disfmarker} I didn't have any weights of these paths or {disfmarker} I didn't have anything like that . professor a: Mm - hmm . phd b: And I played with tweaking the {vocalsound} word transition penalties a bunch , but I couldn't go anywhere . professor a: Hmm . phd b: But um . I thought " well if I only allow {disfmarker} " Yeah , I guess I should have looked at {disfmarker} to see how often there was a mistake where a two - long or a three - long path was actually put out as a hypothesis . Um . But . professor a: Hmm . phd b: So to do that right you 'd probably want to have {disfmarker} {vocalsound} allow for them all but then have weightings and things . So . I just thought that was a interesting {vocalsound} thing about the data . professor a: OK . So we 're gonna read some more digit strings I guess ? phd b: Yeah . You want to go ahead , Morgan ? professor a: Sure .