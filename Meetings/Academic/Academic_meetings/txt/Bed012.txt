grad b: So I guess this is more or less now just to get you up to date , Johno . This is what , uh , grad c: This is a meeting for me . grad b: um , Eva , Bhaskara , and I did . grad d: Did you add more stuff to it ? {pause} later ? grad b: Um . Why ? grad d: Um . I don't know . There were , like , the {disfmarker} you know , @ @ and all that stuff . But . I thought you {disfmarker} you said you were adding stuff grad b: Uh , no . grad d: but {pause} I don't know . grad b: This is {disfmarker} Um , Ha ! Very nice . Um , so we thought that , {vocalsound} We can write up uh , an element , and {disfmarker} for each of the situation nodes that we observed in the Bayes - net ? So . What 's the situation like at the entity that is mentioned ? if we know anything about it ? Is it under construction ? Or is it on fire or something {pause} happening to it ? Or is it stable ? and so forth , going all the way um , f through Parking , Location , Hotel , Car , Restroom , @ @ {comment} Riots , Fairs , Strikes , or Disasters . grad c: So is {disfmarker} This is {disfmarker} A situation are {disfmarker} is all the things which can be happening right now ? Or , what is the situation type ? grad b: That 's basically {pause} just specifying the {disfmarker} the input for the {disfmarker} w what 's grad c: Oh , I see y Why are you specifying it in XML ? grad b: Um . Just because it forces us to be specific about the values {pause} here ? grad c: OK . grad b: And , also , I mean , this is a {disfmarker} what the input is going to be . Right ? So , we will , uh {disfmarker} This is a schema . This is {disfmarker} grad c: Well , yeah . I just don't know if this is th l what the {disfmarker} Does {disfmarker} This is what Java Bayes takes ? as a Bayes - net spec ? grad b: No , because I mean if we {disfmarker} I mean we 're sure gonna interface to {disfmarker} We 're gonna get an XML document from somewhere . Right ? And that XML document will say " We are able to {disfmarker} We were able to observe that w the element , um , @ @ {comment} of the Location that the car is near . " So that 's gonna be {disfmarker} {vocalsound} {comment} Um . grad c: So this is the situational context , everything in it . Is that what Situation is short for , shi situational context ? grad b: Yep . grad c: OK . grad b: So this is just , again , a an XML schemata which defines a set of possible , uh , permissible XML structures , which we view as input into the Bayes - net . Right ? grad c: And then we can r {pause} uh possibly run one of them uh transformations ? That put it into the format that the Bayes n or Java Bayes or whatever wants ? grad b: Yea - Are you talking {disfmarker} are you talking about the {disfmarker} the structure ? grad c: Well it {disfmarker} grad b: I mean when you observe a node . grad c: When you {disfmarker} when you say {pause} the input to the {pause} v Java Bayes , {comment} it takes a certain format , grad b: Um - hmm . grad c: right ? Which I don't think is this . Although I don't know . grad b: No , it 's certainly not this . Nuh . grad c: So you could just {disfmarker} Couldn't you just run a {disfmarker} grad b: XSL . {comment} Yeah . grad c: Yeah . To convert it into the Java Bayes for format ? grad b: Yep . grad c: OK . grad b: That 's {disfmarker} That 's no problem , but I even think that , um {disfmarker} I mean , once {disfmarker} Once you have this sort of as {disfmarker} running as a module {disfmarker} Right ? What you want is {disfmarker} You wanna say , " OK , give me the posterior probabilities of the Go - there {pause} node , when this is happening . " Right ? When the person said this , the car is there , it 's raining , and this is happening . And with this you can specify the {disfmarker} what 's happening in the situation , and what 's happening with the user . So we get {disfmarker} After we are done , through the Situation we get the User Vector . So , this is a {disfmarker} grad c: So this is just a specification of all the possible inputs ? grad b: Yep . And , all the possible outputs , too . So , we have , um , for example , the , uh , Go - there decision node grad c: OK . grad b: which has two elements , going - there and its posterior probability , and not - going - there and its posterior probability , because the output is always gonna be all the decision nodes and all the {disfmarker} the {disfmarker} a all the posterior probabilities for all the values . grad c: And then we would just look at the , eh , Struct that we wanna look at in terms of if {disfmarker} if we 're only asking about one of the {disfmarker} So like , if I 'm just interested in the going - there node , I would just pull that information out of the Struct that gets return that would {disfmarker} that Java Bayes would output ? grad b: Um , pretty much , yes , but I think it 's a little bit more complex . As , if I understand it correctly , it always gives you all the posterior probabilities for all the values of all decision nodes . So , when we input something , we always get the , uh , posterior probabilities for all of these . Right ? grad c: OK . grad b: So there is no way of telling it t not to tell us about the EVA {pause} values . grad c: Yeah , wait I agree , that 's {disfmarker} yeah , use {disfmarker} oh , uh {pause} Yeah , OK . grad b: So {disfmarker} so we get this whole list of {disfmarker} of , um , things , and the question is what to do with it , what to hand on , how to interpret it , in a sense . So y you said if you {disfmarker} " I 'm only interested in whether he wants to go there or not " , then I just look at that node , look which one {disfmarker} grad c: Look at that Struct in the output , grad b: Yep . grad c: right ? grad b: Look at that Struct in the {disfmarker} the output , even though I wouldn't call it a " Struct " . But . grad c: Well i well , it 's an XML Structure that 's being res returned , grad b: Oh . Mm - hmm . grad c: right ? grad b: So every part of a structure is a " Struct " . Yeah . grad c: Yeah , I just uh {disfmarker} I just was {disfmarker} abbreviated it to Struct in my head , and started going with that . grad b: That element or object , I would say . grad c: Not a C Struct . That 's not what I was trying to k grad b: Yeah . grad c: though yeah . grad b: OK . And , um , the reason is {disfmarker} why I think it 's a little bit more complex or why {disfmarker} why we can even think about it as an interesting problem in and of itself is {disfmarker} Um . So . The , uh {disfmarker} Let 's look at an example . grad c: Well , w wouldn't we just take the structure that 's outputted and then run another transformation on it , that would just dump the one that we wanted out ? grad b: Yeah . w We 'd need to prune . Right ? Throw things away . grad c: Well , actually , you don't even need to do that with XML . grad b: No grad c: D Can't you just look at one specific {disfmarker} grad b: Yeah , exactly . The {disfmarker} @ @ {comment} Xerxes allows you to say , u " Just give me the value of that , and that , and that . " But , we don't really know what we 're interested in {pause} before we look at the complete {disfmarker} at {disfmarker} at the overall result . So the person said , um , " Where is X ? " and so , we want to know , um , is {disfmarker} Does he want info ? o on this ? or know the location ? Or does he want to go there ? Let 's assume this is our {disfmarker} our question . grad c: Sure . grad b: Nuh ? So . Um . Do this in Perl . So we get {disfmarker} OK . Let 's assume this is the output . So . We should con be able to conclude from that that {disfmarker} I mean . It 's always gonna give us a value of how likely we think i it is that he wants to go there and doesn't want to go there , or how likely it is that he wants to get information . But , maybe w we should just reverse this to make it a little bit more delicate . So , does he wanna know where it is ? or does he wanna go there ? grad c: He wants to know where it is . grad b: Right . I {disfmarker} I {disfmarker} I tend to agree . And if it 's {disfmarker} If {disfmarker} grad c: Well now , y I mean , you could {disfmarker} grad b: And i if there 's sort of a clear winner here , and , um {disfmarker} and this is pretty , uh {disfmarker} indifferent , then we {disfmarker} then we might conclude that he actually wants to just know where , uh t uh , he does want to go there . grad c: Uh , out of curiosity , is there a reason why we wouldn't combine these three nodes ? into one smaller subnet ? that would just basically be {pause} the question for {disfmarker} We have " where is X ? " is the question , right ? That would just be Info - on or Location ? Based upon {disfmarker} grad b: Or Go - there . A lot of people ask that , if they actually just wanna go there . People come up to you on campus and say , " Where 's the library ? " You 're gonna say {disfmarker} y you 're gonna say , g " Go down that way . " You 're not gonna say " It 's {disfmarker} It 's five hundred yards away from you " or " It 's north of you " , or {disfmarker} " it 's located {disfmarker} " grad c: Well , I mean {disfmarker} But the {disfmarker} there 's {disfmarker} So you just have three decisions for the final node , that would link thes these three nodes in the net together . grad b: Um . I don't know whether I understand what you mean . But . Again , in this {disfmarker} Given this input , we , also in some situations , may wanna postulate an opinion whether that person wants to go there now the nicest way , use a cab , or so s wants to know it {disfmarker} wants to know where it is because he wants something fixed there , because he wants to visit t it or whatever . So , it {disfmarker} n I mean {disfmarker} a All I 'm saying is , whatever our input is , we 're always gonna get the full output . And some {disfmarker} some things will always be sort of too {disfmarker} not significant enough . grad c: Wha Or i or i it 'll be tight . You won't {disfmarker} it 'll be hard to decide . grad b: Yep . grad c: But I mean , I guess {disfmarker} I guess the thing is , uh , this is another , smaller , case of reasoning in the case of an uncertainty , which makes me think Bayes - net should be the way to solve these things . So if you had {disfmarker} If for every construction , grad b: Oh ! grad c: right ? you could say , " Well , there {disfmarker} Here 's the Where - Is construction . " And for the Where - Is construction , we know we need to l look at this node , that merges these three things together grad b: Mm - hmm . grad c: as for th to decide the response . And since we have a finite number of constructions that we can deal with , we could have a finite number of nodes . grad b: OK . Mm - hmm . grad c: Say , if we had to y deal with arbitrary language , it wouldn't make any sense to do that , because there 'd be no way to generate the nodes for every possible sentence . grad b: Mm - hmm . grad c: But since we can only deal with a finite amount of stuff {disfmarker} grad b: So , basically , the idea is to f to feed the output of that belief - net into another belief - net . grad c: Yeah , so basically take these three things and then put them into another belief - net . grad b: But , why {disfmarker} why {disfmarker} why only those three ? Why not the whol grad c: Well , I mean , d For the Where - Is question . So we 'd have a node for the Where - Is question . grad b: Yeah . But we believe that all the decision nodes are {disfmarker} can be relevant for the Where - Is , and the Where {disfmarker} How - do - I - get - to or the Tell - me - something - about . grad c: You can come in if you want . grad b: Yes , it is allowed . grad c: As long as y you 're not wearing your h your h headphones . Well , I do I {disfmarker} See , I don't know if this is a {pause} good idea or not . I 'm just throwing it out . But uh , it seems like we could have {disfmarker} I mea or uh we could put all of the all of the r information that could also be relevant {pause} into the Where - Is node answer grad b: Mm - hmm . Yep . grad c: node thing stuff . And uh {disfmarker} grad d: OK . grad b: I mean {disfmarker} Let 's not forget we 're gonna get some very strong {pause} input from {pause} these sub dis from these discourse things , right ? So . " Tell me the location of X . " Nuh ? Or " Where is X located at ? " grad c: We u grad b: Nuh ? grad c: Yeah , I know , but the Bayes - net would be able to {disfmarker} The weights on the {disfmarker} on the nodes in the Bayes - net would be able to do all that , grad b: Mm - hmm . grad c: wouldn't it ? Here 's a k Oh ! Oh , I 'll wait until you 're {pause} plugged in . Oh , don't sit there . Sit here . You know how you don't like that one . It 's OK . That 's the weird one . That 's the one that 's painful . That hurts . It hurts so bad . I 'm h I 'm happy that they 're recording that . That headphone . The headphone {pause} that you have to put on backwards , with the little {disfmarker} little thing {disfmarker} and the little {disfmarker} little foam block on it ? It 's a painful , painful microphone . grad b: I think it 's th called " the Crown " . grad c: The crown ? grad d: What ? grad b: Yeah , versus " the Sony " . grad a: The Crown ? Is that the actual name ? OK . grad b: Mm - hmm . The manufacturer . grad c: I don't see a manufacturer on it . grad b: You w grad c: Oh , wait , here it is . h This thingy . Yeah , it 's " The Crown " . The crown of pain ! grad a: Yes . grad b: You 're on - line ? grad c: Are you {disfmarker} are your mike o Is your mike on ? grad a: Indeed . grad c: OK . So you 've been working with these guys ? You know what 's going on ? grad a: Yes , I have . And , I do . Yeah , alright . s So where are we ? grad c: Excellent ! grad b: We 're discussing this . grad a: I don't think it can handle French , but anyway . grad b: So . Assume we have something coming in . A person says , " Where is X ? " , and we get a certain {disfmarker} We have a Situation vector and a User vector and everything is fine ? An - an and {disfmarker} and our {disfmarker} and our {disfmarker} grad c: Did you just sti Did you just stick the m the {disfmarker} the {disfmarker} the microphone actually in the tea ? grad a: No . grad b: And , um , grad a: I 'm not drinking tea . What are you talking about ? grad c: Oh , yeah . Sorry . grad b: let 's just assume our Bayes - net just has three decision nodes for the time being . These three , he wants to know something about it , he wants to know where it is , he wants to go there . grad c: In terms of , these would be wha how we would answer the question Where - Is , right ? We u This is {disfmarker} i That 's what you s it seemed like , explained it to me earlier grad b: Yeah , but , mmm . grad c: w We {disfmarker} we 're {disfmarker} we wanna know how to answer the question " Where is X ? " grad b: Yeah . No , I can {disfmarker} I can do the Timing node in here , too , and say " OK . " grad c: Well , yeah , but in the s uh , let 's just deal with the s the simple case of we 're not worrying about timing or anything . We just want to know how we should answer " Where is X ? " grad b: OK . And , um , OK , and , Go - there has two values , right ? , Go - there and not - Go - there . Let 's assume those are the posterior probabilities of that . grad a: Mm - hmm . grad b: Info - on has True or False and Location . So , he wants to know something about it , and he wants to know something {disfmarker} he wants to know Where - it - is , grad a: Excuse me . grad b: has these values . And , um , grad c: Oh , I see why we can't do that . grad b: And , um , in this case we would probably all agree that he wants to go there . Our belief - net thinks he wants to go there , grad a: Yeah . grad b: right ? grad a: Mm - hmm . grad b: In the , uh , whatever , if we have something like this here , and this like that and maybe here also some {disfmarker} grad a: You should probably {comment} make them out of {disfmarker} Yeah . grad b: something like that , grad c: Well , it grad b: then we would guess , " Aha ! He , our belief - net , {comment} has s stronger beliefs that he wants to know where it is , than actually wants to go {pause} there . " Right ? grad c: That it {disfmarker} Doesn't this assume , though , that they 're evenly weighted ? grad d: True . grad c: Like {disfmarker} I guess they are evenly weighted . grad a: The different decision nodes , you mean ? grad c: Yeah , the Go - there , the Info - on , and the Location ? grad a: Well , d yeah , this is making the assumption . Yes . grad c: Like {disfmarker} grad b: What do you mean by " differently weighted " ? They don't feed into anything really anymore . grad a: But I mean , why do we {disfmarker} grad c: Or I jus grad a: If we trusted the Go - there node more th much more than we trusted the other ones , then we would conclude , even in this situation , that he wanted to go there . grad c: Le grad a: So , in that sense , we weight them equally right now . grad b: OK . Makes sense . Yeah . But {disfmarker} grad c: So the But I guess the k the question {disfmarker} that I was as er wondering or maybe Robert was proposing to me is {disfmarker} How do we d make the decision on {disfmarker} as to {disfmarker} which one to listen to ? grad a: Yeah , so , the final d decision is the combination of these three . So again , it 's {disfmarker} it 's some kind of , uh {disfmarker} grad c: Bayes - net . grad a: Yeah , sure . grad c: OK so , then , the question i So then my question is t to you then , would be {disfmarker} So is the only r reason we can make all these smaller Bayes - nets , because we know we can only deal with a finite set of constructions ? Cuz oth If we 're just taking arbitrary language in , we couldn't have a node for every possible question , you know ? grad a: A decision node for every possible question , you mean ? grad c: Well , I {disfmarker} like , in the case of {disfmarker} Yeah . In the ca Any piece of language , we wouldn't be able to answer it with this system , b if we just h Cuz we wouldn't have the correct node . Basically , w what you 're s proposing is a n Where - Is node , right ? grad a: Yeah . grad c: And {disfmarker} and if we {disfmarker} And if someone {disfmarker} says , you know , uh , something in Mandarin to the system , we 'd - wouldn't know which node to look at to answer that question , grad a: So is {disfmarker} Yeah . Yeah . grad c: right ? grad b: Mmm ? grad c: So , but {disfmarker} but if we have a finite {disfmarker} What ? grad b: I don't see your point . What {disfmarker} what {disfmarker} what I am thinking , or what we 're about to propose here is we 're always gonna get the whole list of values and their posterior probabilities . And now we need an expert system or belief - net or something that interprets that , that looks at all the values and says , " The winner is Timing . Now , go there . " " Uh , go there , Timing , now . " Or , " The winner is Info - on , Function - Off . " So , he wants to know {pause} something about it , and what it does . Nuh ? Uh , regardless of {disfmarker} of {disfmarker} of the input . Wh - Regardle grad c: Yeah , but But how does the expert {disfmarker} but how does the expert system know {disfmarker} how who which one to declare the winner , if it doesn't know the question it is , and how that question should be answered ? grad b: Based on the k what the question was , so what the discourse , the ontology , the situation and the user model gave us , we came up with these values for these decisions . grad c: Yeah I know . But how do we weight what we get out ? As , which one i Which ones are important ? So my i So , if we were to it with a Bayes - net , we 'd have to have a node {disfmarker} for every question that we knew how to deal with , that would take all of the inputs and weight them appropriately for that question . grad b: Mm - hmm . grad c: Does that make sense ? Yay , nay ? grad a: Um , I mean , are you saying that , what happens if you try to scale this up to the situation , or are we just dealing with arbitrary language ? grad c: We {disfmarker} grad a: Is that your point ? grad c: Well , no . I {disfmarker} I guess my question is , Is the reason that we can make a node f or {disfmarker} OK . So , lemme see if I 'm confused . Are we going to make a node for every question ? Does that make sense ? {disfmarker} grad a: For every question ? grad c: Or not . grad a: Like {disfmarker} grad c: Every construction . grad a: Hmm . I don't {disfmarker} Not necessarily , I would think . I mean , it 's not based on constructions , it 's based on things like , uh , there 's gonna be a node for Go - there or not , and there 's gonna be a node for Enter , View , Approach . grad c: Wel W OK . So , someone asked a question . grad a: Yeah . grad c: How do we decide how to answer it ? grad b: Well , look at {disfmarker} look {disfmarker} Face yourself with this pr question . You get this {disfmarker} You 'll have {disfmarker} y This is what you get . And now you have to make a decision . What do we think ? What does this tell us ? And not knowing what was asked , and what happened , and whether the person was a tourist or a local , because all of these factors have presumably already gone into making these posterior probabilities . What {disfmarker} what we need is a {disfmarker} just a mechanism that says , " Aha ! There is {disfmarker} " grad c: Yeah . I just don't think a " winner - take - all " type of thing is the {disfmarker} grad a: I mean , in general , like , we won't just have those three , right ? We 'll have , uh , like , many , many nodes . So we have to , like {disfmarker} So that it 's no longer possible to just look at the nodes themselves and figure out what the person is trying to say . grad b: Yep . Because there are interdependencies , right ? The uh {disfmarker} Uh , no . So if {disfmarker} if for example , the Go - there posterior possibility is so high , um , uh , w if it 's {disfmarker} if it has reached {disfmarker} reached a certain height , then all of this becomes irrelevant . So . If {disfmarker} even if {disfmarker} if the function or the history or something is scoring pretty good on the true node , true value {disfmarker} grad c: Wel I don't know about that , cuz that would suggest that {disfmarker} I mean {disfmarker} grad b: He wants to go there and know something about it ? grad c: Do they have to be mutual Yeah . Do they have to be mutually exclusive ? grad b: I think to some extent they are . Or maybe they 're not . grad c: Cuz I , uh {disfmarker} The way you describe what they meant , they weren't mutu uh , they didn't seem mutually exclusive to me . grad b: Well , if he doesn't want to go there , even if the Enter posterior proba So . grad c: Wel grad b: Go - there is No . Enter is High , and Info - on is High . grad c: Well , yeah , just out of the other three , though , that you had in the {disfmarker} grad b: Hmm ? grad c: those three nodes . The - d They didn't seem like they were mutually exclusive . grad b: No , there 's {disfmarker} No . But {disfmarker} It 's through the {disfmarker} grad c: So th s so , yeah , but some {disfmarker} So , some things would drop out , and some things would still be important . grad b: Mm - hmm . grad c: But I guess what 's confusing me is , if we have a Bayes - net to deal w another Bayes - net to deal with this stuff , grad a: Mm - hmm . grad c: you know , uh , is the only reason {disfmarker} OK , so , I guess , if we have a Ba - another Bayes - net to deal with this stuff , the only r reason {pause} we can design it is cuz we know what each question is asking ? grad a: Yeah . I think that 's true . grad c: And then , so , the only reason {disfmarker} way we would know what question he 's asking is based upon {disfmarker} Oh , so if {disfmarker} Let 's say I had a construction parser , and I plug this in , I would know what each construction {disfmarker} the communicative intent of the construction was grad a: Mm - hmm . grad c: and so then I would know how to weight the nodes appropriately , in response . So no matter what they said , if I could map it onto a Where - Is construction , I could say , " ah ! grad a: Ge Mm - hmm . grad c: well the the intent , here , was Where - Is " , grad a: OK , right . grad c: and I could look at those . grad a: Yeah . Yes , I mean . Sure . You do need to know {disfmarker} I mean , to have that kind of information . grad b: Hmm . Yeah , I 'm also agreeing that {pause} a simple pru {comment} Take the ones where we have a clear winner . Forget about the ones where it 's all sort of middle ground . Prune those out and just hand over the ones where we have a winner . Yeah , because that would be the easiest way . We just compose as an output an XML mes {vocalsound} message that says . " Go there {pause} now . " " Enter historical information . " And not care whether that 's consistent with anything . Right ? But in this case if we say , " definitely he doesn't want to go there . He just wants to know where it is . " or let 's call this {disfmarker} this " Look - At - H " He wants to know something about the history of . So he said , " Tell me something about the history of that . " Now , the e But for some reason the Endpoint - Approach gets a really high score , {pause} too . We can't expect this to be sort of at O point {comment} three , three , three , O point , three , three , three , O point , three , three , three . Right ? Somebody needs to zap that . You know ? Or know {disfmarker} There needs to be some knowledge that {disfmarker} grad c: We {disfmarker} Yeah , but , the Bayes - net that would merge {disfmarker} I just realized that I had my hand in between my mouth and my micr er , my and my microphone . So then , the Bayes - net that would merge there , that would make the decision between Go - there , Info - on , and Location , would have a node to tell you which one of those three you wanted , and based upon that node , then you would look at the other stuff . grad b: Yep . Yep . grad c: I mean , it i Does that make sense ? grad b: Yep . It 's sort of one of those , that 's {disfmarker} It 's more like a decision tree , if {disfmarker} if you want . You first look o at the lowball ones , grad c: Yeah , i grad b: and then {disfmarker} grad c: Yeah , I didn't intend to say that every possible {disfmarker} OK . There was a confusion there , k I didn't intend to say every possible thing should go into the Bayes - net , because some of the things aren't relevant in the Bayes - net for a specific question . Like the Endpoint is not necessarily relevant in the Bayes - net for Where - Is until after you 've decided whether you wanna go there or not . grad b: Mm - hmm . grad a: Right . grad c: Show us the way , Bhaskara . grad a: I guess the other thing is that um , yeah . I mean , when you 're asked a specific question and you don't even {disfmarker} Like , if you 're asked a Where - Is question , you may not even look {disfmarker} like , ask for the posterior probability of the , uh , EVA node , right ? Cuz , that 's what {disfmarker} I mean , in the Bayes - net you always ask for the posterior probability of a specific node . So , I mean , you may not even bother to compute things you don't need . grad b: Um . Aren't we always computing all ? grad a: No . You can compute , uh , the posterior probability of one subset of the nodes , given some other nodes , but totally ignore some other nodes , also . Basically , things you ignore get marginalized over . grad b: Yeah , but that 's {disfmarker} that 's just shifting the problem . Then you would have to make a decision , grad a: Yeah . So you have to make {disfmarker} grad b: " OK , if it 's a Where - Is question , which decision nodes do I query ? " grad a: Yeah . Yes . But I would think that 's what you want to do . grad b: That 's un grad a: Right ? grad b: Mmm . grad d: Well , eventually , you still have to pick out which ones you look at . grad b: Yeah . grad d: So it 's pretty much the same problem , grad b: Yeah {disfmarker} it 's {disfmarker} it 's {disfmarker} it 's apples and oranges . grad d: isn't it ? grad b: Nuh ? I mean , maybe it does make a difference in terms of performance , computational time . grad a: Mm - hmm . grad b: So either you always have it compute all the posterior possibilities for all the values for all nodes , and then prune the ones you think that are irrelevant , grad a: Mmm . grad b: or you just make a p @ @ {comment} a priori estimate of what you think might be relevant and query those . grad a: Yeah . grad c: So basically , you 'd have a decision tree {pause} query , {pause} Go - there . If k if that 's false , query this one . If that 's true , query that one . And just basically do a binary search through the {disfmarker} ? grad a: I don't know if it would necessarily be that , uh , complicated . But , uh {disfmarker} I mean , it w grad c: Well , in the case of Go - there , it would be . In the case {disfmarker} Cuz if you needed an If y If Go - there was true , you 'd wanna know what endpoint was . And if it was false , you 'd wanna d look at either Lo - Income Info - on or History . grad a: Yeah . That 's true , I guess . Yeah , {vocalsound} so , in a way you would have that . grad c: Also , I 'm somewhat boggled by that Hugin software . grad a: OK , why 's that ? grad c: I can't figure out how to get the probabilities into it . Like , I 'd look at {disfmarker} grad a: Mm - hmm . grad c: It 's somewha It 's boggling me . grad a: OK . Alright . Well , hopefully it 's {pause} fixable . grad c: Ju grad a: It 's {disfmarker} there 's a {disfmarker} grad c: Oh yeah , yeah . I d I just think I haven't figured out what {disfmarker} the terms in Hugin mean , versus what Java Bayes terms are . grad a: OK . grad b: Um , by the way , are {disfmarker} Do we know whether Jerry and Nancy are coming ? grad a: So we can figure this out . grad b: Or {disfmarker} ? grad a: They should come when they 're done their stuff , basically , whenever that is . So . grad c: What d what do they need to do left ? grad a: Um , I guess , Jerry needs to enter marks , but I don't know if he 's gonna do that now or later . But , uh , if he 's gonna enter marks , it 's gonna take him awhile , I guess , and he won't be here . grad c: And what 's Nancy doing ? grad a: Nancy ? Um , she was sorta finishing up the , uh , calculation of marks and assigning of grades , but I don't know if she should be here . Well {disfmarker} or , she should be free after that , so {disfmarker} assuming she 's coming to this meeting . I don't know if she knows about it . grad c: She 's on the email list , right ? grad a: Is she ? OK . grad b: Mm - hmm . OK . Because basically , what {disfmarker} where we also have decided , prior to this meeting is that we would have a rerun of the three of us sitting together grad d: OK . grad b: sometime {pause} this week {pause} again grad a: OK . grad b: and finish up the , uh , values of this . So we have , uh {disfmarker} Believe it or not , we have all the bottom ones here . grad c: Well , I {disfmarker} grad d: You added a bunch of {pause} nodes , for {disfmarker} ? grad b: Yep . We {disfmarker} we {disfmarker} we have {disfmarker} Actually what we have is this line . grad d: OK . grad b: Right ? grad c: Uh , what do the , uh , structures do ? grad b: Hmm ? grad c: So the {disfmarker} the {disfmarker} the {disfmarker} For instance , this Location node 's got two inputs , grad a: Four inputs . grad b: Hmm . grad c: that one you {disfmarker} grad b: Four . grad a: Those are {disfmarker} The bottom things are inputs , also . grad c: Oh , I see . grad a: Yeah . grad c: OK , that was OK . That makes a lot more sense to me now . grad b: Yep . grad c: Cuz I thought it was like , that one in Stuart 's book about , you know , the {disfmarker} grad a: Alarm in the dog ? grad c: U Yeah . grad a: Yeah . grad c: Or the earthquake and the alarm . grad a: Sorry . Yeah , I 'm confusing two . grad c: Yeah , there 's a dog one , too , but that 's in Java Bayes , grad a: Right . grad c: isn't it ? grad a: Maybe . grad c: But there 's something about bowel problems or something with the dog . grad a: Yeah . grad b: And we have all the top ones , all the ones to which no arrows are pointing . What we 're missing are the {disfmarker} these , where arrows are pointing , where we 're combining top ones . So , we have to come up with values for this , and this , this , this , and so forth . And maybe just fiddle around with it a little bit more . And , um . And then it 's just , uh , edges , many of edges . And , um , we won't {comment} meet next Monday . So . grad c: Cuz of Memorial Day ? grad a: We 'll meet next Tuesday , I guess . grad b: Yep . Yeah . grad c: When 's Jerry leaving for {disfmarker} Italia ? grad b: On {disfmarker} on Friday . grad a: Which Friday ? grad b: This {disfmarker} this Friday . grad a: OK . grad d: Oh . This Friday ? grad c: Ugh . grad b: This Friday . grad c: As in , four days ? grad b: Yep . grad c: Or , three days ? grad a: Is he {disfmarker} How long is he gone for ? grad b: Two weeks . grad a: Italy , huh ? What 's , uh {disfmarker} what 's there ? grad b: Well , it 's a country . Buildings . People . grad a: Pasta . grad c: But it 's not a conference or anything . grad b: Hmm ? grad c: He 's just visiting . grad a: Right . Just visiting . grad b: Vacation . grad a: It 's a pretty nice place , in my brief , uh , encounter with it . grad b: Do you guys {disfmarker} Oh , yeah . So . Part of what we actually want to do is sort of schedule out what we want to surprise him with when {disfmarker} when he comes back . Um , so {disfmarker} grad c: Oh , I think we should disappoint him . grad b: Yeah ? You {disfmarker} or have a finished construction parser and a working belief - net , and uh {disfmarker} grad c: That wouldn't be disappointing . I think w we should do absolutely no work for the two weeks that he 's gone . grad b: Well , that 's actually what I had planned , personally . I had {disfmarker} I {disfmarker} I had sort of scheduled out in my mind that you guys do a lot of work , and I do nothing . And then , I sort of {disfmarker} grad c: Oh , yeah , that sounds good , too . grad b: sort of bask in {disfmarker} in your glory . But , uh , i do you guys have any vacation plans , because I myself am going to be , um , gone , but this is actually not really important . Just this weekend we 're going camping . grad c: Yeah , I 'm wanna be this {disfmarker} gone this weekend , too . grad b: Ah . But we 're all going to be here on Tuesday again ? Looks like it ? grad d: Yeah . grad b: OK , then . Let 's meet {disfmarker} meet again next Tuesday . And , um , finish up this Bayes - net . And once we have finished it , I guess we can , um {disfmarker} and that 's going to be more just you and me , because Bhaskara is doing probabilistic , recursive , structured , object - oriented , uh , grad c: Killing machines ! grad b: reasoning machines . grad a: Yes . grad b: And , um {disfmarker} grad c: Killing , reasoning . What 's the difference ? grad d: Wait . So you 're saying , next Tuesday , is it the whole group meeting , or just us three working on it , or {disfmarker} or {disfmarker} ? grad b: Uh . The whole group . And we present our results , our final , grad d: OK . grad b: definite {disfmarker} grad d: So , when you were saying we {pause} need to do a re - run of , like {disfmarker} grad a: h What ? grad d: What {disfmarker} Like , just working out the rest of the {disfmarker} grad b: Yeah . We should do this th the upcoming days . grad d: This week ? grad b: So , this week , yeah . grad c: When you say , " the whole group " , you mean {pause} the four of us , and Keith ? grad d: OK . grad b: And , Ami might . grad c: Ami might be here , and it 's possible that Nancy 'll be here ? grad b: Yep . grad c: So , yeah . grad b: Because , th you know , once we have the belief - net done {disfmarker} grad c: You 're just gonna have to explain it to me , then , on Tuesday , how it 's all gonna work out . You know . grad b: We will . OK . Because then , once we have it sort of up and running , then we can start you know , defining the interfaces and then feed stuff into it and get stuff out of it , and then hook it up to some fake construction parser and {disfmarker} grad c: That you will have in about nine months or so . grad b: Yeah . grad c: Yeah . grad b: And , um , grad c: The first bad version 'll be done in nine months . grad b: Yeah , I can worry about the ontology interface and you can {disfmarker} Keith can worry about the discourse . I mean , this is pretty {disfmarker} Um , I mean , I {disfmarker} I {disfmarker} I hope everybody uh knows that these are just going to be uh dummy values , right ? grad a: Which {disfmarker} grad b: where the {disfmarker} grad a: Which ones ? grad b: S so {disfmarker} so if the endpoint {disfmarker} if the Go - there is Yes and No , then Go - there - discourse will just be fifty - fifty . Right ? grad a: Um , what do you mean ? If the Go - there says No , then the Go - there is {disfmarker} grad d: I don't get it . grad a: I don't u understand . grad b: Um . grad a: Like , the Go - there depends on all those four things . grad b: Yep . grad a: Yeah . grad b: But , what are the values of the Go - there - discourse ? grad a: Well , it depends on the situation . If the discourse is strongly indicating that {disfmarker} grad b: Yeah , but , uh , we have no discourse input . grad a: Oh , I see . The d See , uh , specifically in our situation , D and O are gonna be , uh {disfmarker} Yeah . Sure . So , whatever . grad d: So , so far we have {disfmarker} Is that what the Keith node is ? grad b: Yep . grad d: OK . And you 're taking it out ? {pause} for now ? grad b: Well , this is D {disfmarker} grad d: Or {disfmarker} ? grad b: OK , this , I can {disfmarker} I can get it in here . grad d: All the D 's are {disfmarker} grad b: I can get it in here , so th We have the , uh , um , sk let 's {disfmarker} let 's call it " Keith - Johno grad a: Johno ? grad b: node " . There is an H {comment} somewhere printed . grad c: There you go . grad a: Yeah . People have the same problem with my name . grad b: Yeah . grad a: Oops . grad b: And , um , grad c: Does th th does the H go b before the A or after the A ? grad a: Oh , in my name ? Before the A . grad c: Yeah . OK , good . Cuz you kn When you said people have the same problem , I thought {disfmarker} Cuz my H goes after the uh e e e the v grad a: People have the inverse problem with my name . grad c: OK . I always have to check , every time y I send you an email , {comment} a past email of yours , {comment} to make sure I 'm spelling your name correctly . grad a: Yeah . That 's good . grad c: I worry about you . grad a: I appreciate that . grad b: But , when you abbreviate yourself as the " Basman " , you don't use any H 's . grad a: " Basman " ? Yeah , it 's because of the chessplayer named Michael Basman , who is my hero . grad b: OK . grad c: You 're a geek . It 's O K . I grad b: OK . grad c: How do you pronou How do you pronounce your name ? grad d: Eva . grad c: Eva ? grad a: Not Eva ? grad d: Yeah . grad c: What if I were {disfmarker} What if I were to call you Eva ? grad d: I 'd probably still respond to it . I 've had people call me Eva , but I don't know . grad c: No , not just Eva , Eva . Like if I u take the V and s pronounce it like it was a German V ? grad b: Which is F . grad c: Yeah . grad d: Um , no idea then . grad b: Voiced . grad d: What ? grad c: It sounds like an F . grad d: I {disfmarker} grad c: There 's also an F in German , grad d: OK . grad b: Well , it 's just the difference between voiced and unvoiced . grad c: which is why I {disfmarker} Yeah . grad d: OK . grad c: As long as that 's O K . grad d: Um . grad c: I mean , I might slip out and say it accidentally . That 's all I 'm saying . grad d: That 's fine . grad a: Yeah . It doesn't matter what those nodes are , anyway , because we 'll just make the weights " zero " for now . grad b: Yep . We 'll make them zero for now , because it {disfmarker} who {disfmarker} who knows what they come up with , what 's gonna come in there . OK . And , um , then should we start on Thursday ? grad a: OK . grad b: And not meet tomorrow ? grad a: Sure . grad b: OK . I 'll send an email , make a time suggestion . grad c: Wait , maybe it 's OK , so that {disfmarker} that {disfmarker} that we can {disfmarker} that we have one node per construction . Cuz even in people , like , they don't know what you 're talking about if you 're using some sort of strange construction . grad b: Yeah , they would still c sort of get the closest , best fit . grad c: Well , yeah , but I mean , the {disfmarker} uh , I mean , that 's what the construction parser would do . grad b: Mm - hmm . grad c: Uh , I mean , if you said something completely arbitrary , it would f find the closest construction , grad b: OK . grad c: right ? But if you said something that was completel er {disfmarker} h theoretically the construction parser would do that {disfmarker} But if you said something for which there was no construction whatsoever , n people wouldn't have any idea what you were talking about . grad b: Mm - hmm . grad c: Like " Bus dog fried egg . " I mean . You know . grad b: Or , if even something Chinese , for example . grad c: Or , something in Mandarin , yeah . Or Cantonese , as the case may be . What do you think about that , Bhaskara ? grad a: I mean {disfmarker} Well {disfmarker} But how many constructions do {disfmarker} could we possibly have {pause} nodes for ? grad c: In this system , or in r grad a: No , we . Like , when people do this kind of thing . grad c: Oh , when p How many constructions do people have ? grad a: Yeah . grad c: I have not {comment} the slightest idea . grad a: Is it considered to be like in {disfmarker} are they considered to be like very , uh , sort of s abstract things ? grad c: Every noun is a construction . grad a: OK , so it 's like in the {pause} thousands . grad c: The {disfmarker} Yeah . Any {disfmarker} any form - meaning pair , to my understanding , is a construction . grad a: OK . grad b: So . grad c: And form u starts at the level of noun {disfmarker} Or actually , maybe even sounds . grad b: Phoneme . Yep . grad c: Yeah . And goes upwards until you get the ditransitive construction . grad a: S grad c: And then , of course , the c I guess , maybe there can be the {disfmarker} Can there be combinations of the dit grad a: Discourse - level {pause} constructions . grad c: Yeah . The " giving a speech " construction , grad b: Rhetorical constructions . grad a: Yes . grad b: Yeah . But , I mean , you know , you can probably count {disfmarker} count the ways . I mean . grad c: It 's probab Yeah , I would s definitely say it 's finite . grad b: Yeah . grad c: And at least in compilers , that 's all that really matters , as long as your analysis is finite . grad a: How 's that ? {nonvocalsound} How it can be finite , again ? grad c: Nah , I can't think of a way it would be infinite . grad b: Well , you can come up with new constructions . grad c: Yeah . {comment} If the {disfmarker} if your {disfmarker} if your brain was totally non - deterministic , then perhaps there 's a way to get , uh , infin an infinite number of constructions that you 'd have to worry about . grad a: But , I mean , in the {nonvocalsound} practical sense , it 's impossible . grad c: Right . Cuz if we have a fixed number of neurons {disfmarker} ? grad a: Yeah . grad c: So the best - case scenario would be the number of constructions {disfmarker} or , the worst - case scenario is the number of constructions equals the number of neurons . grad a: Well , two to the power of the number of neurons . grad c: Right . But still finite . grad b: OK . grad c: No , wait . Not necessarily , is it ? We can end the {pause} meeting . I just {disfmarker} Can't you use different var different levels of activation ? across , uh {disfmarker} lots of different neurons , to specify different values ? grad b: Mm - hmm . grad a: Um , yeah , but there 's , like , a certain level of {disfmarker} grad c: There 's a bandwidth issue , grad a: Bandw - Yeah , so you can't do better than something . grad c: right ? Yeah . grad b: Turn off the mikes . Otherwise it gets really tough for the tr