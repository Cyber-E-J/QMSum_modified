phd a: It 's not very significant . professor b: Uh , channel one . Yes . grad d: Channel three . professor b: OK . phd f: Mm - hmm . grad d: Channel three . phd a: Ta grad d: Channel three . Alright . professor b: OK , did you solve speech recognition last week ? grad e: Almost . professor b: Alright ! Let 's do image processing . phd c: Yes , again . phd a: Great . phd c: We did it again , Morgan . professor b: Alright ! grad e: Doo - doop , doo - doo . phd a: What 's wrong with {disfmarker} ? professor b: OK . It 's April fifth . Actually , Hynek should be getting back in town shortly if he isn't already . phd c: Is he gonna come here ? professor b: Uh . Well , we 'll drag him here . I know where he is . phd c: So when you said " in town " , you mean {pause} Oregon . professor b: U u u u uh , I meant , you know , this end of the world , yeah , {vocalsound} is really what I meant , phd c: Oh . grad e: Doo , doo - doo . professor b: uh , cuz he 's been in Europe . grad e: Doo - doo . professor b: So . phd c: I have something just fairly brief to report on . professor b: Mmm . phd c: Um , I did some {pause} experim uh , uh , just a few more experiments before I had to , {vocalsound} uh , go away for the w well , that week . professor b: Great ! phd c: Was it last week or whenever ? Um , so what I was started playing with was the {disfmarker} th again , this is the HTK back - end . And , um , I was curious because the way that they train up the models , {vocalsound} they go through about four sort of rounds of {disfmarker} of training . And in the first round they do {disfmarker} uh , I think it 's three iterations , and for the last three rounds e e they do seven iterations of re - estimation in each of those three . And so , you know , that 's part of what takes so long to train the {disfmarker} the {disfmarker} the back - end for this . professor b: I 'm sorry , I didn't quite get that . There 's {disfmarker} there 's four and there 's seven and {disfmarker} I {disfmarker} I 'm sorry . phd c: Yeah . Uh , maybe I should write it on the board . So , {vocalsound} there 's four rounds of training . Um , I g I g I guess you could say iterations . The first one is three , then seven , seven , and seven . And what these numbers refer to is the number of times that the , uh , HMM re - estimation is run . It 's this program called H E professor b: But in HTK , what 's the difference between , uh , a {disfmarker} an inner loop and an outer loop in these iterations ? phd c: OK . So what happens is , um , at each one of these points , you increase the number of Gaussians in the model . professor b: Yeah . Oh , right ! This was the mix up stuff . phd c: Yeah . The mix up . professor b: That 's right . phd c: Right . professor b: I remember now . phd c: And so , in the final one here , you end up with , uh {disfmarker} for all of the {disfmarker} the digit words , you end up with , uh , three {pause} mixtures per state , professor b: Yeah . phd c: eh , in the final {pause} thing . So I had done some experiments where I was {disfmarker} I {disfmarker} I want to play with the number of mixtures . professor b: Mm - hmm . phd c: But , um , uh , I wanted to first test to see if we actually need to do {pause} this many iterations early on . grad e: Uh , one , two , professor b: Mm - hmm . phd c: And so , um , I {disfmarker} I ran a couple of experiments where I {vocalsound} reduced that to l to be three , two , two , {vocalsound} uh , five , I think , and I got almost the exact same results . professor b: Mm - hmm . phd c: And {disfmarker} but it runs much much faster . So , um , I {disfmarker} I think m {pause} it only took something like , uh , three or four hours to do the full training , professor b: As opposed to {disfmarker} ? phd f: Good . phd c: as opposed to wh what , sixteen hours or something like that ? I mean , it takes {disfmarker} you have to do an overnight basically , the way it is set up now . phd f: Yeah . It depends . phd a: Mm - hmm . professor b: Mm - hmm . phd c: So , uh , even we don't do anything else , doing something like this could allow us to turn experiments around a lot faster . professor b: And then when you have your final thing , do a full one , so it 's {disfmarker} phd c: And when you have your final thing , we go back to this . phd f: Yeah . phd c: So , um , and it 's a real simple change to make . I mean , it 's like one little text file you edit and change those numbers , and you don't do anything else . phd f: Oh , this is a {disfmarker} phd a: Mm - hmm . phd c: And then you just run . phd f: OK . phd c: So it 's a very simple change to make and it doesn't seem to hurt all that much . phd a: So you {disfmarker} you run with three , two , two , five ? That 's a phd c: So I {disfmarker} Uh , I {disfmarker} I have to look to see what the exact numbers were . phd a: Yeah . phd c: I {disfmarker} I thought was , like , three , two , two , five , phd a: Mm - hmm . phd c: but I I 'll {disfmarker} I 'll double check . It was {vocalsound} over a week ago that I did it , phd a: OK . Mm - hmm . phd c: so I can't remember exactly . grad e: Oh . phd c: But , uh {disfmarker} professor b: Mm - hmm . phd c: um , but it 's so much faster . I it makes a big difference . grad e: Hmm . phd c: So we could do a lot more experiments and throw a lot more stuff in there . phd f: Yeah . professor b: That 's great . phd c: Um . Oh , the other thing that I did was , um , {vocalsound} I compiled {pause} the HTK stuff for the Linux boxes . So we have this big thing that we got from IBM , which is a five - processor machine . Really fast , but it 's running Linux . So , you can now run your experiments on that machine and you can run five at a time and it runs , {vocalsound} uh , as fast as , you know , uh , five different machines . phd a: Mm - hmm . phd f: Mm - hmm . phd c: So , um , I 've forgotten now what the name of that machine is but I can {disfmarker} I can send email around about it . phd a: Yeah . phd c: And so we 've got it {disfmarker} now HTK 's compiled for both the Linux and for , um , the Sparcs . Um , you have to make {disfmarker} you have to make sure that in your dot CSHRC , {vocalsound} um , it detects whether you 're running on the Linux or a {disfmarker} a Sparc and points to the right executables . Uh , and you may not have had that in your dot CSHRC before , if you were always just running the Sparc . So , um , phd a: Mm - hmm . phd c: uh , I can {disfmarker} I can tell you exactly what you need to do to get all of that to work . But it 'll {disfmarker} it really increases what we can run on . grad e: Hmm . Cool . phd c: So , {vocalsound} together with the fact that we 've got these {pause} faster Linux boxes and that it takes less time to do {pause} these , um , we should be able to crank through a lot more experiments . phd a: Mm - hmm . phd c: So . grad e: Hmm . phd c: So after I did that , then what I wanted to do {comment} was try {pause} increasing the number of mixtures , just to see , um {disfmarker} see how {disfmarker} how that affects performance . phd a: Yeah . phd c: So . professor b: Yeah . In fact , you could do something like {pause} keep exactly the same procedure and then add a fifth thing onto it phd c: Mm - hmm . professor b: that had more . phd c: Exactly . professor b: Yeah . phd c: Right . Right . grad e: So at {disfmarker} at the middle o where the arrows are showing , that 's {disfmarker} you 're adding one more mixture per state , phd c: Uh - huh . Uh , grad e: or {disfmarker} ? phd c: let 's see , uh . It goes from this {disfmarker} uh , try to go it backwards {disfmarker} this {disfmarker} at this point it 's two mixtures {pause} per state . So this just adds one . Except that , uh , actually for the silence model , it 's six mixtures per state . professor b: Mm - hmm . phd c: Uh , so it goes to two . grad e: OK . phd c: Um . And I think what happens here is {disfmarker} professor b: Might be between , uh , shared , uh {disfmarker} shared variances or something , phd c: Yeah . I think that 's what it is . professor b: or {disfmarker} phd c: Uh , yeah . It 's , uh {disfmarker} Shoot . I {disfmarker} I {disfmarker} I can't remember now what happens at that first one . Uh , I have to look it up and see . grad e: Oh , OK . phd c: Um , there {disfmarker} because they start off with , uh , an initial model which is just this global model , and then they split it to the individuals . And so , {vocalsound} it may be that that 's what 's happening here . I {disfmarker} I {disfmarker} {vocalsound} I have to look it up and see . I {disfmarker} I don't exactly remember . grad e: OK . professor b: OK . phd c: So . That 's it . professor b: Alright . So what else ? phd a: Um . Yeah . There was a conference call this Tuesday . Um . I don't know yet the {disfmarker} {vocalsound} what happened {vocalsound} Tuesday , but {vocalsound} the points that they were supposed to discuss is still , {vocalsound} uh , things like {vocalsound} the weights , uh {disfmarker} professor b: Oh , this is a conference call for , uh , uh , Aurora participant sort of thing . grad e: For {disfmarker} phd a: Yeah . Yeah . professor b: I see . phd a: Mmm . professor b: Do you know who was {disfmarker} who was {disfmarker} since we weren't in on it , uh , do you know who was in from OGI ? Was {disfmarker} {vocalsound} was {disfmarker} was Hynek involved or was it Sunil phd a: I have no idea . professor b: or {disfmarker} ? phd a: Mmm , I just {disfmarker} professor b: Oh , you don't know . OK . phd a: Yeah . professor b: Alright . phd a: Um , yeah . So the points were the {disfmarker} the weights {disfmarker} how to weight the different error rates {vocalsound} that are obtained from different language and {disfmarker} and conditions . Um , it 's not clear that they will keep the same kind of weighting . Right now it 's a weighting on {disfmarker} on improvement . professor b: Mm - hmm . phd a: Some people are arguing that it would be better to have weights on uh {disfmarker} well , to {disfmarker} to combine error rates {pause} before computing improvement . Uh , and the fact is that for {disfmarker} right now for {pause} the English , they have weights {disfmarker} they {disfmarker} they combine error rates , but for the other languages they combine improvement . So it 's not very consistent . Um {disfmarker} professor b: Mm - hmm . phd a: Yeah . The , um {disfmarker} Yeah . And so {disfmarker} Well , {vocalsound} this is a point . And right now actually there is a thing also , {vocalsound} uh , that happens with the current weight is that a very non - significant improvement {pause} on the well - matched case result in {pause} huge differences in {disfmarker} {vocalsound} in the final number . professor b: Mm - hmm . phd a: And so , perhaps they will change the weights to {disfmarker} phd c: Hmm . phd a: Yeah . phd c: How should that be done ? I mean , it {disfmarker} it seems like there 's a simple way {disfmarker} phd a: Mm - hmm . phd c: Uh , this seems like an obvious mistake or something . professor b: Well , I mean , the fact that it 's inconsistent is an obvious mistake . phd c: Th - they 're {disfmarker} professor b: But the {disfmarker} but , um , the other thing {disfmarker} phd a: In professor b: I don't know I haven't thought it through , but one {disfmarker} one would think that {vocalsound} each {disfmarker} It {disfmarker} it 's like if you say what 's the {disfmarker} what 's the best way to do an average , an arithmetic average or a geometric average ? phd c: Mm - hmm . professor b: It depends what you wanna show . phd a: Mm - hmm . professor b: Each {disfmarker} each one is gonna have a different characteristic . phd a: Yeah . professor b: So {disfmarker} phd c: Well , it seems like they should do , like , the percentage improvement or something , rather than the {pause} absolute improvement . phd a: Tha - that 's what they do . professor b: Well , they are doing that . phd a: Yeah . professor b: No , that is relative . But the question is , do you average the relative improvements {pause} or do you average the error rates and take the relative improvement maybe of that ? phd a: Yeah . Yeah . professor b: And the thing is it 's not just a pure average because there are these weightings . phd c: Oh . professor b: It 's a weighted average . Um . phd a: Yeah . And so when you average the {disfmarker} the relative improvement it tends to {disfmarker} {vocalsound} to give a lot of {disfmarker} of , um , {vocalsound} importance to the well - matched case because {pause} the baseline is already very good and , um , i it 's {disfmarker} phd c: Why don't they not look at improvements but just look at your av your scores ? You know , figure out how to combine the scores phd a: Mm - hmm . phd c: with a weight or whatever , and then give you a score {disfmarker} here 's your score . And then they can do the same thing for the baseline system {disfmarker} and here 's its score . And then you can look at {disfmarker} phd a: Mm - hmm . professor b: Well , that 's what he 's seeing as one of the things they could do . phd a: Yeah . professor b: It 's just when you {disfmarker} when you get all done , I think that they pro I m I {disfmarker} I wasn't there but I think they started off this process with the notion that {vocalsound} you should be {pause} significantly better than the previous standard . phd c: Mm - hmm . professor b: And , um , so they said " how much is significantly better ? what do you {disfmarker} ? " And {disfmarker} and so they said " well , {vocalsound} you know , you should have half the errors , " or something , " that you had before " . phd a: Mm - hmm . Hmm . phd c: Mm - hmm . phd a: Yeah . professor b: So it 's , uh , But it does seem like phd c: Hmm . professor b: i i it does seem like it 's more logical to combine them first and then do the {disfmarker} phd a: Combine error rates and then {disfmarker} professor b: Yeah . phd a: Yeah . Well {disfmarker} professor b: Yeah . phd a: But there is this {disfmarker} this {disfmarker} is this still this problem of weights . When {disfmarker} when you combine error rate it tends to {pause} give more importance to the difficult cases , and some people think that {disfmarker} professor b: Oh , yeah ? phd a: well , they have different , {vocalsound} um , opinions about this . Some people think that {vocalsound} it 's more important to look at {disfmarker} {vocalsound} to have ten percent imp relative improvement on {pause} well - matched case than to have fifty percent on the m mismatched , and other people think that it 's more important to improve a lot on the mismatch and {disfmarker} So , bu phd c: It sounds like they don't really have a good idea about what the final application is gonna be . phd a: l de fff ! Mmm . professor b: Well , you know , the {disfmarker} the thing is {vocalsound} that if you look at the numbers on the {disfmarker} on the more difficult cases , {vocalsound} um , if you really believe that was gonna be the predominant use , {vocalsound} none of this would be good enough . phd a: Yeah . Mmm . Yeah . professor b: Nothing anybody 's {disfmarker} phd c: Mm - hmm . professor b: whereas {vocalsound} you sort of with some reasonable error recovery could imagine in the better cases that these {disfmarker} these systems working . So , um , I think the hope would be that it would {disfmarker} {vocalsound} uh , it would work well {pause} for the good cases and , uh , it would have reasonable {disfmarker} reas {vocalsound} soft degradation as you got to worse and worse conditions . Um . phd c: Yeah . I {disfmarker} I guess what I 'm {disfmarker} I mean , I {disfmarker} I was thinking about it in terms of , if I were building the final product and I was gonna test to see which front - end I 'd {disfmarker} {vocalsound} I wanted to use , I would {vocalsound} try to {pause} weight things depending on the exact environment that I was gonna be using the system in . professor b: But {disfmarker} but {disfmarker} No . phd c: If I {disfmarker} professor b: Well , no {disfmarker} well , no . I mean , {vocalsound} it isn't the operating theater . I mean , they don they {disfmarker} they don't {disfmarker} they don't really {pause} know , I think . phd c: Yeah . professor b: I mean , I th phd c: So if {disfmarker} if they don't know , doesn't that suggest the way for them to go ? Uh , you assume everything 's equal . I mean , y y I mean , you {disfmarker} professor b: Well , I mean , I {disfmarker} I think one thing to do is to just not rely on a single number {disfmarker} to maybe have two or three numbers , phd c: Yeah . professor b: you know , phd c: Right . professor b: and {disfmarker} and {disfmarker} and say {vocalsound} here 's how much you , uh {disfmarker} you improve {vocalsound} the , uh {disfmarker} the {disfmarker} the relatively clean case and here 's {disfmarker} or {disfmarker} or well - matched case , and here 's how {disfmarker} here 's how much you , phd c: Mm - hmm . professor b: uh {disfmarker} phd c: So not {disfmarker} professor b: So . phd c: So not try to combine them . professor b: Yeah . Uh , actually it 's true . phd c: Yeah . professor b: Uh , I had forgotten this , uh , but , uh , well - matched is not actually clean . What it is is just that , u uh , the training and testing are similar . phd c: The training and testing . phd a: Mmm . professor b: So , I guess what you would do in practice is you 'd try to get as many , {vocalsound} uh , examples of similar sort of stuff as you could , and then , phd c: Yeah . professor b: uh {disfmarker} So the argument for that being the {disfmarker} the {disfmarker} the more important thing , {vocalsound} is that you 're gonna try and do that , {vocalsound} but you wanna see how badly it deviates from that when {disfmarker} when {disfmarker} when the , uh {disfmarker} it 's a little different . phd c: So {disfmarker} professor b: Um , phd c: so you should weight those other conditions v very {disfmarker} you know , really small . professor b: But {disfmarker} No . That 's a {disfmarker} that 's a {disfmarker} that 's an arg phd c: I mean , that 's more of an information kind of thing . professor b: that 's an ar Well , that 's an argument for it , but let me give you the opposite argument . The opposite argument is you 're never really gonna have a good sample of all these different things . phd c: Uh - huh . professor b: I mean , are you gonna have w uh , uh , examples with the windows open , half open , full open ? Going seventy , sixty , fifty , forty miles an hour ? On what kind of roads ? phd c: Mm - hmm . professor b: With what passing you ? With {disfmarker} uh , I mean , phd c: Mm - hmm . professor b: I {disfmarker} I {disfmarker} I think that you could make the opposite argument that the well - matched case is a fantasy . phd c: Mm - hmm . professor b: You know , so , grad e: Uh - huh . professor b: I think the thing is is that if you look at the well - matched case versus the po you know , the {disfmarker} the medium and the {disfmarker} and the fo and then the mismatched case , {vocalsound} um , we 're seeing really , really big differences in performance . Right ? And {disfmarker} and y you wouldn't like that to be the case . You wouldn't like that as soon as you step outside {disfmarker} You know , a lot of the {disfmarker} the cases it 's {disfmarker} is {disfmarker} phd c: Well , that 'll teach them to roll their window up . professor b: I mean , in these cases , if you go from the {disfmarker} the , uh {disfmarker} I mean , I don't remember the numbers right off , but if you {disfmarker} if you go from the well - matched case to the medium , {vocalsound} it 's not an enormous difference in the {disfmarker} in the {disfmarker} the training - testing situation , and {disfmarker} and {disfmarker} and it 's a really big {vocalsound} performance drop . phd c: Mm - hmm . professor b: You know , so , um {disfmarker} Yeah , I mean the reference one , for instance {disfmarker} this is back old on , uh {disfmarker} on Italian {disfmarker} uh , was like {pause} six percent error for the well - matched and eighteen for the medium - matched and sixty for the {disfmarker} {vocalsound} for highly - mismatched . Uh , and , you know , with these other systems we {disfmarker} we {vocalsound} helped it out quite a bit , but still there 's {disfmarker} there 's something like a factor of two or something between well - matched and medium - matched . And {vocalsound} so I think that {vocalsound} if what you 're {disfmarker} {vocalsound} if the goal of this is to come up with robust features , it does mean {disfmarker} So you could argue , in fact , that the well - matched is something you shouldn't be looking at at all , that {disfmarker} that the goal is to come up with features {vocalsound} that will still give you reasonable performance , you know , with again gentle degregra degradation , um , even though the {disfmarker} the testing condition is not the same as the training . phd c: Hmm . professor b: So , you know , I {disfmarker} I could argue strongly that something like the medium mismatch , which is you know not compl pathological but {disfmarker} I mean , what was the {disfmarker} the medium - mismatch condition again ? phd a: Um , {vocalsound} it 's {disfmarker} Yeah . Medium mismatch is everything with the far {pause} microphone , but trained on , like , low noisy condition , like low speed and {disfmarker} or {pause} stopped car and tested on {pause} high - speed conditions , I think , like on a highway and {disfmarker} professor b: Right . phd a: So {disfmarker} professor b: So it 's still the same {disfmarker} same microphone in both cases , phd a: Same microphone but {disfmarker} Yeah . professor b: but , uh , it 's {disfmarker} there 's a mismatch between the car conditions . And that 's {disfmarker} uh , you could argue that 's a pretty realistic situation phd c: Yeah . phd a: Mm - hmm . professor b: and , uh , I 'd almost argue for weighting that highest . But the way they have it now , {vocalsound} it 's {disfmarker} I guess it 's {disfmarker} it 's {disfmarker} They {disfmarker} they compute the relative improvement first and then average that with a weighting ? phd a: Yeah . professor b: And so then the {disfmarker} that {disfmarker} that makes the highly - matched the really big thing . phd a: Mm - hmm . professor b: Um , so , u i since they have these three categories , it seems like the reasonable thing to do {vocalsound} is to go across the languages {pause} and to come up with an improvement for each of those . phd a: Mm - hmm . professor b: Just say " OK , in the {disfmarker} in the highly - matched case this is what happens , in the {disfmarker} {vocalsound} m the , uh {disfmarker} this other m medium if this happens , in the highly - mismatched {pause} that happens " . phd a: Mm - hmm . professor b: And , uh , you should see , uh , a gentle degradation {pause} through that . phd a: Mmm . professor b: Um . But {disfmarker} I don't know . phd a: Yeah . professor b: I think that {disfmarker} that {disfmarker} I {disfmarker} I {disfmarker} I gather that in these meetings it 's {disfmarker} it 's really tricky to make anything {vocalsound} ac {vocalsound} make any {comment} policy change because {vocalsound} {vocalsound} everybody has {disfmarker} has , uh , their own opinion phd a: Mm - hmm . professor b: and {disfmarker} I don't know . phd a: Yeah . professor b: Yeah . phd a: Uh , so {disfmarker} Yeah . Yeah , but there is probably a {disfmarker} a big change that will {vocalsound} be made is that the {disfmarker} the baseline {disfmarker} th they want to have a new baseline , perhaps , which is , um , MFCC but with {vocalsound} a voice activity detector . And apparently , {vocalsound} uh , some people are pushing to still keep this fifty percent number . So they want {vocalsound} to have at least fifty percent improvement on the baseline , but w which would be a much better baseline . professor b: Mm - hmm . Mm - hmm . phd a: And if we look at the result that Sunil sent , {vocalsound} just putting the VAD in the baseline improved , like , more than twenty percent , professor b: Mm - hmm . phd a: which would mean then {disfmarker} then {disfmarker} mean that fifty percent on this new baseline is like , well , more than sixty percent improvement on {disfmarker} on {disfmarker} o e e uh {disfmarker} professor b: So nobody would {pause} be there , probably . Right ? phd a: Right now , nobody would be there , but {disfmarker} Yeah . professor b: Good . Work to do . phd a: Uh - huh . professor b: So whose VAD is {disfmarker} Is {disfmarker} is this a {disfmarker} ? phd a: Uh , they didn't decide yet . I guess i this was one point of the conference call also , but {disfmarker} mmm , so I don't know . Um , but {disfmarker} Yeah . grad e: Oh . professor b: Oh , I {disfmarker} I think th that would be {vocalsound} good . I mean , it 's not that the design of the VAD isn't important , but it 's just that it {disfmarker} it {disfmarker} it does seem to be i uh , a lot of {pause} work to do a good job on {disfmarker} on that and as well as being a lot of work to do a good job on the feature {vocalsound} design , phd a: Yeah . professor b: so phd a: Yeah . professor b: if we can {pause} cut down on that maybe we can make some progress . phd a: M Yeah . grad e: Hmm . phd a: But I guess perhaps {disfmarker} I don't know w {vocalsound} Yeah . Uh , yeah . Per - e s s someone told that perhaps it 's not fair to do that because the , um {disfmarker} to make a good VAD {pause} you don't have enough to {disfmarker} with the {disfmarker} the features that are {disfmarker} the baseline features . So {disfmarker} mmm , you need more features . So you really need to put more {disfmarker} more in the {disfmarker} in {disfmarker} in the front - end . professor b: Yeah . phd a: So i professor b: Um , phd a: S professor b: sure . But i bu phd c: Wait a minute . I {disfmarker} I 'm confused . phd a: Yeah . phd c: Wha - what do you mean ? phd a: Yeah , if i professor b: So y so you m s Yeah , but {disfmarker} Well , let 's say for ins see , MFCC for instance doesn't have anything in it , uh , related to the pitch . So just {disfmarker} just for example . So suppose you 've {disfmarker} that {vocalsound} what you really wanna do is put a good pitch detector on there and if it gets an unambiguous {disfmarker} phd c: Oh , oh . I see . phd a: Mm - hmm . professor b: if it gets an unambiguous result then you 're definitely in a {disfmarker} in a {disfmarker} in a voice in a , uh , s region with speech . Uh . phd c: So there 's this assumption that the v the voice activity detector can only use the MFCC ? phd a: That 's not clear , but this {disfmarker} {vocalsound} e professor b: Well , for the baseline . phd c: Yeah . professor b: So {disfmarker} so if you use other features then y But it 's just a question of what is your baseline . Right ? What is it that you 're supposed to do better than ? phd c: I g Yeah . professor b: And so having the baseline be the MFCC 's {pause} means that people could {pause} choose to pour their ener their effort into trying to do a really good VAD phd c: I don't s But they seem like two {pause} separate issues . professor b: or tryi They 're sort of separate . phd c: Right ? I mean {disfmarker} professor b: Unfortunately there 's coupling between them , which is part of what I think Stephane is getting to , is that {vocalsound} you can choose your features in such a way as to improve the VAD . phd a: Yeah . professor b: And you also can choose your features in such a way as to prove {disfmarker} improve recognition . They may not be the same thing . phd c: But it seems like you should do both . professor b: You should do both phd c: Right ? professor b: and {disfmarker} and I {disfmarker} I think that this still makes {disfmarker} I still think this makes sense as a baseline . It 's just saying , as a baseline , we know {disfmarker} phd a: Mmm . professor b: you know , we had the MFCC 's before , lots of people have done voice activity detectors , phd a: Mm - hmm . professor b: you might as well pick some voice activity detector and make that the baseline , just like you picked some version of HTK and made that the baseline . phd a: Yeah . Right . professor b: And then {pause} let 's try and make everything better . Um , and if one of the ways you make it better is by having your features {pause} be better features for the VAD then that 's {disfmarker} so be it . phd a: Mm - hmm . professor b: But , uh , uh , uh , at least you have a starting point that 's {disfmarker} um , cuz i i some of {disfmarker} the some of the people didn't have a VAD at all , I guess . Right ? And {disfmarker} and phd a: Yeah . professor b: then they {disfmarker} they looked pretty bad and {disfmarker} and in fact what they were doing wasn't so bad at all . phd a: Mm - hmm . Mm - hmm . professor b: But , um . phd c: Yeah . It seems like you should try to make your baseline as good as possible . And if it turns out that {pause} you can't improve on that , well , I mean , then , you know , nobody wins and you just use MFCC . Right ? professor b: Yeah . I mean , it seems like , uh , it should include sort of the current state of the art {vocalsound} that you want {disfmarker} are trying to improve , and MFCC 's , you know , or PLP or something {disfmarker} it seems like {vocalsound} reasonable baseline for the features , and anybody doing this task , {vocalsound} uh , is gonna have some sort of voice activity detection at some level , in some way . They might use the whole recognizer to do it {vocalsound} but {disfmarker} rather than {vocalsound} a separate thing , but {disfmarker} {vocalsound} but they 'll have it on some level . So , um . phd c: It seems like whatever they choose they shouldn't , {vocalsound} you know , purposefully brain - damage a part of the system to {pause} make a worse baseline , or {disfmarker} professor b: Well , I think people just had phd c: You know ? professor b: it wasn't that they purposely brain - damaged it . I think people hadn't really thought through {vocalsound} about the , uh {disfmarker} the VAD issue . phd c: Mmm . phd a: Mm - hmm . professor b: And {disfmarker} and then when the {disfmarker} the {disfmarker} the proposals actually came in and half of them had V A Ds and half of them didn't , and the half that did did well and the {vocalsound} half that didn't did poorly . phd c: Mm - hmm . professor b: So it 's {disfmarker} phd a: Mm - hmm . Um . professor b: Uh . phd a: Yeah . So we 'll see what happen with this . And {disfmarker} Yeah . So what happened since , um , {vocalsound} last week is {disfmarker} well , from OGI , these experiments on {pause} putting VAD on the baseline . And these experiments also are using , uh , some kind of noise compensation , so spectral subtraction , and putting on - line normalization , um , just after this . So I think spectral subtraction , LDA filtering , and on - line normalization , so which is similar to {vocalsound} the pro proposal - one , but with {pause} spectral subtraction in addition , and it seems that on - line normalization doesn't help further when you have spectral subtraction . phd c: Is this related to the issue that you brought up a couple of meetings ago with the {disfmarker} the {vocalsound} musical tones phd a: I {disfmarker} phd c: and {disfmarker} ? phd a: I have no idea , because the issue I brought up was with a very simple spectral subtraction approach , phd c: Mmm . phd a: and the one that {vocalsound} they use at OGI is one from {disfmarker} from {vocalsound} the proposed {disfmarker} the {disfmarker} the {disfmarker} the Aurora prop uh , proposals , which might be much better . So , yeah . I asked {vocalsound} Sunil for more information about that , but , uh , I don't know yet . Um . And what 's happened here is that we {disfmarker} so we have this kind of new , um , reference system which {vocalsound} use a nice {disfmarker} a {disfmarker} a clean downsampling - upsampling , which use a new filter {vocalsound} that 's much shorter and which also cuts the frequency below sixty - four hertz , professor b: Right . phd a: which was not done on our first proposal . professor b: When you say " we have that " , does Sunil have it now , too , phd a: I No . professor b: or {disfmarker} ? phd a: No . professor b: OK . phd a: Because we 're still testing . So we have the result for , {vocalsound} uh , just the features professor b: OK . phd a: and we are currently testing with putting the neural network in the KLT . Um , it seems to improve on the well - matched case , um , {vocalsound} but it 's a little bit worse on the mismatch and highly - mismatched {disfmarker} I mean when we put the neural network . And with the current weighting I think it 's sh it will be better because the well - matched case is better . Mmm . professor b: But how much worse {disfmarker} since the weighting might change {disfmarker} how {disfmarker} how much worse is it on the other conditions , when you say it 's a little worse ? phd a: It 's like , uh , fff , fff {comment} {vocalsound} {pause} um , {comment} {vocalsound} {vocalsound} {pause} ten percent relative . Yeah . professor b: OK . Um . phd a: Mm - hmm . professor b: But it has the , uh {disfmarker} the latencies are much shorter . That 's {disfmarker} phd a: Uh - y w when I say it 's worse , it 's not {disfmarker} it 's when I {disfmarker} I {disfmarker} uh , compare proposal - two to proposal - one , so , r uh , y putting neural network {vocalsound} compared to n not having any neural network . I mean , this new system is {disfmarker} is {disfmarker} is better , professor b: Uh - huh . phd a: because it has {vocalsound} um , this sixty - four hertz cut - off , uh , clean {vocalsound} downsampling , and , um {disfmarker} what else ? Uh , yeah , a good VAD . We put the good VAD . So . Yeah , I don't know . I {disfmarker} I {disfmarker} j uh , uh {disfmarker} pr professor b: But the latencies {disfmarker} but you 've got the latency shorter now . phd a: Latency is short {disfmarker} is {disfmarker} Yeah . professor b: Yeah . phd f: Isn't it phd a: And so professor b: So it 's better than the system that we had before . phd a: Yeah . Mainly because {pause} {vocalsound} of {pause} the sixty - four hertz and the good VAD . professor b: OK . phd a: And then I took this system and , {vocalsound} mmm , w uh , I p we put the old filters also . So we have this good system , with good VAD , with the short filter and with the long filter , and , um , with the short filter it 's not worse . So {disfmarker} well , is it {disfmarker} professor b: OK . phd a: it 's in {disfmarker} professor b: So that 's {disfmarker} that 's all fine . phd a: Yes . Uh {disfmarker} professor b: But what you 're saying is that when you do these {disfmarker} So let me try to understand . When {disfmarker} when you do these same improvements {vocalsound} to proposal - one , phd a: Mm - hmm . professor b: that , uh , on the {disfmarker} i things are somewhat better , uh , in proposal - two for the well - matched case and somewhat worse for the other two cases . phd a: Yeah . professor b: So does , uh {disfmarker} when you say , uh {disfmarker} So {disfmarker} The th now that these other things are in there , is it the case maybe that the additions of proposal - two over proposal - one are {pause} less im important ? phd a: Yeah . Probably , yeah . professor b: I get it . phd a: Um {disfmarker} So , yeah . Uh . Yeah , but it 's a good thing anyway to have {vocalsound} shorter delay . Then we tried , um , {vocalsound} to do something like proposal - two but having , um , e using also MSG features . So there is this KLT part , which use just the standard features , professor b: Mm - hmm . Right . phd a: and then two neura two neural networks . professor b: Mm - hmm . phd a: Mmm , and it doesn't seem to help . Um , however , we just have {vocalsound} one result , which is the Italian mismatch , so . Uh . We have to wait for that to fill the whole table , but {disfmarker} professor b: OK . There was a {vocalsound} start of some effort on something related to voicing or something . Is that {disfmarker} ? phd a: Yeah . Um , {vocalsound} yeah . So basically we try to , {vocalsound} {vocalsound} uh , find {vocalsound} good features that could be used for voicing detection , uh , but it 's still , uh {disfmarker} on the , um {disfmarker} t phd f: Oh , well , I have the picture . phd a: we {disfmarker} w basically we are still playing with Matlab to {disfmarker} {vocalsound} to look at {disfmarker} at what happened , phd c: What sorts of {disfmarker} phd f: Yeah . phd a: and {disfmarker} phd c: what sorts of features are you looking at ? phd f: We have some {disfmarker} phd a: So we would be looking at , um , the {pause} variance of the spectrum of the excitation , phd f: uh , um , this , this , and this . phd a: something like this , which is {disfmarker} should be high for voiced sounds . Uh , we {disfmarker} phd c: Wait a minute . I {disfmarker} what does that mean ? The variance of the spectrum of excitation . phd a: Yeah . So the {disfmarker} So basically the spectrum of the excitation {vocalsound} for a purely periodic sig signal shou sh professor b: OK . Yeah , w what yo what you 're calling the excitation , as I recall , is you 're subtracting the {disfmarker} the , um {disfmarker} the mel {disfmarker} mel {disfmarker} {vocalsound} mel filter , uh , spectrum from the FFT spectrum . phd a: e That 's right . Yeah . So {disfmarker} professor b: Right . phd a: Yeah . phd f: Mm - hmm . phd a: So we have the mel f filter bank , we have the FFT , so we {pause} just {disfmarker} professor b: So it 's {disfmarker} it 's not really an excitation , phd a: No . professor b: but it 's something that hopefully tells you something about the excitation . phd a: Yeah , that 's right . professor b: Yeah , yeah . phd a: Um {disfmarker} Yeah . phd f: We have here some histogram , phd a: E yeah , phd f: but they have a lot of overlap . phd a: but it 's {disfmarker} it 's still {disfmarker} Yeah . So , well , for unvoiced portion we have something tha {vocalsound} that has a mean around O point three , and for voiced portion the mean is O point fifty - nine . But the variance seem quite {vocalsound} high . phd c: How do you know {disfmarker} ? phd a: So {disfmarker} Mmm . phd c: How did you get your {pause} voiced and unvoiced truth data ? phd a: We used , uh , TIMIT and we used canonical mappings between the phones phd f: Yeah . We , uh , use {pause} TIMIT on this , phd a: and phd f: for {disfmarker} phd a: th Yeah . phd f: But if we look at it in one sentence , it {disfmarker} apparently it 's good , I think . phd a: Yeah , but {disfmarker} Yeah . Uh , so it 's noisy TIMIT . That 's right . Yeah . grad e: It 's noisy TIMIT . phd f: Yeah . phd a: It seems quite robust to noise , so when we take {disfmarker} we draw its parameters across time for a clean sentence and then nois the same noisy sentence , it 's very close . professor b: Mm - hmm . phd a: Yeah . So there are {disfmarker} there is this . There could be also the , um {disfmarker} {vocalsound} something like the maximum of the auto - correlation function or {disfmarker} which {disfmarker} phd c: Is this a {disfmarker} a s a trained system ? Or is it a system where you just pick some thresholds ? Ho - how does it work ? phd a: Right now we just are trying to find some features . And , phd c: Mm - hmm . phd a: uh {disfmarker} Yeah . Hopefully , I think what we want to have is to put these features in s some kind of , um {disfmarker} well , to {disfmarker} to obtain a statistical model on these features and to {disfmarker} or just to use a neural network and hopefully these features w would help {disfmarker} phd c: Because it seems like what you said about the mean of the {disfmarker} the voiced and the unvoiced {disfmarker} {comment} {vocalsound} that seemed pretty encouraging . phd a: Mm - hmm . professor b: Well , yeah , except the variance was big . phd c: Right ? phd a: Yeah . Except the variance is quite high . professor b: Right ? phd c: Well , y phd a: Yeah . phd c: Well , y I {disfmarker} I don't know that I would trust that so much because you 're doing these canonical mappings from TIMIT labellings . phd a: Uh - huh . phd c: Right ? So , really that 's sort of a cartoon picture about what 's voiced and unvoiced . So that could be giving you a lot of variance . phd a: Yeah . phd c: I mean , i it {disfmarker} it may be that {disfmarker} that you 're finding something good and that the variance is sort of artificial because of how you 're getting your truth . phd a: Mm - hmm . professor b: Yeah . But another way of looking at it {vocalsound} might be that {disfmarker} I mean , what w we we are coming up with feature sets after all . So another way of looking at it is that {vocalsound} um , the mel cepstru mel {pause} spectrum , mel cepstrum , {vocalsound} any of these variants , um , give you the smooth spectrum . It 's the spectral envelope . By going back to the FFT , {vocalsound} you 're getting something that is {pause} more like the raw data . So the question is , what characterization {disfmarker} and you 're playing around with this {disfmarker} another way of looking at it is what characterization {vocalsound} of the difference between {pause} the raw data {pause} and this smooth version {pause} is something that you 're missing that could help ? So , I mean , looking at different statistical measures of that difference , coming up with some things and just trying them out and seeing if you add them onto the feature vector does that make things better or worse in noise , where you 're really just i i the way I 'm looking at it is not so much you 're trying to f find the best {disfmarker} the world 's best voiced - unvoiced , uh , uh , classifier , phd c: Mm - hmm . phd a: Mmm . professor b: but it 's more that , {vocalsound} you know , uh , uh , try some different statistical characterizations of that difference back to the raw data phd c: Right . professor b: and {disfmarker} and m maybe there 's something there that {pause} the system can use . phd c: Right . phd a: Yeah . Yeah , but ther more obvious is that {disfmarker} Yeah . The {disfmarker} the more obvious is that {disfmarker} that {disfmarker} well , using the {disfmarker} th the FFT , um , {vocalsound} you just {disfmarker} it gives you just information about if it 's voiced or not voiced , ma mainly , I mean . But {disfmarker} So , professor b: Yeah . phd a: this is why we {disfmarker} we started to look {pause} by having sort of voiced phonemes professor b: Well , that 's the rea w w what I 'm arguing is that 's Yeah . I mean , uh , what I 'm arguing is that that {disfmarker} that 's givi you {disfmarker} gives you your intuition . phd a: and {disfmarker} Mm - hmm . professor b: But in {disfmarker} in reality , it 's {disfmarker} you know , there 's all of this {disfmarker} this overlap and so forth , grad e: Oh , sorry . professor b: and {disfmarker} But what I 'm saying is that may be OK , because what you 're really getting is not actually voiced versus unvoiced , both for the fac the reason of the overlap and {disfmarker} and then , uh , th you know , structural reasons , uh , uh , like the one that Chuck said , that {disfmarker} that in fact , well , the data itself is {disfmarker} {vocalsound} that you 're working with is not perfect . phd a: Yeah . Mm - hmm . professor b: So , what I 'm saying is maybe that 's not a killer because you 're just getting some characterization , one that 's driven by your intuition about voiced - unvoiced certainly , phd a: Mm - hmm . professor b: but it 's just some characterization {vocalsound} of something back in the {disfmarker} in the {disfmarker} in the almost raw data , rather than the smooth version . phd a: Mm - hmm . professor b: And your intuition is driving you towards particular kinds of , {vocalsound} uh , statistical characterizations of , um , what 's missing from the spectral envelope . phd a: Mm - hmm . professor b: Um , obviously you have something about the excitation , um , and what is it about the excitation , and , you know {disfmarker} and you 're not getting the excitation anyway , you know . So {disfmarker} so I {disfmarker} I would almost take a {disfmarker} uh , especially if {disfmarker} if these trainings and so forth are faster , I would almost just take a {vocalsound} uh , a scattershot at a few different {vocalsound} ways of look of characterizing that difference and , uh , you could have one of them but {disfmarker} and {disfmarker} and see , you know , which of them helps . phd a: Mm - hmm . OK . phd c: So i is the idea that you 're going to take {pause} whatever features you develop and {disfmarker} and just add them onto the future vector ? Or , what 's the use of the {disfmarker} the voiced - unvoiced detector ? phd a: Uh , I guess we don't know exactly yet . But , {vocalsound} um {disfmarker} Yeah . Th phd c: It 's not part of a VAD system that you 're doing ? phd f: No . phd a: Uh , no . No . phd c: Oh , OK . phd a: No , the idea was , I guess , to {disfmarker} to use them as {disfmarker} as features . phd c: Features . I see . phd a: Uh {disfmarker} Yeah , it could be , uh {disfmarker} it could be {vocalsound} a neural network that does voiced and unvoiced detection , phd c: Mm - hmm . phd a: but it could be in the {disfmarker} also the big neural network that does phoneme classification . phd c: Mm - hmm . phd a: Mmm . Yeah . professor b: But each one of the mixture components {disfmarker} I mean , you have , uh , uh , variance only , so it 's kind of like you 're just multiplying together these , um , probabilities from the individual features {pause} within each mixture . So it 's {disfmarker} so , uh , it seems l you know {disfmarker} phd c: I think it 's a neat thing . Uh , it seems like a good idea . professor b: Yeah . Um . Yeah . I mean , {vocalsound} I know that , um , people doing some robustness things a ways back were {disfmarker} were just doing {disfmarker} just being gross and just throwing in the FFT and actually it wasn't {disfmarker} wasn't {disfmarker} wasn't so bad . Uh , so it would s and {disfmarker} and you know that i it 's gotta hurt you a little bit to not have a {disfmarker} {vocalsound} a spectral , uh {disfmarker} a s a smooth spectral envelope , so there must be something else that you get {pause} in return for that {disfmarker} phd a: Mm - hmm . professor b: that , uh {disfmarker} uh {disfmarker} So . phd c: So how does {disfmarker} uh , maybe I 'm going in too much detail , but {vocalsound} how exactly do you make the difference between the FFT and the smoothed {pause} spectral envelope ? Wha - wh i i uh , how is that , uh {disfmarker} ? phd a: Um , we just {disfmarker} How did we do it up again ? phd f: Uh , we distend the {disfmarker} we have the twenty - three coefficient af after the mel f {vocalsound} filter , phd a: Mm - hmm . phd f: and we extend these coefficient between the {disfmarker} all the frequency range . phd c: Mm - hmm . phd f: And i the interpolation i between the point {vocalsound} is {disfmarker} give for the triang triangular filter , the value of the triangular filter and of this way we obtained this mode this model speech . phd a: S professor b: So you essentially take the values that {disfmarker} th that you get from the triangular filter and extend them to sor sort of like a rectangle , that 's at that m value . phd f: Yeah . phd a: Yeah . I think we have linear interpolation . phd f: Mm - hmm . phd a: So we have {disfmarker} we have one point for {disfmarker} one energy for each filter bank , phd f: mmm Yeah , it 's linear . phd c: Mmm . professor b: Oh . phd a: which is {pause} the energy {pause} that 's centered on {disfmarker} on {disfmarker} on the triangle {disfmarker} phd f: Yeah . At the n at the center of the filter {disfmarker} phd c: So you {disfmarker} you end up with a vector that 's the same length as the FFT {pause} vector ? phd a: Yeah . That 's right . phd f: Yeah . phd c: And then you just , uh , compute differences phd f: Yeah . I have here one example if you {disfmarker} if you want see something like that . phd a: Then we compute the difference . phd c: and , phd a: Yeah . Uh - huh . professor b: OK . phd c: uh , sum the differences ? phd a: So . And I think the variance is computed only from , like , two hundred hertz to {pause} one {disfmarker} to fifteen hundred . phd c: Oh ! OK . professor b: Mm - hmm . phd f: Two thou two {disfmarker} {comment} fifteen hundred ? professor b: Mm - hmm . phd a: Because {disfmarker} phd f: No . professor b: Right . phd f: Two hundred and fifty thousand . phd a: Fifteen hundred . Because {disfmarker} Yeah . phd f: Yeah . Two thousand and fifteen hundred . phd a: Above , um {disfmarker} {vocalsound} it seems that {disfmarker} Well , some voiced sound can have also , {vocalsound} like , a noisy {pause} part on high frequencies , and {disfmarker} But {disfmarker} professor b: Yeah . phd a: Well , it 's just {disfmarker} professor b: No , it 's {disfmarker} makes sense to look at {pause} low frequencies . phd c: So this is {disfmarker} uh , basically this is comparing {vocalsound} an original version of the signal to a smoothed version of the same signal ? phd f: Yeah . professor b: Right . So i so i i this is {disfmarker} I mean , i you could argue about whether it should be linear interpolation or {disfmarker} or {disfmarker} or {disfmarker} or zeroeth order , but {disfmarker} but phd c: Uh - huh . professor b: at any rate something like this {pause} is what you 're feeding your recognizer , typically . phd c: Like which of the {disfmarker} ? professor b: No . Uh , so the mel cepstrum is the {disfmarker} is the {disfmarker} is the cepstrum of this {disfmarker} {vocalsound} this , uh , spectrum or log spectrum , phd a: So this is {disfmarker} Yeah . phd c: Yeah . Right , right . professor b: whatever it {disfmarker} You - you 're subtracting in {disfmarker} in {disfmarker} in {vocalsound} power domain or log domain ? phd a: In log domain . Yeah . phd f: Log domain . professor b: OK . So it 's sort of like division , when you do the {disfmarker} yeah , the spectra . phd f: Yeah . phd a: Uh , yeah . phd c: It 's the ratio . professor b: Um . Yeah . But , anyway , um {disfmarker} and that 's {disfmarker} phd c: So what 's th uh , what 's the intuition behind this kind of a thing ? I {disfmarker} I don't know really know the signal - processing well enough to understand what {disfmarker} {vocalsound} what is that doing . phd a: So . Yeah . What happen if {disfmarker} what we have {disfmarker} have {disfmarker} what we would like to have is {pause} some spectrum of the excitation signal , professor b: Yeah . I guess that makes sense . Yeah . phd a: which is for voiced sound ideally a {disfmarker} a pulse train phd c: Uh - huh . phd a: and for unvoiced it 's something that 's more flat . phd c: Uh - huh . Right . phd a: And the way to do this {vocalsound} is that {disfmarker} well , we have the {disfmarker} we have the FFT because it 's computed in {disfmarker} in the {disfmarker} in the system , and we have {vocalsound} the mel {vocalsound} filter banks , phd c: Mm - hmm . Mm - hmm . phd a: and so if we {disfmarker} if we , like , remove the mel filter bank from the FFT , {vocalsound} we have something that 's {pause} close to the {pause} excitation signal . grad e: Oh . phd a: It 's something that 's like {vocalsound} a {disfmarker} a a train of p a pulse train for voiced sound phd c: OK . professor b: Yeah . phd c: Oh ! OK . Yeah . phd a: and that 's {disfmarker} that should be flat for {disfmarker} professor b: Yeah . phd c: I see . So do you have a picture that sh ? phd a: So - It 's {disfmarker} Y phd c: Is this for a voiced segment , phd a: yeah . phd c: this picture ? What does it look like for unvoiced ? phd f: Yeah . phd a: You have several {disfmarker} some unvoiced ? phd f: The dif No . Unvoiced , I don't have phd a: Oh . phd f: for unvoiced . professor b: Yeah . So , you know , all {disfmarker} phd f: I 'm sorry . phd a: But {disfmarker} Yeah . professor b: Yeah . phd f: Yeah . This is the {disfmarker} between {disfmarker} phd a: This is another voiced example . Yeah . phd f: No . But it 's this , phd a: Oh , yeah . This is {disfmarker} phd f: but between the frequency that we are considered for the excitation {disfmarker} phd a: Right . Mm - hmm . phd f: for the difference and this is the difference . phd a: Yeah . phd c: This is the difference . OK . phd a: So , of course , it 's around zero , professor b: Yeah . grad e: Sure looks {disfmarker} phd a: but {disfmarker} grad e: Hmm . phd a: Well , no . phd c: Hmm . phd a: It is {disfmarker} phd f: Yeah . Because we begin , {vocalsound} uh , in fifteen {vocalsound} point {disfmarker} the fifteen point . phd c: So , does {disfmarker} does the periodicity of this signal say something about the {disfmarker} the {disfmarker} phd f: Fifteen p phd a: So it 's {disfmarker} Yeah . professor b: Pitch . phd a: It 's the pitch . phd c: the pitch ? phd a: Yeah . Mm - hmm . professor b: Yeah . phd c: OK . professor b: That 's like fundamental frequency . phd a: Mm - hmm . professor b: So , I mean , i t t phd c: OK . I see . professor b: I mean , to first order {vocalsound} what you 'd {disfmarker} what you 're doing {disfmarker} I mean , ignore all the details and all the ways which is {disfmarker} that these are complete lies . Uh , the {disfmarker} the {disfmarker} you know , what you 're doing in feature extraction for speech recognition is you have , {vocalsound} uh , in your head a {disfmarker} a {disfmarker} a {disfmarker} a simplified production model for speech , phd c: Mm - hmm . professor b: in which you have a periodic or aperiodic source that 's driving some filters . phd c: Mm - hmm . phd f: Yeah . This is the {disfmarker} the auto - correlation {disfmarker} the R - zero energy . phd a: Do you have the mean {disfmarker} do you have the mean for the auto - correlation {disfmarker} ? professor b: Uh , first order for speech recognition , you say " I don't care about the source " . phd f: For {disfmarker} Yeah . phd a: Well , I mean for the {disfmarker} the energy . phd f: I have the mean . professor b: Right ? phd c: Right . professor b: And so you just want to find out what the filters are . phd c: Right . phd f: Yeah . professor b: The filters {vocalsound} roughly act like a , um {disfmarker} {vocalsound} a , uh {disfmarker} {vocalsound} a an overall resonant {disfmarker} you know , f some resonances and so forth that th that 's processing excitation . phd f: Here . phd a: They should be more close . phd f: Ah , no . This is this ? More close . Is this ? And this . phd c: Mm - hmm . phd a: Yeah . phd c: Mm - hmm . phd a: So they are {disfmarker} this is {disfmarker} there is less difference . phd f: Mm - hmm . professor b: So if you look at the spectral envelope , just the very smooth properties of it , {vocalsound} you get something closer to that . phd a: This is less {disfmarker} it 's less robust . phd f: Less robust . Yeah . phd a: Oh , yeah . professor b: And the notion is if you have the full spectrum , with all the little nitty - gritty details , {vocalsound} that that has the effect of both , phd c: Yeah . professor b: and it would be a multiplication in {disfmarker} in frequency domain phd c: Mm - hmm . professor b: so that would be like an addition in log {disfmarker} {vocalsound} power spectrum domain . phd c: Mm - hmm . Mm - hmm . professor b: And so this is saying , well , if you really do have that {vocalsound} sort of vocal tract envelope , and you subtract that off , what you get is the excitation . And I call that lies because you don't really have that , you just have some kind of {vocalsound} signal - processing trickery to get something that 's kind of smooth . It 's not really what 's happening in the vocal tract phd c: Yeah . professor b: so you 're not really getting the vocal excitation . phd c: Right . professor b: That 's why I was going to the {disfmarker} why I was referring to it in a more {disfmarker} {vocalsound} a more , uh , {vocalsound} uh , {vocalsound} conservative way , when I was saying " well , it 's {disfmarker} yeah , it 's the excitation " . But it 's not really the excitation . It 's whatever it is that 's different between {disfmarker} phd c: Oh . This moved in the {disfmarker} professor b: So {disfmarker} so , stand standing back from that , you sort of say there 's this very detailed representation . phd c: Yeah . professor b: You go to a smooth representation . phd c: Mm - hmm . professor b: You go to a smooth representation cuz this typically generalizes better . phd c: Mm - hmm . professor b: Um , but whenever you smooth you lose something , so the question is have you lost something you can you use ? phd c: Right . professor b: Um , probably you wouldn't want to go to the extreme of just ta saying " OK , our feature set will be the FFT " , cuz we really think we do gain something in robustness from going to something smoother , but maybe there 's something that we missed . phd c: Mm - hmm . professor b: So what is it ? phd c: Yeah . professor b: And then you go back to the intuition that , well , you don't really get the excitation , but you get something related to it . phd c: Mm - hmm . professor b: And it {disfmarker} and as you can see from those pictures , you do get something {vocalsound} that shows some periodicity , uh , in frequency , phd c: Mm - hmm . professor b: you know , and {disfmarker} and {disfmarker} and also in time . phd c: Hmm . professor b: So {disfmarker} phd c: That 's {disfmarker} that 's really neat . professor b: so , phd c: So you don't have one for unvoiced {pause} picture ? phd f: Uh , not here . phd c: Oh . phd f: No , I have s phd a: Mm - hmm . professor b: Yeah . phd f: But not here . professor b: But presumably you 'll see something that won't have this kind of , uh , uh , uh , regularity in frequency , uh , in the {disfmarker} phd a: But {disfmarker} Yeah . Well . phd f: Not here . phd c: I would li I would like to see those {pause} pictures . phd f: Well , so . professor b: Yeah . phd f: I can't see you {comment} now . professor b: Yeah . phd c: Yeah . professor b: Yeah . phd a: Mm - hmm . phd f: I don't have . phd c: And so you said this is pretty {disfmarker} doing this kind of thing is pretty robust to noise ? phd a: It seems , yeah . Um , phd c: Huh . phd f: Pfft . Oops . The mean is different {vocalsound} with it , because the {disfmarker} {vocalsound} the histogram for the {disfmarker} {vocalsound} the classifica phd a: No , no , no . But th the kind of robustness to noise {disfmarker} phd f: Oh ! phd a: So if {disfmarker} if you take this frame , {vocalsound} uh , from the noisy utterance and the same frame from the clean utterance {disfmarker} phd f: Hmm . phd c: You end up with a similar difference phd a: Y y y yeah . We end up with {disfmarker} phd c: over here ? phd a: Yeah . phd c: OK . Cool ! phd f: I have here the same frame for the {pause} clean speech {disfmarker} phd c: Oh , that 's clean . phd f: the same cle phd c: Oh , OK phd f: But they are a difference . phd a: Yeah , that 's {disfmarker} phd f: Because here the FFT is only with {vocalsound} two hundred fifty - six point phd c: Oh . phd f: and this is with five hundred {pause} twelve . phd a: Yeah . This is kind of inter interesting also phd c: OK . phd a: because if we use the standard , {vocalsound} uh , frame length of {disfmarker} of , like , twenty - five milliseconds , {vocalsound} um , {vocalsound} what happens is that for low - pitched voiced , because of the frame length , y you don't really have {disfmarker} {vocalsound} you don't clearly see this periodic structure , professor b: Mm - hmm . phd a: because of the first lobe of {disfmarker} of each {disfmarker} each of the harmonics . phd c: So this one inclu is a longer {disfmarker} Ah . phd a: So , this is like {disfmarker} yeah , fifty milliseconds or something like that . phd f: Fifty millis Yeah . phd a: Yeah , but it 's the same frame and {disfmarker} phd c: Oh , it 's that time - frequency trade - off thing . phd a: Yeah . phd c: Right ? I see . Yeah . phd a: So , yeah . professor b: Mm - hmm . phd c: Oh . Oh , so this i is this the difference here , for that ? phd f: No . This is the signal . This is the signal . phd a: I see that . Oh , yeah . phd f: The frame . phd c: Oh , that 's the f the original . phd a: Yeah . phd f: This is the fra the original frame . phd a: So with a short frame basically you have only two periods phd c: Yeah . phd a: and it 's not {disfmarker} not enough to {disfmarker} to have this kind of neat things . phd c: Mm - hmm . phd f: Mm - hmm . phd c: Yeah . phd a: But {disfmarker} phd f: And here {disfmarker} No , well . phd a: Yeah . So probably we 'll have to use , {vocalsound} like , long f long frames . Mm - hmm . phd c: Mm - hmm . grad e: Hmm . phd c: Oh . professor b: Mmm . phd c: That 's interesting . professor b: Yeah , maybe . Well , I mean it looks better , but , I mean , the thing is if {disfmarker} if , uh {disfmarker} if you 're actually asking {disfmarker} you know , if you actually j uh , need to do {disfmarker} place along an FFT , it may be {disfmarker} it may be pushing things . phd a: Yeah . professor b: And {disfmarker} and , uh {disfmarker} phd c: Would you {disfmarker} would you wanna do this kind of , uh , difference thing {vocalsound} after you do spectral subtraction ? phd a: Uh , {vocalsound} maybe . phd f: No . Maybe we can do that . phd a: Mmm . professor b: Hmm . The spectral subtraction is being done at what level ? Is it being done at the level of FFT bins or at the level of , uh , mel spectrum or something ? phd a: Um , I guess it depends . professor b: I mean , how are they doing it ? phd a: How they 're doing it ? Yeah . Um , I guess Ericsson is on the , um , filter bank , phd f: FFT . Filter bank , phd a: no ? It 's on the filter bank , phd f: yeah . phd a: so . So , yeah , probably {disfmarker} I i it {disfmarker} Yeah . professor b: So in that case , it might not make much difference at all . phd c: Seems like you 'd wanna do it on the FFT bins . professor b: Maybe . I mean , certainly it 'd be better . phd c: I I mean , if you were gonna {disfmarker} uh , for {disfmarker} for this purpose , that is . phd a: Mm - hmm . professor b: Yeah . phd a: Mm - hmm . professor b: Yeah . OK . phd a: Mmm . professor b: What else ? phd a: Uh . {vocalsound} Yeah , that 's all . So we 'll perhaps {vocalsound} {vocalsound} {vocalsound} try to convince OGI people to use the new {disfmarker} {vocalsound} the new filters and {disfmarker} Yeah . professor b: OK . Uh , has {disfmarker} has anything happened yet on this business of having some sort of standard , uh , source , phd a: Uh , not yet professor b: or {disfmarker} ? phd a: but I wi I will {vocalsound} call them and {disfmarker} professor b: OK . phd a: now they are {disfmarker} I think they have more time because they have this {disfmarker} well , Eurospeech deadline is {vocalsound} over phd c: When is the next , um , Aurora {pause} deadline ? phd a: and {disfmarker} It 's , um , in June . Yeah . phd c: June . professor b: Early June , late June , middle June ? phd a: I don't know w professor b: Hmm . grad e: Hmm . professor b: OK . Um , and {pause} he 's been doing all the talking but {disfmarker} but {vocalsound} these {disfmarker} {vocalsound} he 's {disfmarker} he 's , uh {disfmarker} phd f: Yeah . professor b: This is {disfmarker} this by the way a bad thing . We 're trying to get , um , m more female voices in this record as well . So . Make sur make sure Carmen {vocalsound} talks as well . Uh , but has he pretty much been talking about what you 're doing also , and {disfmarker} ? phd f: Oh , I {disfmarker} I am doing this . professor b: Yes . phd f: Yeah , yeah . I don't know . I 'm sorry , but I think that for the recognizer for the meeting recorder that it 's better that I don't speak . professor b: Yeah , well . phd f: Because {disfmarker} professor b: You know , uh , we 'll get {disfmarker} we 'll get to , uh , Spanish voices sometime , and {vocalsound} we do {disfmarker} we want to recognize , {vocalsound} uh , you too . phd f: After the {disfmarker} after , uh , the result for the TI - digits {vocalsound} on the meeting record there will be foreigns people . phd a: Yeah , but {disfmarker} professor b: Oh , no . phd c: Y professor b: We like {disfmarker} we {disfmarker} we 're {disfmarker} we 're {disfmarker} w we are {disfmarker} we 're in the , uh , Bourlard - Hermansky - Morgan , uh , frame of mind . Yeah , we like high error rates . It 's {disfmarker} phd a: Yeah . professor b: That way there 's lots of work to do . So it 's {disfmarker} Uh , anything to talk about ? grad d: N um , not not not much is new . So when I talked about what I 'm planning to do last time , {vocalsound} I said I was , um , going to use Avendano 's method of , um , {vocalsound} using a transformation , um , {vocalsound} to map from long analysis frames which are used for removing reverberation to short analysis frames for feature calculation . He has a trick for doing that {pause} involving viewing the DFT as a matrix . Um , but , uh , um , I decided {vocalsound} not to do that after all because I {disfmarker} I realized to use it I 'd need to have these short analysis frames get plugged directly into the feature computation somehow professor b: Mm - hmm . grad d: and right now I think our feature computation is set to up to , um , {vocalsound} take , um , audio as input , in general . So I decided that I {disfmarker} I 'll do the reverberation removal on the long analysis windows and then just re - synthesize audio and then send that . professor b: This is in order to use the SRI system or something . Right ? grad d: Um , or {disfmarker} or even if I 'm using our system , I was thinking it might be easier to just re - synthesize the audio , professor b: Yeah ? grad d: because then I could just feacalc as is and I wouldn't have to change the code . professor b: Oh , OK . Yeah . I mean , it 's {disfmarker} um , certainly in a short {disfmarker} short - term this just sounds easier . grad d: Uh - huh . professor b: Yeah . I mean , longer - term if it 's {disfmarker} {vocalsound} if it turns out to be useful , one {disfmarker} one might want to do something else , grad d: Right . That 's true . professor b: but {disfmarker} Uh , uh , I mean , in {disfmarker} in other words , you {disfmarker} you may be putting other kinds of errors in {pause} from the re - synthesis process . grad d: But {disfmarker} e u From the re - synthesis ? Um , professor b: Yeah . grad d: O - OK . I don't know anything about re - synthesis . Uh , how likely do you think that is ? professor b: Uh , it depends what you {disfmarker} what you do . I mean , it 's {disfmarker} it 's {disfmarker} it 's , uh , um {disfmarker} Don't know . But anyway it sounds like a reasonable way to go for a {disfmarker} for an initial thing , and we can look at {disfmarker} {vocalsound} at exactly what you end up doing and {disfmarker} and then figure out if there 's some {disfmarker} {vocalsound} something that could be {disfmarker} be hurt by the end part of the process . grad d: OK . professor b: OK . So that 's {disfmarker} That was it , huh ? grad d: That {disfmarker} Yeah , e That 's it , that 's it . professor b: OK . OK . grad d: Uh - huh . professor b: Um , anything to {pause} add ? grad e: Um . Well , I 've been continuing reading . I went off on a little tangent this past week , um , looking at , uh , {vocalsound} uh , modulation s spectrum stuff , um , and {disfmarker} and learning a bit about what {disfmarker} what , um {disfmarker} what it is , and , uh , the importance of it in speech recognition . And I found some {disfmarker} {vocalsound} some , uh , neat papers , {vocalsound} um , historical papers from , {vocalsound} um , {vocalsound} Kanedera , Hermansky , and Arai . professor b: Yeah . grad e: And they {disfmarker} they did a lot of experiments where th where , {vocalsound} um , they take speech {vocalsound} and , um , e they modify {vocalsound} the , uh {disfmarker} they {disfmarker} they {disfmarker} they measure the relative importance of having different , um , portions of the modulation spectrum intact . professor b: Yeah . grad e: And they find that the {disfmarker} the spectrum between one and sixteen hertz in the modulation {vocalsound} is , uh {disfmarker} is im important for speech recognition . professor b: Sure . I mean , this sort of goes back to earlier stuff by Drullman . grad e: Um . professor b: And {disfmarker} and , uh , the {disfmarker} the MSG features were sort of built up {vocalsound} with this notion {disfmarker} grad e: Yeah . Right . professor b: But , I guess , I thought you had brought this up in the context of , um , targets somehow . grad e: Right . professor b: But i m grad e: Um {disfmarker} professor b: i it 's not {disfmarker} I mean , they 're sort of not in the same kind of category as , say , a phonetic target or a syllabic target grad e: Mmm . Mm - hmm . professor b: or a {disfmarker} grad e: Um , I was thinking more like using them as {disfmarker} as the inputs to {disfmarker} to the detectors . professor b: or a feature or something . Oh , I see . Well , that 's sort of what MSG does . grad e: Yeah . Yeah . professor b: Right ? So it 's {disfmarker} grad e: Mm - hmm . professor b: But {disfmarker} but , uh {disfmarker} grad e: S professor b: Yeah . grad e: Yeah . professor b: Anyway , we 'll talk more about it later . grad e: OK . professor b: Yeah . grad e: We can talk more about it later . professor b: Yeah . Yeah . grad e: Yeah . professor b: So maybe , {vocalsound} le phd c: Should we do digits ? professor b: let 's do digits . Let you {disfmarker} you start . grad d: Oh , OK . grad e: L fifty . phd a: Right .