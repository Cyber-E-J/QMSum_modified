phd a: Alright . We 're on . professor b: Test , um . Test , test , test . Guess that 's me . Yeah . OK . grad d: Ooh , Thursday . professor b: So . There 's two sheets of paper in front of us . phd a: What are these ? phd e: Yeah . So . professor b: This is the arm wrestling ? phd c: Uh . Yeah , we formed a coalition actually . phd e: Yeah . Almost . phd c: We already made it into one . professor b: Oh , good . phd c: Yeah . professor b: Excellent . phd e: Yeah . professor b: That 's the best thing . phd e: Mm - hmm . professor b: So , tell me about it . phd e: So it 's {disfmarker} well , it 's {pause} spectral subtraction or Wiener filtering , um , depending on if we put {disfmarker} if we square the transfer function or not . professor b: Right . phd e: And then with over - estimation of the noise , depending on the , uh {disfmarker} the SNR , with smoothing along time , um , smoothing along frequency . professor b: Mm - hmm . phd e: It 's very simple , smoothing things . professor b: Mm - hmm . phd e: And , um , {vocalsound} the best result is {vocalsound} when we apply this procedure on FFT bins , uh , with a Wiener filter . professor b: Mm - hmm . phd e: And there is no noise addition after {disfmarker} after that . professor b: OK . phd e: So it 's good because {vocalsound} {vocalsound} it 's difficult when we have to add noise to {disfmarker} to {disfmarker} to find the right level . professor b: OK . phd a: Are you looking at one in {disfmarker} in particular of these two ? phd e: Yeah . So the sh it 's the sheet that gives fifty - f three point sixty - six . professor b: Mm - hmm . phd e: Um , {vocalsound} the second sheet is abo uh , about the same . It 's the same , um , idea but it 's working on mel bands , {vocalsound} and it 's a spectral subtraction instead of Wiener filter , and there is also a noise addition after , uh , cleaning up the mel bins . Mmm . Well , the results are similar . professor b: Yeah . I mean , {vocalsound} it 's {disfmarker} {comment} it 's actually , uh , very similar . phd e: Mm - hmm . professor b: I mean , {vocalsound} if you look at databases , uh , the , uh , one that has the smallest {disfmarker} smaller overall number is actually better on the Finnish and Spanish , uh , but it is , uh , worse on the , uh , Aurora {disfmarker} phd e: It 's worse on {disfmarker} professor b: I mean on the , uh , TI - TI - digits , phd e: on the multi - condition in TI - digits . Yeah . professor b: uh , uh . Um . phd e: Mmm . professor b: So , it probably doesn't matter that much either way . But , um , when you say u uh , unified do you mean , uh , it 's one piece of software now , or {disfmarker} ? phd e: So now we are , yeah , setting up the software . professor b: Mm - hmm . phd e: Um , it should be ready , uh , very soon . Um , and we phd a: So what 's {disfmarker} what 's happened ? I think I 've missed something . professor b: OK . So a week ago {disfmarker} maybe you weren't around when {disfmarker} when {disfmarker} when Hynek and Guenther and I {disfmarker} ? phd c: Hynek was here . phd a: Yeah . I didn't . professor b: Oh , OK . So {disfmarker} Yeah , let 's summarize . Um {disfmarker} And then if I summarize somebody can tell me if I 'm wrong , which will also be possibly helpful . What did I just press here ? I hope this is still working . phd e: p - p - p professor b: We , uh {disfmarker} we looked at , {nonvocalsound} uh {disfmarker} anyway we {disfmarker} {vocalsound} after coming back from QualComm we had , you know , very strong feedback and , uh , I think it was {vocalsound} Hynek and Guenter 's and my opinion also that , um , you know , we sort of spread out to look at a number of different ways of doing noise suppression . But given the limited time , uh , it was sort of time to {pause} choose one . phd a: Mm - hmm . Mmm . professor b: Uh , and so , uh , th the vector Taylor series hadn't really worked out that much . Uh , the subspace stuff , uh , had not been worked with so much . Um , so it sort of came down to spectral subtraction versus Wiener filtering . phd a: Hmm . professor b: Uh , we had a long discussion about how they were the same and how they were d uh , completely different . phd a: Mm - hmm . professor b: And , uh , I mean , fundamentally they 're the same sort of thing but the math is a little different so that there 's a {disfmarker} a {disfmarker} {vocalsound} there 's an exponent difference in the index {disfmarker} you know , what 's the ideal filtering , and depending on how you construct the problem . phd a: Uh - huh . professor b: And , uh , I guess it 's sort {disfmarker} you know , after {disfmarker} after that meeting it sort of made more sense to me because {vocalsound} um , if you 're dealing with power spectra then how are you gonna choose your error ? And typically you 'll do {disfmarker} choose something like a variance . And so that means it 'll be something like the square of the power spectra . Whereas when you 're {disfmarker} when you 're doing the {disfmarker} the , uh , um , {vocalsound} looking at it the other way , you 're gonna be dealing with signals phd c: Mm - hmm . professor b: and you 're gonna end up looking at power {disfmarker} uh , noise power that you 're trying to reduce . And so , eh {disfmarker} so there should be a difference {vocalsound} of {disfmarker} you know , conceptually of {disfmarker} of , uh , a factor of two in the exponent . phd a: Mm - hmm . professor b: But there 're so many different little factors that you adjust in terms of {disfmarker} of , uh , {vocalsound} uh , over - subtraction and {disfmarker} and {disfmarker} and {disfmarker} and {disfmarker} and so forth , um , that {vocalsound} arguably , you 're c and {disfmarker} and {disfmarker} and the choice of do you {disfmarker} do you operate on the mel bands or do you operate on the FFT beforehand . There 're so many other choices to make that are {disfmarker} are almost {disfmarker} well , if not independent , certainly in addition to {pause} the choice of whether you , uh , do spectral subtraction or Wiener filtering , that , um , {vocalsound} @ @ again we sort of felt the gang should just sort of figure out which it is they wanna do and then let 's pick it , go forward with it . So that 's {disfmarker} that was {disfmarker} that was last week . And {disfmarker} {vocalsound} and , uh , we said , uh , take a week , go arm wrestle , you know , grad d: Oh . professor b: figure it out . I mean , and th the joke there was that each of them had specialized in one of them . phd a: Oh , OK . professor b: And {disfmarker} and so they {disfmarker} so instead they went to Yosemite and bonded , and {disfmarker} and they came out with a single {disfmarker} single piece of software . So it 's {vocalsound} another {disfmarker} another victory for international collaboration . So . phd a: So {disfmarker} so you guys have combined {disfmarker} or you 're going to be combining the software ? professor b: Uh . phd c: Well , the piece of software has , like , plenty of options , phd e: Oh boy . phd c: like you can parse command - line arguments . So depending on that , it {disfmarker} it becomes either spectral subtraction or Wiener filtering . phd a: Oh , OK . phd c: So , ye phd a: They 're close enough . professor b: Well , that 's fine , but the thing is {disfmarker} the important thing is that there is a piece of software that you {disfmarker} that we all will be using now . phd c: Yeah . Yeah . professor b: Yes . phd c: There 's just one piece of software . phd e: Yeah . professor b: Yeah . phd e: I need to allow it to do everything and even more {disfmarker} more than this . phd c: Right . phd e: Well , if we want to , like , optimize different parameters of {disfmarker} phd c: Parameters . Yeah . professor b: Sure . phd e: Yeah , we can do it later . But , still {disfmarker} so , there will be a piece of software with , {vocalsound} {vocalsound} uh , will give this system , the fifty - three point sixty - six , by default and {disfmarker} professor b: Mm - hmm . phd a: How {disfmarker} how is {disfmarker} how good is that ? phd e: Mm - hmm . phd a: I {disfmarker} I {disfmarker} I don't have a sense of {disfmarker} phd e: It 's just one percent off of the {pause} best proposal . phd c: Best system . phd e: It 's between {disfmarker} i we are second actually if we take this system . phd a: OK . professor b: Yeah . phd c: Yeah . phd e: Right ? phd a: Compared to the last evaluation numbers ? Yeah . professor b: But , uh {disfmarker} w which we sort of were before phd c: Yeah . phd e: Mm - hmm . Yeah . professor b: but we were considerably far behind . And the thing is , this doesn't have neural net in yet for instance . You know ? phd e: Mm - hmm . phd a: Hmm . professor b: So it {disfmarker} so , um , it 's {disfmarker} it it 's not using our full bal bag of tricks , if you will . phd a: Mm - hmm . professor b: And , uh , and it {disfmarker} it is , uh , very close in performance to the best thing that was there before . Uh , but , you know , looking at it another way , maybe more importantly , uh , {vocalsound} we didn't have any explicit noise , uh , handling {disfmarker} stationary {disfmarker} dealing with {disfmarker} e e we didn't explicitly have anything to deal with stationary noise . phd a: Mm - hmm . professor b: And now we do . phd a: So will the {pause} neural net operate on the output from either the Wiener filtering or the spectral subtraction ? Or will it operate on the original ? professor b: Well , so {disfmarker} so {disfmarker} so argu arguably , I mean , what we should do {disfmarker} I mean , I gather you have {disfmarker} it sounds like you have a few more days of {disfmarker} of nailing things down with the software and so on . But {disfmarker} and then {disfmarker} but , um , {vocalsound} arguably what we should do is , even though the software can do many things , we should for now pick a set of things , th these things I would guess , and not change that . phd e: Mm - hmm . professor b: And then focus on {pause} everything that 's left . And I think , you know , that our goal should be by next week , when Hynek comes back , {vocalsound} uh , to {disfmarker} uh , really just to have a firm path , uh , for the {disfmarker} you know , for the time he 's gone , of {disfmarker} of , uh , what things will be attacked . But I would {disfmarker} I would {disfmarker} I would thought think that what we would wanna do is not futz with this stuff for a while because what 'll happen is we 'll change many other things in the system , phd a: Mm - hmm . professor b: and then we 'll probably wanna come back to this and possibly make some other choices . But , um . phd a: But just conceptually , where does the neural net go ? Do {disfmarker} do you wanna h run it on the output of the spectrally subtracted {disfmarker} ? phd e: Mmm . professor b: Well , depending on its size {disfmarker} Well , one question is , is it on the , um , server side or is it on the terminal side ? Uh , if it 's on the server side , it {disfmarker} you probably don't have to worry too much about size . phd a: Mm - hmm . professor b: So that 's kind of an argument for that . We do still , however , have to consider its latency . So the issue is {disfmarker} is , um , {vocalsound} for instance , could we have a neural net that only looked at the past ? phd a: Right . professor b: Um , what we 've done in uh {disfmarker} in the past is to use the neural net , uh , to transform , {vocalsound} um , all of the features that we use . So this is done early on . This is essentially , {vocalsound} um , um {disfmarker} I guess it 's {disfmarker} it 's more or less like a spee a speech enhancement technique here {disfmarker} phd a: Mm - hmm . professor b: right ? {disfmarker} where we 're just kind of creating {vocalsound} new {disfmarker} if not new speech at least new {disfmarker} new FFT 's that {disfmarker} that have {disfmarker} you know , which could be turned into speech {disfmarker} uh , that {disfmarker} that have some of the noise removed . phd e: Mm - hmm . phd a: Mm - hmm . professor b: Um , after that we still do a mess of other things to {disfmarker} to produce a bunch of features . phd a: Right . professor b: And then those features are not now currently transformed {vocalsound} by the neural net . And then the {disfmarker} the way that we had it in our proposal - two before , we had the neural net transformed features and we had {vocalsound} the untransformed features , which I guess you {disfmarker} you actually did linearly transform with the KLT , phd e: Yeah . Yeah . Right . professor b: but {disfmarker} but {disfmarker} but {disfmarker} uh , to orthogonalize them {disfmarker} but {disfmarker} {vocalsound} but they were not , uh , processed through a neural net . And Stephane 's idea with that , as I recall , was that {vocalsound} you 'd have one part of the feature vector that was very discriminant and another part that wasn't , phd a: Mm - hmm . professor b: uh , which would smooth things a bit for those occasions when , uh , the testing set was quite different than what you 'd trained your discriminant features for . So , um , all of that is {disfmarker} is , uh {disfmarker} still seems like a good idea . The thing is now we know some other constraints . We can't have unlimited amounts of latency . Uh , y you know , that 's still being debated by the {disfmarker} by people in Europe but , {vocalsound} uh , no matter how they end up there , it 's not going to be unlimited amounts , phd a: Yeah . professor b: so we have to be a little conscious of that . Um . So there 's the neural net issue . There 's the VAD issue . And , uh , there 's the second stream {pause} thing . And I think those that we {disfmarker} last time we agreed that those are the three things that have to get , uh , focused on . phd a: What was the issue with the VAD ? professor b: Well , better {comment} ones are good . phd a: And so the w the default , uh , boundaries that they provide are {disfmarker} they 're OK , but they 're not all that great ? professor b: I guess they still allow two hundred milliseconds on either side or some ? Is that what the deal is ? phd e: Mm - hmm . Uh , so th um , they keep two hundred milliseconds at the beginning and end of speech . And they keep all the {disfmarker} phd a: Outside the beginnings and end . phd e: Yeah . phd a: Uh - huh . phd e: And all the speech pauses , which is {disfmarker} Sometimes on the SpeechDat - Car you have pauses that are more than one or two seconds . phd a: Wow . phd e: More than one second for sure . Um . phd a: Hmm . phd e: Yeah . And , yeah , it seems to us that this way of just dropping the beginning and end is not {disfmarker} We cou we can do better , I think , phd a: Mm - hmm . phd e: because , um , {vocalsound} with this way of dropping the frames they improve {pause} over the baseline by fourteen percent and {vocalsound} Sunil already showed that with our current VAD we can improve by more than twenty percent . phd a: On top of the VAD that they provide ? phd c: No . phd e: Just using either their VAD or our current VAD . phd c: Our way . phd a: Oh , OK . phd e: So , our current VAD is {disfmarker} is more than twenty percent , while their is fourteen . phd a: Theirs is fourteen ? I see . phd e: Yeah . phd a: Huh . phd e: So . Yeah . And {pause} another thing that we did also is that we have all this training data for {disfmarker} let 's say , for SpeechDat - Car . We have channel zero which is clean , channel one which is far - field microphone . And if we just take only the , um , VAD probabilities computed on the clean signal and apply them on the far - field , uh , test utterances , {vocalsound} then results are much better . phd a: Mm - hmm . phd e: In some cases it divides the error rate by two . phd a: Wow . phd e: So it means that there are stim {comment} still {disfmarker} phd a: How {disfmarker} how much latency does the , uh {disfmarker} does our VAD add ? phd e: If {disfmarker} if we can have a good VAD , well , it would be great . phd a: Is it significant , phd e: Uh , right now it 's , um , a neural net with nine frames . phd a: or {disfmarker} ? phd e: So it 's forty milliseconds plus , um , the rank ordering , which , uh , should be phd c: Like another ten frames . phd e: ten {disfmarker} Yeah . grad d: Rank . Oh . phd e: So , right now it 's one hundred and forty {pause} milliseconds . professor b: With the rank ordering {disfmarker} ? I 'm sorry . phd c: The {disfmarker} the {disfmarker} the smoothing {disfmarker} the m the {disfmarker} the filtering of the probabilities . phd e: The {disfmarker} The , um {disfmarker} phd c: on the R . phd e: Yeah . It 's not a median filtering . It 's just {disfmarker} We don't take the median value . We take something {disfmarker} Um , so we have eleven , um , frames . professor b: Oh , this is for the VAD . phd c: Yeah . phd e: And {disfmarker} for the VAD , yeah {disfmarker} professor b: Oh , OK . phd c: Yeah . phd e: and we take th the third . phd c: Yeah . grad d: Dar phd e: Um . professor b: Yeah . Um . So {disfmarker} {comment} Yeah , I was just noticing on this that it makes reference to delay . phd e: Mmm . professor b: So what 's the {disfmarker} ? If you ignore {disfmarker} Um , the VAD is sort of in {disfmarker} in parallel , isn't i isn't it , with {disfmarker} with the {disfmarker} ? I mean , it isn't additive with the {disfmarker} the , uh , LDA and the Wiener filtering , and so forth . phd c: The LDA ? professor b: Right ? phd c: Yeah . So {disfmarker} so what happened right now , we removed the delay of the LDA . phd e: Mm - hmm . professor b: Yeah . phd c: So we {disfmarker} I mean , if {disfmarker} so if we {disfmarker} if {disfmarker} so which is like if we reduce the delay of VA So , the f the final delay 's now ba is f determined by the delay of the VAD , because the LDA doesn't have any delay . So if we re if we reduce the delay of the VAD , I mean , it 's like effectively reducing the delay . phd a: How {disfmarker} how much , uh , delay was there on the LDA ? phd c: So the LDA and the VAD both had a hundred millisecond delay . So and they were in parallel , so which means you pick either one of them {disfmarker} phd a: Mmm . phd c: the {disfmarker} the biggest , whatever . phd a: I see . professor b: Mm - hmm . phd c: So , right now the LDA delays are more . professor b: And there {disfmarker} phd a: Oh , OK . professor b: And there didn't seem to be any , uh , penalty for that ? There didn't seem to be any penalty for making it causal ? phd c: Pardon ? Oh , no . It actually made it , like , point one percent better or something , actually . professor b: OK . Well , may as well , then . phd c: Or something like that professor b: And he says Wiener filter is {disfmarker} is forty milliseconds delay . phd c: and {disfmarker} professor b: So is it {disfmarker} ? phd c: Yeah . So that 's the one which Stephane was discussing , like {disfmarker} phd e: Mmm . professor b: The smoothing ? phd c: Yeah . The {disfmarker} you smooth it and then delay the decision by {disfmarker} So . professor b: Right . OK . So that 's {disfmarker} that 's really not {disfmarker} not bad . So we may in fact {disfmarker} we 'll see what they decide . We may in fact have , {vocalsound} um , the {disfmarker} the , uh , latency time available for {disfmarker} to have a neural net . I mean , sounds like we probably will . So . phd c: Mm - hmm . professor b: That 'd be good . Cuz I {disfmarker} cuz it certainly always helped us before . So . phd a: What amount of latency are you thinking about when you say that ? professor b: Uh . Well , they 're {disfmarker} you know , they 're disputing it . phd a: Mmm . professor b: You know , they 're saying , uh {disfmarker} one group is saying a hundred and thirty milliseconds and another group is saying two hundred and fifty milliseconds . Two hundred and fifty is what it was before actually . So , phd a: Oh . professor b: uh , some people are lobbying {disfmarker} lobbying {comment} to make it shorter . phd a: Hmm . professor b: Um . And , um . phd a: Were you thinking of the two - fifty or the one - thirty when you said we should {pause} have enough for the neural net ? professor b: Well , it just {disfmarker} it {disfmarker} when we find that out it might change exactly how we do it , is all . phd a: Oh , OK . professor b: I mean , how much effort do we put into making it causal ? I mean , {vocalsound} I think the neural net will probably do better if it looks at a little bit of the future . phd a: Mm - hmm . professor b: But , um , it will probably work to some extent to look only at the past . And we ha you know , limited machine and human time , and {vocalsound} effort . And , you know , how {disfmarker} how much time should we put into {disfmarker} into that ? So it 'd be helpful if we find out from the {disfmarker} the standards folks whether , you know , they 're gonna restrict that or not . phd a: Mm - hmm . professor b: Um . But I think , you know , at this point our major concern is making the performance better and {disfmarker} and , um , {vocalsound} if , uh , something has to take a little longer in latency in order to do it that 's {pause} you know , a secondary issue . phd a: Mm - hmm . professor b: But if we get told otherwise then , you know , we may have to c clamp down a bit more . grad d: Mmm . phd c: So , the one {disfmarker} one {disfmarker} one difference is that {disfmarker} was there is like we tried computing the delta and then doing the frame - dropping . grad d: S phd e: Mm - hmm . phd c: The earlier system was do the frame - dropping and then compute the delta on the {disfmarker} professor b: Uh - huh . phd c: So this {disfmarker} phd a: Which could be a kind of a funny delta . Right ? phd c: Yeah . professor b: Oh , oh . So that 's fixed in this . Yeah , we talked about that . phd c: Yeah . So we have no delta . And then {disfmarker} phd e: Yeah . Uh - huh . professor b: Good . phd c: So the frame - dropping is the last thing that we do . So , yeah , what we do is we compute the silence probability , convert it to that binary flag , professor b: Uh - huh . phd c: and then in the end you c up upsample it to {vocalsound} match the final features number of {disfmarker} phd e: Mm - hmm . phd a: Did that help then ? phd c: It seems to be helping on the well - matched condition . So that 's why this improvement I got from the last result . So . And it actually r reduced a little bit on the high mismatch , so in the final weightage it 's b b better because the well - matched is still weighted more than {disfmarker} professor b: So , @ @ I mean , you were doing a lot of changes . Did you happen to notice how much , {vocalsound} uh , the change was due to just this frame - dropping problem ? What about this ? phd c: Uh , y you had something on it . Right ? phd e: Just the frame - dropping problem . Yeah . But it 's {disfmarker} it 's difficult . Sometime we {disfmarker} we change two {disfmarker} two things together and {disfmarker} But it 's around {pause} maybe {disfmarker} it 's less than one percent . professor b: Uh - huh . phd c: Yeah . phd e: It {disfmarker} professor b: Well . {vocalsound} But like we 're saying , if there 's four or five things like that then {vocalsound} pretty sho soon you 're talking real improvement . phd e: Yeah . Yeah . And it {disfmarker} Yeah . And then we have to be careful with that also {disfmarker} with the neural net professor b: Yeah . phd e: because in {comment} the proposal the neural net was also , uh , working on {disfmarker} after frame - dropping . professor b: Mm - hmm . phd e: Um . professor b: Oh , that 's a real good point . phd e: So . Well , we 'll have to be {disfmarker} to do the same kind of correction . professor b: It might be hard if it 's at the server side . Right ? phd e: Mmm . Well , we can do the frame - dropping on the server side or we can just be careful at the terminal side to send a couple of more frames before and after , and {disfmarker} So . I think it 's OK . professor b: OK . phd a: You have , um {disfmarker} So when you {disfmarker} Uh , maybe I don't quite understand how this works , but , um , couldn't you just send all of the frames , but mark the ones that are supposed to be dropped ? Cuz you have a bunch more bandwidth . Right ? professor b: Well , you could . Yeah . I mean , it {disfmarker} it always seemed to us that it would be kind of nice to {disfmarker} in addition to , uh , reducing insertions , actually use up less bandwidth . phd a: Yeah . Yeah . professor b: But nobody seems to have {vocalsound} cared about that in this {pause} evaluation . phd a: And that way the net could use {disfmarker} professor b: So . phd a: If the net 's on the server side then it could use all of the {pause} frames . phd c: Yes , it could be . It 's , like , you mean you just transferred everything and then finally drop the frames after the neural net . phd a: Mm - hmm . phd c: Right ? Yeah . That 's {disfmarker} that 's one thing which {disfmarker} phd e: Mm - hmm . phd a: But you could even mark them , before they get to the server . phd c: Yeah . Right now we are {disfmarker} Uh , ri Right now what {disfmarker} wha what we did is , like , we just mark {disfmarker} we just have this additional bit which goes around the features , {vocalsound} saying it 's currently a {disfmarker} it 's a speech or a nonspeech . phd a: Oh , OK . phd c: So there is no frame - dropping till the final features , like , including the deltas are computed . phd a: I see . phd c: And after the deltas are computed , you just pick up the ones that are marked silence and then drop them . phd a: Mm - hmm . I see . I see . professor b: So it would be more or less the same thing with the neural net , I guess , actually . phd e: Mm - hmm . phd c: So . Yeah , that 's what {disfmarker} that 's what {disfmarker} that 's what , uh , this is doing right now . phd a: I see . OK . professor b: Yeah . phd e: Mm - hmm . professor b: Um . OK . So , uh , what 's , uh {disfmarker} ? That 's {disfmarker} that 's a good set of work that {disfmarker} that , uh {disfmarker} phd c: Just one more thing . Like , should we do something f more for the noise estimation , because we still {disfmarker} ? professor b: Yeah . I was wondering about that . That was {disfmarker} I {disfmarker} I had written that down there . phd c: Yeah . phd e: Mm - hmm . professor b: Um {disfmarker} phd e: So , we , uh {disfmarker} actually I did the first experiment . This is {pause} with just fifteen frames . Um . We take the first fifteen frame of each utterance to it , professor b: Yeah . phd e: and average their power spectra . Um . I tried just plugging the , um , {vocalsound} uh , Guenter noise estimation on this system , and it {disfmarker} uh , it got worse . Um , but of course I didn't play {pause} with it . professor b: Uh - huh . phd e: But {disfmarker} Mm - hmm . Uh , I didn't {pause} do much more {pause} for noise estimation . I just tried this , professor b: Hmm . Yeah . Well , it 's not surprising it 'd be worse the first time . phd e: and {disfmarker} professor b: But , um , phd e: Mm - hmm . professor b: it does seem like , you know , i i i i some compromise between always depending on the first fifteen frames and a a always depending on a {disfmarker} a pause is {disfmarker} is {disfmarker} is a good idea . Uh , maybe you have to weight the estimate from the first - teen {disfmarker} fifteen frames more heavily than {disfmarker} than was done in your first attempt . But {disfmarker} phd e: Mm - hmm . professor b: but {disfmarker} phd e: Yeah , I guess . professor b: Yeah . Um . No , I mean {disfmarker} Um , do you have any way of assessing how well or how poorly the noise estimation is currently doing ? phd e: Mmm . No , we don't . professor b: Yeah . phd e: We don't have nothing {pause} that {disfmarker} phd c: Is there {disfmarker} was there any experiment with {disfmarker} ? Well , I {disfmarker} I did {disfmarker} The only experiment where I tried was I used the channel zero VAD for the noise estimation and frame - dropping . So I don't have a {disfmarker} {vocalsound} I don't have a split , like which one helped more . phd e: Yeah . phd c: So . It {disfmarker} it was the best result I could get . phd e: Mm - hmm . phd c: So , that 's the {disfmarker} professor b: So that 's something you could do with , um , this final system . Right ? Just do this {disfmarker} everything that is in this final system except , {vocalsound} uh , use the channel zero . phd c: Mm - hmm . For the noise estimation . professor b: Yeah . phd c: Yeah . We can try something . professor b: And then see how much better it gets . phd c: Mm - hmm . Sure . professor b: If it 's , you know , essentially not better , then {pause} it 's probably not worth phd e: Yeah . professor b: any more . phd c: Yeah . But the Guenter 's argument is slightly different . It 's , like , ev even {disfmarker} even if I use a channel zero VAD , I 'm just averaging the {disfmarker} {vocalsound} the s power spectrum . But the Guenter 's argument is , like , if it is a non - stationary {pause} segment , then he doesn't update the noise spectrum . So he 's , like {disfmarker} he tries to capture only the stationary part in it . So the averaging is , like , {vocalsound} different from {pause} updating the noise spectrum only during stationary segments . So , th the Guenter was arguing that , I mean , even if you have a very good VAD , averaging it , like , over the whole thing is not a good idea . professor b: I see . phd c: Because you 're averaging the stationary and the non - stationary , and finally you end up getting something which is not really the s because , you {disfmarker} anyway , you can't remove the stationary part fr I mean , non - stationary part from {vocalsound} the signal . professor b: Not using these methods anyway . Yeah . phd c: So {disfmarker} Yeah . So you just {pause} update only doing {disfmarker} or update only the stationary components . Yeah . So , that 's {disfmarker} so that 's still a slight difference from what Guenter is trying  professor b: Well , yeah . And {disfmarker} and also there 's just the fact that , um , eh , uh , although we 're trying to do very well on this evaluation , um , we actually would like to have something that worked well in general . And , um , relying on having fifteen frames at the front or something is {disfmarker} is pretty {disfmarker} phd c: Yeah , yeah . professor b: I mean , you might , you might not . phd c: Mmm . phd e: Mm - hmm . professor b: So , um . Um , it 'd certainly be more robust to different kinds of input if you had at least some updates . Um . phd e: Mm - hmm . professor b: But , um . Well , I don't know . What {disfmarker} what do you , uh {disfmarker} what do you guys see as {disfmarker} as being what you would be doing in the next week , given wha what 's {pause} happened ? phd c: Cure the VAD ? phd e: Yeah . phd a: What was that ? phd c: VAD . phd a: Oh . phd c: And {disfmarker}  professor b: OK . phd e: So , should we keep the same {disfmarker} ? I think we might try to keep the same idea of having a neural network , but {vocalsound} training it on more data and adding better features , I think , but {disfmarker} because the current network is just PLP features . Well , it 's trained on noisy {pause} PLP {disfmarker} phd c: Just the cepstra . Yeah . phd e: PLP features computed on noisy speech . But {vocalsound} {vocalsound} there is no nothing particularly robust in these features . phd a: So , I I uh {disfmarker} phd c: No . phd e: There 's no RASTA , no {disfmarker} phd a: So , uh , I {disfmarker} I don't remember what you said {vocalsound} the answer to my , uh , question earlier . Will you {disfmarker} will you train the net on {disfmarker} after you 've done the spectral subtraction or the Wiener filtering ? professor b: This is a different net . phd a: Oh . phd c: So we have a VAD which is like neur that 's a neural net . phd e: Oh , yeah . Hmm . phd a: Oh , you 're talking about the VAD net . OK . phd c: Yeah . phd e: Mm - hmm . phd a: I see . phd c: So that {disfmarker} that VAD was trained on the noisy features . phd a: Mm - hmm . phd c: So , right now we have , like , uh {disfmarker} we have the cleaned - up features , so we can have a better VAD by training the net on {pause} the cleaned - up speech . phd a: Mm - hmm . I see . I see . phd c: Yeah , but we need a VAD for uh noise estimation also . So it 's , like , where do we want to put the VAD ? Uh , it 's like {disfmarker} phd a: Can you use the same net to do both , or {disfmarker} ? phd c: For {disfmarker} phd a: Can you use the same net that you {disfmarker} that I was talking about to do the VAD ? phd c: Mm - hmm . Uh , it actually comes at v at the very end . phd a: Mm - hmm . phd c: So the net {disfmarker} the final net {disfmarker} I mean , which is the feature net {disfmarker} so that actually comes after a chain of , like , LDA plus everything . So it 's , like , it takes a long time to get a decision out of it . And {disfmarker} {vocalsound} and you can actually do it for final frame - dropping , but not for the VA - f noise estimation . phd a: Mm - hmm . professor b: You see , the idea is that the , um , initial decision to {disfmarker} that {disfmarker} that you 're in silence or speech happens pretty quickly . phd a: Oh , OK . phd c: Hmm . phd a: Cuz that 's used by some of these other {disfmarker} ? professor b: And that {disfmarker} Yeah . And that 's sort of fed forward , and {disfmarker} and you say " well , flush everything , it 's not speech anymore " . phd a: Oh , OK . I see . phd c: Yeah . phd a: I thought that was only used for doing frame - dropping later on . professor b: Um , it is used , uh {disfmarker} Yeah , it 's only used f Well , it 's used for frame - dropping . Um , it 's used for end of utterance phd e: Mmm . professor b: because , you know , there 's {disfmarker} {vocalsound} if you have {pause} more than five hundred milliseconds of {disfmarker} of {disfmarker} of nonspeech then you figure it 's end of utterance or something like that . phd a: Mm - hmm . professor b: So , um . phd e: And it seems important for , like , the on - line normalization . Um . We don't want to update the mean and variance during silen long silence portions . Um . So it {disfmarker} it has to be done before phd a: Oh . I see . phd e: this mean and variance normalization . Um . professor b: Um . Yeah . So probably the VAD and {disfmarker} and maybe testing out the noise {pause} estimation a little bit . I mean , keeping the same method but {disfmarker} but , uh , {vocalsound} seeing if you cou but , um noise estimation could be improved . Those are sort of related issues . phd e: Mm - hmm . professor b: It probably makes sense to move from there . And then , uh , {vocalsound} later on in the month I think we wanna start including the {pause} neural net at the end . Um . OK . Anything else ? phd e: The Half Dome was great . professor b: Good . Yeah . You didn't {disfmarker} didn't fall . That 's good . phd c: Well , yeah . professor b: Our e our effort would have been devastated if you guys had {comment} {vocalsound} run into problems . phd a: So , Hynek is coming back next week , you said ? professor b: Yeah , that 's the plan . phd a: Hmm . professor b: I guess the week after he 'll be , uh , going back to Europe , and so we wanna {disfmarker} phd a: Is he in Europe right now or is he up at {disfmarker} ? professor b: No , no . He 's {disfmarker} he 's {disfmarker} he 's dropped into the US . Yeah . Yeah . phd a: Oh . Hmm . professor b: So . Uh . {vocalsound} So , uh . Uh , the idea was that , uh , we 'd {disfmarker} we 'd sort out where we were going next with this {disfmarker} with this work before he , uh , left on this next trip . Good . {vocalsound} {vocalsound} Uh , Barry , you just got through your {vocalsound} quals , so I don't know if you {vocalsound} have much to say . But , uh . grad d: Mmm . No , just , uh , looking into some {disfmarker} some of the things that , um , {vocalsound} uh , John Ohala and Hynek , um , gave as feedback , um , as {disfmarker} as a starting point for the project . Um . In {disfmarker} in my proposal , I {disfmarker} I was thinking about starting from a set of , uh , phonological features , {vocalsound} or a subset of them . Um , but that might not be necessarily a good idea according to , um , John . phd a: Mm - hmm . grad d: He said , uh , um , these {disfmarker} these phonological features are {disfmarker} are sort of figments of imagination also . phd a: Mm - hmm . grad d: Um . S professor b: In conversational speech in particular . I think you can {disfmarker} you can put them in pretty reliably in synthetic speech . grad d: Ye professor b: But {vocalsound} we don't have too much trouble recognizing synthetic speech since we create it in the first place . So , it 's {disfmarker} grad d: Right . Yeah . So , um , a better way would be something more {disfmarker} more data - driven , phd a: Mm - hmm . grad d: just looking at the data and seeing what 's similar and what 's not similar . phd a: Mm - hmm . grad d: So , I 'm {disfmarker} I 'm , um , taking a look at some of , um , {vocalsound} Sangita 's work on {disfmarker} on TRAPS . She did something where , um {disfmarker} {vocalsound} w where the TRAPS learn She clustered the {disfmarker} the temporal patterns of , um , certain {disfmarker} certain phonemes in {disfmarker} in m averaged over many , many contexts . And , uh , some things tended to cluster . phd a: Mm - hmm . grad d: Right ? You know , like stop {disfmarker} stop consonants clustered really well . phd a: Hmm . grad d: Um , silence was by its own self . phd a: Mm - hmm . grad d: And , uh , um , {vocalsound} v vocalic was clustered . phd a: Mm - hmm . grad d: And , {vocalsound} um , so , {vocalsound} those are {pause} interesting things to {disfmarker} phd a: So you 're {disfmarker} now you 're sort of looking to try to gather a set of these types of features ? grad d: Right . phd a: Mm - hmm . grad d: Yeah . Just to see where {disfmarker} where I could start off from , phd a: Mm - hmm . grad d: uh , you know ? A {disfmarker} a {disfmarker} a set of small features and continue to iterate and find , uh , a better set . phd a: Mm - hmm . grad d: Yeah . professor b: OK . Well , short meeting . That 's OK . phd a: Yeah . professor b: OK . So next week hopefully we 'll {disfmarker} can get Hynek here to {disfmarker} to join us and , uh , uh . phd a: Should we do digits ? professor b: Digits , digits . OK , now . phd a: Go ahead , Morgan . You can start . professor b: Alright . Let me get my glasses on so I can {pause} see them . OK . phd a: OK . And we 're off . professor b: Mm